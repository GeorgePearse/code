{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"kode","text":"<p>kode is a fast, local coding agent for your terminal. It's a community-driven fork of <code>openai/codex</code> focused on real developer ergonomics: Browser integration, multi-agents, theming, and reasoning control \u2014 all while staying compatible with upstream.</p>"},{"location":"#whats-new-in-v040-october-26-2025","title":"What's New in v0.4.0 (October 26, 2025)","text":"<ul> <li>Auto Drive upgraded \u2013 hand <code>/auto</code> a task and it now plans, coordinates agents, reruns checks, and recovers from hiccups without babysitting.</li> <li>Unified settings \u2013 <code>/settings</code> centralizes limits, model routing, themes, and CLI integrations so you can audit configuration in one place.</li> <li>Card-based activity \u2013 Agents, browser sessions, web search, and Auto Drive render as compact cards with drill-down overlays for full logs.</li> <li>Turbocharged performance \u2013 History rendering and streaming were optimized to stay smooth even during long multi-agent sessions.</li> <li>Smarter agents \u2013 Mix and match orchestrator CLIs (Claude, Gemini, GPT-5, Qwen, and more) per <code>/plan</code>, <code>/code</code>, or <code>/solve</code> run.</li> </ul>"},{"location":"#why-kode","title":"Why kode","text":"<ul> <li>\ud83d\ude80 Auto Drive orchestration \u2013 Multi-agent automation that now self-heals and ships complete tasks.</li> <li>\ud83c\udf10 Browser Integration \u2013 CDP support, headless browsing, screenshots captured inline.</li> <li>\ud83e\udd16 Multi-agent commands \u2013 <code>/plan</code>, <code>/code</code> and <code>/solve</code> coordinate multiple CLI agents.</li> <li>\ud83e\udded Unified settings hub \u2013 <code>/settings</code> overlay for limits, theming, approvals, and provider wiring.</li> <li>\ud83c\udfa8 Theme system \u2013 Switch between accessible presets, customize accents, and preview live via <code>/themes</code>.</li> <li>\ud83d\udd0c MCP support \u2013 Extend with filesystem, DBs, APIs, or your own tools.</li> <li>\ud83d\udd12 Safety modes \u2013 Read-only, approvals, and workspace sandboxing.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#run","title":"Run","text":"<pre><code>npx -y @just-every/code\n</code></pre>"},{"location":"#install-run","title":"Install &amp; Run","text":"<pre><code>npm install -g @just-every/code\ncode // or `coder` if you're using VS Code\n</code></pre> <p>Authenticate (one of the following): - Sign in with ChatGPT (Plus/Pro/Team; uses models available to your plan)   - Run <code>code</code> and pick \"Sign in with ChatGPT\" - API key (usage-based)   - Set <code>export OPENAI_API_KEY=xyz</code> and run <code>code</code></p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-agent-commands","title":"Multi-Agent Commands","text":"<ul> <li><code>/plan</code> - Plan code changes (Claude, Gemini and GPT-5 consensus)</li> <li><code>/solve</code> - Solve complex problems (fastest preferred)</li> <li><code>/code</code> - Write code! (consensus approach)</li> </ul>"},{"location":"#auto-drive","title":"Auto Drive","text":"<ul> <li>Hand off multi-step tasks; Auto Drive coordinates agents and approvals</li> <li><code>/auto \"Refactor the auth flow and add device login\"</code></li> </ul>"},{"location":"#browser-integration","title":"Browser Integration","text":"<ul> <li>Connect to external Chrome browser via CDP</li> <li>Use internal headless browser mode</li> <li>Screenshots captured inline for visual context</li> </ul>"},{"location":"#settings-configuration","title":"Settings &amp; Configuration","text":"<ul> <li>Centralized <code>/settings</code> for all configuration</li> <li>Theme system with accessible presets</li> <li>Model routing and reasoning controls</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li>Installation Guide</li> <li>Getting Started</li> <li>Command Reference</li> <li>Configuration Guide</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! This fork maintains compatibility with upstream while adding community-requested features.</p> <p>See our Contributing Guide for details.</p>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 - See LICENSE file for details.</p> <p>This project is a community fork of the original Codex CLI. We maintain compatibility while adding enhanced features requested by the developer community.</p> <p>Need help? Open an issue on GitHub or check our documentation.</p>"},{"location":"CLA/","title":"Individual Contributor License Agreement (v1.0, OpenAI)","text":"<p>Based on the Apache Software Foundation Individual CLA\u00a0v\u00a02.2.</p> <p>By commenting \u201cI have read the CLA Document and I hereby sign the CLA\u201d on a Pull Request, you (\u201cContributor\u201d) agree to the following terms for any past and future \u201cContributions\u201d submitted to the OpenAI\u00a0Codex\u00a0CLI project (the \u201cProject\u201d).</p>"},{"location":"CLA/#1-definitions","title":"1.\u00a0Definitions","text":"<ul> <li>\u201cContribution\u201d \u2013 any original work of authorship submitted to the Project   (code, documentation, designs, etc.).</li> <li>\u201cYou\u201d / \u201cYour\u201d \u2013 the individual (or legal entity) posting the acceptance   comment.</li> </ul>"},{"location":"CLA/#2-copyright-license","title":"2.\u00a0Copyright\u00a0License","text":"<p>You grant OpenAI,\u00a0Inc. and all recipients of software distributed by the Project a perpetual, worldwide, non\u2011exclusive, royalty\u2011free, irrevocable license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and derivative works.</p>"},{"location":"CLA/#3-patent-license","title":"3.\u00a0Patent\u00a0License","text":"<p>You grant OpenAI,\u00a0Inc. and all recipients of the Project a perpetual, worldwide, non\u2011exclusive, royalty\u2011free, irrevocable (except as below) patent license to make, have made, use, sell, offer to sell, import, and otherwise transfer Your Contributions alone or in combination with the Project.</p> <p>If any entity brings patent litigation alleging that the Project or a Contribution infringes a patent, the patent licenses granted by You to that entity under this CLA terminate.</p>"},{"location":"CLA/#4-representations","title":"4.\u00a0Representations","text":"<ol> <li>You are legally entitled to grant the licenses above.</li> <li>Each Contribution is either Your original creation or You have authority to    submit it under this CLA.</li> <li>Your Contributions are provided \u201cAS\u00a0IS\u201d without warranties of any kind.</li> <li>You will notify the Project if any statement above becomes inaccurate.</li> </ol>"},{"location":"CLA/#5-miscellany","title":"5.\u00a0Miscellany","text":"<p>This Agreement is governed by the laws of the State of California, USA, excluding its conflict\u2011of\u2011laws rules. If any provision is held unenforceable, the remaining provisions remain in force.</p>"},{"location":"HOMEBREW/","title":"Homebrew","text":"<p>Homebrew (macOS)</p> <p>This repository now includes a helper script to generate a Homebrew formula from the latest GitHub release artifacts. Publishing to Homebrew requires a tap repository (for example, <code>just-every/homebrew-tap</code>). Once a tap exists, you can generate and publish the formula like so:</p> <p>1) Generate the formula for the latest version:</p> <pre><code>scripts/generate-homebrew-formula.sh\n</code></pre> <p>2) Copy the generated <code>Code.rb</code> into your tap repo under <code>Formula/Code.rb</code> and update the <code>url</code>/<code>sha256</code> if needed.</p> <p>3) Users can then install with:</p> <pre><code>brew tap just-every/tap\nbrew install code\n</code></pre> <p>Notes</p> <ul> <li>The formula expects release assets named like:</li> <li><code>code-aarch64-apple-darwin.tar.gz</code></li> <li><code>code-x86_64-apple-darwin.tar.gz</code></li> <li>The CLI is installed as <code>code</code> and <code>coder</code> shims for compatibility.</li> </ul>"},{"location":"THEME_CONFIG/","title":"Code TUI Theme Configuration","text":"<p>The Code TUI now supports customizable color themes! You can use predefined themes or create your own custom color scheme.</p>"},{"location":"THEME_CONFIG/#configuration","title":"Configuration","text":"<p>Add theme configuration to your <code>~/.code/config.toml</code> file:</p>"},{"location":"THEME_CONFIG/#using-predefined-themes","title":"Using Predefined Themes","text":"<pre><code>[tui.theme]\nname = \"light-photon\"  # Default theme\n</code></pre>"},{"location":"THEME_CONFIG/#available-predefined-themes","title":"Available Predefined Themes","text":""},{"location":"THEME_CONFIG/#light-themes","title":"Light Themes","text":"<ul> <li>light-photon (default): Clean professional light theme with confident blue accent - great for bright environments</li> <li>light-prism-rainbow: Vibrant rainbow accents with blue, violet, and teal highlights</li> <li>light-vivid-triad: Striking cyan, pink, and amber color triad for a modern look</li> <li>light-porcelain: Refined porcelain tones with sophisticated blue and purple accents</li> <li>light-sandbar: Warm sandy beach colors with amber and muted teal highlights</li> <li>light-glacier: Cool glacier blues with purple accents for a fresh, crisp appearance</li> </ul>"},{"location":"THEME_CONFIG/#dark-themes","title":"Dark Themes","text":"<ul> <li>dark-carbon-night: Sleek modern dark theme with blue accents - perfect for long coding sessions</li> <li>dark-shinobi-dusk: Japanese-inspired twilight theme with soothing purple and green tones</li> <li>dark-oled-black-pro: True black background for OLED displays with vibrant neon accents</li> <li>dark-amber-terminal: Retro amber CRT monitor aesthetic - nostalgic monochrome orange</li> <li>dark-aurora-flux: Northern lights inspired with cool blues and greens</li> <li>dark-charcoal-rainbow: High-contrast accessible theme with rainbow accents on dark gray</li> <li>dark-zen-garden: Calm, peaceful theme with mint and lavender tones</li> <li>dark-paper-light-pro: Premium paper-like dark theme for comfortable extended use</li> </ul>"},{"location":"THEME_CONFIG/#custom-color-configuration","title":"Custom Color Configuration","text":"<p>You can override individual colors while using a base theme:</p> <pre><code>[tui.theme]\nname = \"carbon-night\"  # Base theme\n\n[tui.theme.colors]\nprimary = \"#25c2ff\"      # Override the primary color\nsecondary = \"#a9e69e\"    # Green\nbackground = \"#000000\"   # Black\nforeground = \"#ffffff\"   # White\nborder = \"#404040\"       # Dark gray\nborder_focused = \"#00ffff\"  # Cyan\ntext = \"#ffffff\"         # White\ntext_dim = \"#808080\"     # Gray\ntext_bright = \"#ffffff\"  # Bright white\nsuccess = \"#00ff00\"      # Green\nwarning = \"#ffff00\"      # Yellow\nerror = \"#ff0000\"        # Red\ninfo = \"#00ffff\"         # Cyan\nspinner = \"#404040\"      # Dark gray\nprogress = \"#00ffff\"     # Cyan\n</code></pre>"},{"location":"THEME_CONFIG/#complete-custom-theme","title":"Complete Custom Theme","text":"<p>For a fully custom theme:</p> <pre><code>[tui.theme]\nname = \"custom\"\n\n[tui.theme.colors]\n# Primary colors\nprimary = \"#ff79c6\"      # Pink\nsecondary = \"#50fa7b\"    # Green\nbackground = \"#282a36\"   # Dark purple\nforeground = \"#f8f8f2\"   # Off-white\n\n# UI elements\nborder = \"#44475a\"       # Purple-gray\nborder_focused = \"#bd93f9\"  # Purple\nselection = \"#44475a\"    # Purple-gray\ncursor = \"#f8f8f2\"       # Off-white\n\n# Status colors\nsuccess = \"#50fa7b\"      # Green\nwarning = \"#f1fa8c\"      # Yellow\nerror = \"#ff5555\"        # Red\ninfo = \"#8be9fd\"         # Cyan\n\n# Text colors\ntext = \"#f8f8f2\"         # Off-white\ntext_dim = \"#6272a4\"     # Comment blue\ntext_bright = \"#ffffff\"  # Pure white\n\n# Special colors\nkeyword = \"#ff79c6\"      # Pink\nstring = \"#f1fa8c\"       # Yellow\ncomment = \"#6272a4\"      # Comment blue\nfunction = \"#50fa7b\"     # Green\n\n# Animation colors\nspinner = \"#6272a4\"      # Comment blue\nprogress = \"#bd93f9\"     # Purple\n</code></pre>"},{"location":"THEME_CONFIG/#color-format","title":"Color Format","text":"<p>Colors can be specified in the following formats:</p>"},{"location":"THEME_CONFIG/#hex-colors","title":"Hex Colors","text":"<ul> <li><code>\"#rrggbb\"</code> - Standard 6-digit hex color (e.g., <code>\"#ff79c6\"</code>)</li> </ul>"},{"location":"THEME_CONFIG/#named-colors","title":"Named Colors","text":"<ul> <li><code>\"black\"</code>, <code>\"red\"</code>, <code>\"green\"</code>, <code>\"yellow\"</code>, <code>\"blue\"</code>, <code>\"magenta\"</code>, <code>\"cyan\"</code>, <code>\"white\"</code></li> <li><code>\"gray\"</code> or <code>\"grey\"</code>, <code>\"darkgray\"</code> or <code>\"darkgrey\"</code></li> <li><code>\"lightred\"</code>, <code>\"lightgreen\"</code>, <code>\"lightyellow\"</code>, <code>\"lightblue\"</code>, <code>\"lightmagenta\"</code>, <code>\"lightcyan\"</code></li> </ul>"},{"location":"THEME_CONFIG/#examples","title":"Examples","text":""},{"location":"THEME_CONFIG/#example-1-dark-shinobi-dusk-theme","title":"Example 1: Dark Shinobi Dusk Theme","text":"<pre><code>[tui.theme]\nname = \"dark-shinobi-dusk\"\n</code></pre>"},{"location":"THEME_CONFIG/#example-2-light-photon-with-custom-accent","title":"Example 2: Light Photon with Custom Accent","text":"<pre><code>[tui.theme]\nname = \"light-photon\"\n\n[tui.theme.colors]\nprimary = \"#007acc\"      # VS Code blue\nborder_focused = \"#007acc\"\ninfo = \"#007acc\"\n</code></pre>"},{"location":"THEME_CONFIG/#example-3-dark-amber-terminal-for-retro-feel","title":"Example 3: Dark Amber Terminal for Retro Feel","text":"<pre><code>[tui.theme]\nname = \"dark-amber-terminal\"\n\n[tui.theme.colors]\nprimary = \"#ffa500\"      # Make it more orange\nsuccess = \"#ffff00\"      # Bright yellow\nerror = \"#ff6600\"        # Orange-red\n</code></pre>"},{"location":"THEME_CONFIG/#example-4-custom-theme-solarized-inspired","title":"Example 4: Custom Theme - Solarized-inspired","text":"<pre><code>[tui.theme]\nname = \"custom\"\n\n[tui.theme.colors]\nbackground = \"#002b36\"   # Base03\nforeground = \"#839496\"   # Base0\nprimary = \"#268bd2\"      # Blue\nsecondary = \"#859900\"    # Green\nborder = \"#073642\"       # Base02\nborder_focused = \"#268bd2\"  # Blue\ntext = \"#839496\"         # Base0\ntext_dim = \"#586e75\"     # Base01\ntext_bright = \"#93a1a1\"  # Base1\nsuccess = \"#859900\"      # Green\nwarning = \"#b58900\"      # Yellow\nerror = \"#dc322f\"        # Red\ninfo = \"#2aa198\"         # Cyan\n</code></pre>"},{"location":"THEME_CONFIG/#tips","title":"Tips","text":"<ol> <li>Start with a predefined theme that's close to what you want, then customize individual colors</li> <li>Use a color picker tool to find exact hex values</li> <li>Test your theme in different lighting conditions</li> <li>Consider accessibility - ensure sufficient contrast between text and background colors</li> <li>The theme takes effect immediately when you save the config file and restart Codex</li> </ol>"},{"location":"THEME_CONFIG/#color-meanings","title":"Color Meanings","text":"<ul> <li>primary: Main accent color, used for important UI elements</li> <li>secondary: Secondary accent color</li> <li>background: Main background color</li> <li>foreground: Default text color</li> <li>border: Normal border color</li> <li>border_focused: Border color when element has focus</li> <li>selection: Background color for selected text</li> <li>cursor: Cursor color</li> <li>success: Success messages and indicators</li> <li>warning: Warning messages</li> <li>error: Error messages</li> <li>info: Informational messages</li> <li>text: Normal text</li> <li>text_dim: Dimmed/secondary text</li> <li>text_bright: Bright/emphasized text</li> <li>spinner: Loading spinner color</li> <li>progress: Progress bar color</li> </ul>"},{"location":"advanced/","title":"Advanced Topics","text":""},{"location":"advanced/#advanced","title":"Advanced","text":""},{"location":"advanced/#non-interactive-ci-mode","title":"Non-interactive / CI mode","text":"<p>Run Codex head-less in pipelines. Example GitHub Action step:</p> <pre><code>- name: Update changelog via Codex\n  run: |\n    npm install -g @openai/codex\n    export OPENAI_API_KEY=\"${{ secrets.OPENAI_KEY }}\"\n    codex exec --full-auto \"update CHANGELOG for next release\"\n</code></pre>"},{"location":"advanced/#resuming-non-interactive-sessions","title":"Resuming non-interactive sessions","text":"<p>You can resume a previous headless run to continue the same conversation context and append to the same rollout file.</p> <p>Interactive TUI equivalent:</p> <pre><code>codex resume             # picker\ncodex resume --last      # most recent\ncodex resume &lt;SESSION_ID&gt;\n</code></pre> <p>Compatibility:</p> <ul> <li>Latest source builds include <code>codex exec resume</code> (examples below).</li> <li>Current released CLI may not include this yet. If <code>codex exec --help</code> shows no <code>resume</code>, use the workaround in the next subsection.</li> </ul> <pre><code># Resume the most recent recorded session and run with a new prompt (source builds)\ncodex exec \"ship a release draft changelog\" resume --last\n\n# Alternatively, pass the prompt via stdin (source builds)\n# Note: omit the trailing '-' to avoid it being parsed as a SESSION_ID\necho \"ship a release draft changelog\" | codex exec resume --last\n\n# Or resume a specific session by id (UUID) (source builds)\ncodex exec resume 7f9f9a2e-1b3c-4c7a-9b0e-123456789abc \"continue the task\"\n</code></pre> <p>Notes:</p> <ul> <li>When using <code>--last</code>, Codex picks the newest recorded session; if none exist, it behaves like starting fresh.</li> <li>Resuming appends new events to the existing session file and maintains the same conversation id.</li> </ul>"},{"location":"advanced/#tracing-verbose-logging","title":"Tracing / verbose logging","text":"<p>Because Codex is written in Rust, it honors the <code>RUST_LOG</code> environment variable to configure its logging behavior.</p> <p>The TUI defaults to <code>RUST_LOG=codex_core=info,codex_tui=info</code> and log messages are written to <code>~/.code/log/codex-tui.log</code> (Code still reads the legacy <code>~/.codex/log/</code> path), so you can leave the following running in a separate terminal to monitor log messages as they are written:</p> <pre><code>tail -F ~/.code/log/codex-tui.log\n</code></pre> <p>When you also enable the CLI <code>--debug</code> flag, request/response JSON is partitioned into helper-specific folders under <code>~/.code/debug_logs/</code>. Expect subdirectories such as:</p> <ul> <li><code>auto/coordinator</code></li> <li><code>auto/observer/bootstrap</code></li> <li><code>auto/observer/cadence</code></li> <li><code>auto/observer/cross_check</code></li> <li><code>guided_terminal/agent_install_flow</code></li> <li><code>guided_terminal/upgrade_terminal_flow</code></li> <li><code>tui/rate_limit_refresh</code></li> <li><code>ui/theme_spinner</code></li> <li><code>ui/theme_builder</code></li> <li><code>cli/manual_prompt</code></li> </ul> <p>Tags become nested path components, so custom helpers appear alongside the existing timestamped filenames.</p> <p>By comparison, the non-interactive mode (<code>codex exec</code>) defaults to <code>RUST_LOG=error</code>, but messages are printed inline, so there is no need to monitor a separate file.</p> <p>See the Rust documentation on <code>RUST_LOG</code> for more information on the configuration options.</p>"},{"location":"advanced/#model-context-protocol-mcp","title":"Model Context Protocol (MCP)","text":"<p>The Codex CLI can be configured to leverage MCP servers by defining an <code>mcp_servers</code> section in <code>~/.code/config.toml</code> (Code will also read a legacy <code>~/.codex/config.toml</code>). It is intended to mirror how tools such as Claude and Cursor define <code>mcpServers</code> in their respective JSON config files, though the Codex format is slightly different since it uses TOML rather than JSON, e.g.:</p> <pre><code># IMPORTANT: the top-level key is `mcp_servers` rather than `mcpServers`.\n[mcp_servers.server-name]\ncommand = \"npx\"\nargs = [\"-y\", \"mcp-server\"]\nenv = { \"API_KEY\" = \"value\" }\n</code></pre>"},{"location":"advanced/#using-codex-as-an-mcp-server","title":"Using Codex as an MCP Server","text":"<p>[!TIP] It is somewhat experimental, but the Codex CLI can also be run as an MCP server via <code>codex mcp</code>. If you launch it with an MCP client such as <code>npx @modelcontextprotocol/inspector codex mcp</code> and send it a <code>tools/list</code> request, you will see that there is only one tool, <code>codex</code>, that accepts a grab-bag of inputs, including a catch-all <code>config</code> map for anything you might want to override. Feel free to play around with it and provide feedback via GitHub issues. </p>"},{"location":"authentication/","title":"Authentication","text":""},{"location":"authentication/#usage-based-billing-alternative-use-an-openai-api-key","title":"Usage-based billing alternative: Use an OpenAI API key","text":"<p>If you prefer to pay-as-you-go, you can still authenticate with your OpenAI API key by setting it as an environment variable:</p> <pre><code>export OPENAI_API_KEY=\"your-api-key-here\"\n</code></pre> <p>Alternatively, read from a file:</p> <pre><code>codex login --with-api-key &lt; my_key.txt\n</code></pre> <p>The legacy <code>--api-key</code> flag now exits with an error instructing you to use <code>--with-api-key</code> so that the key never appears in shell history or process listings.</p> <p>This key must, at minimum, have write access to the Responses API.</p>"},{"location":"authentication/#migrating-to-chatgpt-login-from-api-key","title":"Migrating to ChatGPT login from API key","text":"<p>If you've used the Codex CLI before with usage-based billing via an API key and want to switch to using your ChatGPT plan, follow these steps:</p> <ol> <li>Update the CLI and ensure <code>codex --version</code> is <code>0.20.0</code> or later</li> <li>Delete <code>~/.code/auth.json</code> (and remove the legacy <code>~/.codex/auth.json</code> if it exists; on Windows these live under <code>C:\\\\Users\\\\USERNAME\\\\.code\\\\auth.json</code> and <code>C:\\\\Users\\\\USERNAME\\\\.codex\\\\auth.json</code>)</li> <li>Run <code>codex login</code> again</li> </ol>"},{"location":"authentication/#forcing-a-specific-auth-method-advanced","title":"Forcing a specific auth method (advanced)","text":"<p>You can explicitly choose which authentication Codex should prefer when both are available.</p> <ul> <li>To always use your API key (even when ChatGPT auth exists), set:</li> </ul> <pre><code># ~/.code/config.toml (Code also reads legacy ~/.codex/config.toml)\npreferred_auth_method = \"apikey\"\n</code></pre> <p>Or override ad-hoc via CLI:</p> <pre><code>codex --config preferred_auth_method=\"apikey\"\n</code></pre> <ul> <li>To prefer ChatGPT auth (default), set:</li> </ul> <pre><code># ~/.code/config.toml (Code also reads legacy ~/.codex/config.toml)\npreferred_auth_method = \"chatgpt\"\n</code></pre> <p>Notes:</p> <ul> <li>When <code>preferred_auth_method = \"apikey\"</code> and an API key is available, the login screen is skipped.</li> <li>When <code>preferred_auth_method = \"chatgpt\"</code> (default), Codex prefers ChatGPT auth if present; if only an API key is present, it will use the API key. Certain account types may also require API-key mode.</li> <li>To check which auth method is being used during a session, use the <code>/status</code> command in the TUI.</li> </ul>"},{"location":"authentication/#project-env-safety-openai_api_key","title":"Project .env safety (OPENAI_API_KEY)","text":"<p>By default, Codex will no longer read <code>OPENAI_API_KEY</code> or <code>AZURE_OPENAI_API_KEY</code> from a project\u2019s local <code>.env</code> file.</p> <p>Why: many repos include an API key in <code>.env</code> for unrelated tooling, which could cause Codex to silently use the API key instead of your ChatGPT plan in that folder.</p> <p>What still works:</p> <ul> <li><code>~/.code/.env</code> (or <code>~/.codex/.env</code>) is loaded first and may contain your <code>OPENAI_API_KEY</code> for global use.</li> <li>A shell-exported <code>OPENAI_API_KEY</code> is honored.</li> </ul> <p>Project <code>.env</code> provider keys are always ignored \u2014 there is no opt\u2011in.</p> <p>UI clarity:</p> <ul> <li>When Codex is using an API key, the chat footer shows a bold \u201cAuth: API key\u201d badge so it\u2019s obvious which mode you\u2019re in.</li> </ul>"},{"location":"authentication/#connecting-on-a-headless-machine","title":"Connecting on a \"Headless\" Machine","text":"<p>Today, the login process entails running a server on <code>localhost:1455</code>. If you are on a \"headless\" server, such as a Docker container or are <code>ssh</code>'d into a remote machine, loading <code>localhost:1455</code> in the browser on your local machine will not automatically connect to the webserver running on the headless machine, so you must use one of the following workarounds:</p>"},{"location":"authentication/#authenticate-locally-and-copy-your-credentials-to-the-headless-machine","title":"Authenticate locally and copy your credentials to the \"headless\" machine","text":"<p>The easiest solution is likely to run through the <code>codex login</code> process on your local machine such that <code>localhost:1455</code> is accessible in your web browser. When you complete the authentication process, an <code>auth.json</code> file should be available at <code>$CODE_HOME/auth.json</code> (defaults to <code>~/.code/auth.json</code>; Code will still read <code>$CODEX_HOME</code>/<code>~/.codex/auth.json</code> if present).</p> <p>Because the <code>auth.json</code> file is not tied to a specific host, once you complete the authentication flow locally, you can copy the <code>$CODEX_HOME/auth.json</code> file to the headless machine and then <code>codex</code> should \"just work\" on that machine. Note to copy a file to a Docker container, you can do:</p> <pre><code># substitute MY_CONTAINER with the name or id of your Docker container:\nCONTAINER_HOME=$(docker exec MY_CONTAINER printenv HOME)\ndocker exec MY_CONTAINER mkdir -p \"$CONTAINER_HOME/.code\"\ndocker cp auth.json MY_CONTAINER:\"$CONTAINER_HOME/.code/auth.json\"\n</code></pre> <p>whereas if you are <code>ssh</code>'d into a remote machine, you likely want to use <code>scp</code>:</p> <pre><code>ssh user@remote 'mkdir -p ~/.code'\nscp ~/.code/auth.json user@remote:~/.code/auth.json\n</code></pre> <p>or try this one-liner:</p> <pre><code>ssh user@remote 'mkdir -p ~/.code &amp;&amp; cat &gt; ~/.code/auth.json' &lt; ~/.code/auth.json\n</code></pre>"},{"location":"authentication/#connecting-through-vps-or-remote","title":"Connecting through VPS or remote","text":"<p>If you run Codex on a remote machine (VPS/server) without a local browser, the login helper starts a server on <code>localhost:1455</code> on the remote host. To complete login in your local browser, forward that port to your machine before starting the login flow:</p> <pre><code># From your local machine\nssh -L 1455:localhost:1455 &lt;user&gt;@&lt;remote-host&gt;\n</code></pre> <p>Then, in that SSH session, run <code>codex</code> and select \"Sign in with ChatGPT\". When prompted, open the printed URL (it will be <code>http://localhost:1455/...</code>) in your local browser. The traffic will be tunneled to the remote server.</p>"},{"location":"auto-drive-phase-migration-TODO/","title":"Auto Drive Phase Migration TODO","text":""},{"location":"auto-drive-phase-migration-TODO/#phase-invariants-snapshot","title":"Phase Invariants Snapshot","text":"<ul> <li><code>Idle</code> \u2014 Auto Drive inactive; no pending coordinator/diagnostics/review; countdown cleared.</li> <li><code>AwaitingGoalEntry</code> \u2014 Auto Drive inactive; goal entry composer visible; legacy <code>awaiting_goal_input</code> flag collapses into this variant.</li> <li><code>Launching</code> \u2014 Preparing first turn; mirrors <code>Idle</code> legacy booleans until launch success/failure.</li> <li><code>Active</code> \u2014 Run active with no pending gates; diagnostics/review/manual/edit/transient flags cleared.</li> <li><code>PausedManual { resume_after_submit, bypass_next_submit }</code> \u2014 Run active; manual editor visible; <code>resume_after_manual_submit</code> mirrors payload and bypass flag controls coordinator auto-submit.</li> <li><code>AwaitingCoordinator { prompt_ready }</code> \u2014 Run active; prompt staged; coordinator waiting true regardless of <code>prompt_ready</code>; countdown enabled when legacy auto-submit applies.</li> <li><code>AwaitingDiagnostics</code> \u2014 Awaiting model response (streaming); coordinator waiting false; review/manual flags cleared.</li> <li><code>AwaitingReview { diagnostics_pending }</code> \u2014 Awaiting user review; diagnostics chip toggled by payload; other waits cleared.</li> <li> <p><code>TransientRecovery { backoff_ms }</code> \u2014 Backoff between restart attempts; transient wait flag true; coordinator/manual/review cleared.</p> </li> <li> <p>Remove legacy fields from <code>AutoDriveController</code> that duplicate phase state (<code>active</code>, <code>awaiting_submission</code>, <code>waiting_for_response</code>, <code>paused_for_manual_edit</code>, <code>resume_after_manual_submit</code>, <code>waiting_for_review</code>, <code>waiting_for_transient_recovery</code>, <code>coordinator_waiting</code>).</p> </li> <li>Update remaining TUI call sites (outside ChatWidget hot paths) to use controller helpers (<code>is_active</code>, <code>is_paused_manual</code>, <code>resume_after_submit</code>, <code>awaiting_coordinator_submit</code>, <code>awaiting_review</code>, <code>in_transient_recovery</code>).</li> <li>Replace test harness helpers in <code>tui/tests</code> that mutate legacy flags with phase-aware helpers or controller transitions.</li> <li>Ensure ESC routing (<code>describe_esc_context</code>, <code>execute_esc_intent</code>) exclusively inspects <code>AutoRunPhase</code>/helpers and removes stop-gap flag checks.</li> <li>Add unit/VT100 coverage for manual pause/resume and transient recovery sequences under the new phase helpers.</li> <li>Acceptance: <code>./build-fast.sh</code> green; no direct reads/writes of legacy flags across the repo; ESC flows verified in snapshot tests.</li> </ul>"},{"location":"auto-drive-state-inventory/","title":"Auto Drive State Inventory","text":"<p>This document catalogs every <code>auto_state</code> field access across the TUI and controller so we can migrate toward single-phase semantics without missing any flag interactions.</p>"},{"location":"auto-drive-state-inventory/#sources-scanned","title":"Sources Scanned","text":"<ul> <li><code>code-rs/tui/src/chatwidget.rs</code></li> <li><code>code-rs/tui/src/bottom_pane/auto_coordinator_view.rs</code></li> <li><code>code-rs/tui/src/bottom_pane/auto_drive_settings_view.rs</code></li> <li><code>code-rs/tui/src/bottom_pane/paste_burst.rs</code></li> <li><code>code-rs/tui/src/chatwidget/smoke_helpers.rs</code></li> <li><code>code-rs/code-auto-drive-core/src/controller.rs</code></li> </ul> <p>Each entry below lists read vs. write occurrences (line numbers and snippets). Counts help highlight high-traffic fields.</p>"},{"location":"auto-drive-state-inventory/#field-classification","title":"Field Classification","text":"Field Category Notes <code>active</code> Phase control Primary on/off latch for Auto Drive; should collapse into <code>AutoRunPhase::Active</code>/<code>Idle</code>. <code>awaiting_submission</code> Phase control Drives countdown and gating for prompt submission; redundant with <code>AutoRunPhase::AwaitingCoordinator</code>. <code>waiting_for_response</code> Phase control Distinguishes coordinator wait vs. live-streaming response; overlaps with <code>AwaitingDiagnostics</code>. <code>paused_for_manual_edit</code> Phase control Legacy manual-edit gate; duplicative of <code>AutoRunPhase::PausedManual</code>. <code>resume_after_manual_submit</code> Phase control Remembers whether manual submits should resume automatically; belongs inside <code>PausedManual</code> payload. <code>waiting_for_review</code> Phase control Tracks post-turn review gating; mirrors <code>AutoRunPhase::AwaitingReview</code>. <code>waiting_for_transient_recovery</code> Phase control Marks exponential backoff windows; mirrors <code>AutoRunPhase::TransientRecovery</code>. <code>coordinator_waiting</code> UI view data Indicates coordinator prompt handshake state; used for progress copy and hint toggles."},{"location":"auto-drive-state-inventory/#active-phase-control","title":"<code>active</code> (phase control)","text":"<ul> <li>Reads (8)</li> <li><code>code-rs/tui/src/chatwidget.rs:14153</code> \u2014 guard before mutating review UI while idle.</li> <li><code>\u2026:14307</code> \u2014 skip coordinator pane when idle.</li> <li><code>\u2026:14587</code> \u2014 hide goal banner unless active or awaiting goal.</li> <li><code>\u2026:14613</code> \u2014 bail on streaming renderer unless active.</li> <li><code>\u2026:14646</code> \u2014 hide progress plaque while idle.</li> <li><code>\u2026:14675</code> \u2014 stop summary aggregation if run stopped.</li> <li><code>\u2026:14762</code> \u2014 guard reasoning title updates.</li> <li><code>\u2026:23380</code> \u2014 keep review shortcuts disabled when idle.</li> <li>Writes (10)</li> <li><code>code-rs/tui/src/chatwidget.rs:21879</code> \u2014 smoke helper seeds active state for tests.</li> <li>Additional nine test helpers (<code>\u2026:22020</code>, <code>\u2026:22047</code>, <code>\u2026:22128</code>, <code>\u2026:22194</code>, <code>\u2026:22250</code>, <code>\u2026:22304</code>, <code>\u2026:22333</code>, <code>\u2026:22394</code>, <code>chatwidget/smoke_helpers.rs:311</code>).</li> <li>Controller mirror updates appear in <code>sync_booleans_from_phase()</code> (<code>code-auto-drive-core/src/controller.rs:263</code>, <code>273</code>, <code>283</code>, <code>293</code>, <code>303</code>, <code>313</code>, <code>323</code>, <code>479</code>, <code>510</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#awaiting_submission-phase-control","title":"<code>awaiting_submission</code> (phase control)","text":"<ul> <li>Reads (5)</li> <li><code>code-rs/tui/src/chatwidget.rs:14570</code> \u2014 decides whether to elide ellipsis on summary lines during pending submit.</li> <li>Controller helper logic uses it (<code>code-auto-drive-core/src/controller.rs:678</code>, <code>699</code>, <code>774</code>, <code>841</code>).</li> <li>Writes (9)</li> <li>All originate from controller transitions (<code>controller.rs:264</code>, <code>274</code>, <code>284</code>, <code>294</code>, <code>304</code>, <code>314</code>, <code>324</code>, <code>568</code>, <code>655</code>).</li> <li>No TUI caller writes directly.</li> </ul>"},{"location":"auto-drive-state-inventory/#waiting_for_response-phase-control","title":"<code>waiting_for_response</code> (phase control)","text":"<ul> <li>Reads (10)</li> <li>Reactive UI checks (<code>chatwidget.rs:13215</code>, <code>14153</code>, <code>14403</code>, <code>14434</code>, <code>14514</code>, <code>14528</code>, <code>14773</code>).</li> <li>Test assertions (<code>chatwidget.rs:22037</code>, <code>22115</code>, <code>22184</code>).</li> <li>Writes (10)</li> <li>TUI clears after finalization (<code>chatwidget.rs:13557</code>).</li> <li>Smoke helpers and fixtures seed state (<code>chatwidget.rs:22021</code>, <code>22049</code>, <code>22130</code>, <code>22396</code>).</li> <li>Controller toggles across transitions (<code>controller.rs:265</code>, <code>275</code>, <code>285</code>, <code>295</code>, <code>305</code>, <code>315</code>, <code>325</code>, <code>336</code>, <code>498</code>, <code>566</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#paused_for_manual_edit-phase-control","title":"<code>paused_for_manual_edit</code> (phase control)","text":"<ul> <li>Reads (6)</li> <li>Manual editor banner (<code>chatwidget.rs:14369</code>).</li> <li>Controller helper guards (<code>controller.rs:678</code>, <code>700</code>, <code>775</code>, <code>830</code>, <code>841</code>).</li> <li>Writes (8)</li> <li>Solely controller-managed (<code>controller.rs:267</code>, <code>277</code>, <code>287</code>, <code>297</code>, <code>307</code>, <code>317</code>, <code>327</code>, <code>569</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#resume_after_manual_submit-phase-control","title":"<code>resume_after_manual_submit</code> (phase control)","text":"<ul> <li>Reads (1)</li> <li>Manual resume decision (<code>controller.rs:836</code>).</li> <li>Writes (9)</li> <li>Controller clears or copies flag during transitions (<code>controller.rs:268</code>, <code>278</code>, <code>288</code>, <code>298</code>, <code>308</code>, <code>318</code>, <code>328</code>, <code>356</code>, <code>570</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#waiting_for_review-phase-control","title":"<code>waiting_for_review</code> (phase control)","text":"<ul> <li>Reads (13)</li> <li>Review UI gating and tests (<code>chatwidget.rs:13819</code>, <code>14182</code>, <code>14378</code>, <code>22032</code>, <code>22093</code>, <code>22099</code>, <code>22174</code>, <code>22180</code>, <code>22221</code>, <code>22437</code>, <code>22452</code>, <code>23380</code>).</li> <li>Phase helper fallback (<code>controller.rs:845</code>).</li> <li>Writes (9)</li> <li>Controller transitions (<code>controller.rs:269</code>, <code>279</code>, <code>289</code>, <code>299</code>, <code>309</code>, <code>319</code>, <code>329</code>, <code>571</code>).</li> <li>ChatWidget clears on forced stop (<code>chatwidget.rs:23386</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#waiting_for_transient_recovery-phase-control","title":"<code>waiting_for_transient_recovery</code> (phase control)","text":"<ul> <li>Reads (1)</li> <li>Phase helper fallback (<code>controller.rs:850</code>).</li> <li>Writes (9)</li> <li>ChatWidget clears before scheduling restart (<code>chatwidget.rs:13456</code>).</li> <li>Controller transitions (<code>controller.rs:270</code>, <code>280</code>, <code>290</code>, <code>300</code>, <code>310</code>, <code>320</code>, <code>330</code>, <code>383</code>, <code>565</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#coordinator_waiting-ui-view-data","title":"<code>coordinator_waiting</code> (UI view data)","text":"<ul> <li>Reads (3)</li> <li>Coordinator progress hints (<code>chatwidget.rs:14403</code>, <code>14434</code>, <code>14529</code>).</li> <li>Writes (12)</li> <li>ChatWidget resets on completion (<code>chatwidget.rs:13400</code>, <code>13558</code>).</li> <li>Controller mirrors during transitions (<code>controller.rs:266</code>, <code>276</code>, <code>286</code>, <code>296</code>, <code>306</code>, <code>316</code>, <code>326</code>, <code>340</code>, <code>499</code>, <code>567</code>).</li> </ul>"},{"location":"auto-drive-state-inventory/#current-boolean-combinations","title":"Current Boolean Combinations","text":"<ul> <li><code>waiting_for_response &amp;&amp; !coordinator_waiting</code> \u2014 drives \u201cmodel is thinking\u201d messaging and hides coordinator countdown. (<code>chatwidget.rs:14403</code>)</li> <li><code>awaiting_submission &amp;&amp; !paused_for_manual_edit</code> \u2014 determines when countdown and auto-submit button should be live. (<code>controller.rs:678</code>, <code>chatwidget.rs:14501</code>)</li> <li><code>active &amp;&amp; waiting_for_review</code> \u2014 blocks manual resume until review flow resolves. (<code>chatwidget.rs:22093</code>)</li> <li><code>is_paused_manual() &amp;&amp; should_bypass_coordinator_next_submit()</code> \u2014 keeps manual edit overlay while skipping coordinator prompt; helper now derives directly from <code>AutoRunPhase::PausedManual { bypass_next_submit }</code>. (<code>chatwidget.rs:14045</code> vicinity)</li> <li><code>in_transient_recovery() &amp;&amp; waiting_for_transient_recovery</code> \u2014 redundant gating around restart timer, still split between phase helper and boolean. (<code>controller.rs:850</code>)</li> </ul> <p>These combinations highlight where the new <code>AutoRunPhase</code> variants must carry the data currently modeled via multiple booleans, allowing legacy mirrors to be dropped once consumers migrate.</p>"},{"location":"auto-qa/","title":"Auto QA Orchestration","text":""},{"location":"auto-qa/#overview","title":"Overview","text":"<ul> <li>Auto Drive now runs the coordinator and observer threads in parallel.</li> <li>The coordinator handles CLI execution and agent planning.</li> <li>QA orchestration owns review cadence, observer bootstrap, and the   forced cross-check before completion.</li> </ul>"},{"location":"auto-qa/#observer-lifecycle","title":"Observer Lifecycle","text":"<ol> <li>Bootstrap \u2013 The ChatWidget starts the observer worker and sends a    bootstrap prompt. The observer performs a read-only scan, records a    baseline summary, and emits <code>AppEvent::AutoObserverReady</code>. Cadence    triggers remain paused until this event arrives.</li> <li>Delta ingestion \u2013 After bootstrap only new user and assistant    turns (no reasoning) are forwarded. ChatWidget tracks    <code>observer_history.last_sent_index</code> so the observer never replays the    full transcript.</li> <li>Thinking stream \u2013 During bootstrap, cadence, and cross-check    prompts the observer streams reasoning via    <code>AppEvent::AutoObserverThinking</code>. ChatWidget stores each frame in    <code>ObserverHistory</code> and labels it in the Auto Threads overlay    (Bootstrap / Observer / Cross-check thinking).</li> <li>Cadence checks \u2013 On the configured cadence the observer reviews    the latest delta. Failures push banners such as \u201cObserver guidance:    \u2026\u201d and can replace the CLI prompt; successes only update telemetry.</li> <li>Cross-check reuse \u2013 When the coordinator reports    <code>finish_success</code>, it slices the observer transcript starting at    <code>observer_history.bootstrap_len</code> and issues <code>BeginCrossCheck</code>. The    observer reuses that slice, runs with a stricter tool policy, and    only if the cross-check passes does the coordinator forward the    pending decision. Failures convert to a restart banner and abort    completion.</li> </ol>"},{"location":"auto-qa/#tool-policies-by-mode","title":"Tool Policies by Mode","text":"<ul> <li>Bootstrap (<code>ObserverMode::Bootstrap</code>) \u2013 Read-only tools (web   search only) so the observer can assess the repository without   modifying files.</li> <li>Cadence (<code>ObserverMode::Cadence</code>) \u2013 Limited tools (web search) for   light guidance while the run is in flight.</li> <li>Cross-check (<code>ObserverMode::CrossCheck</code>) \u2013 Full audit tools (local   shell plus web search) so the observer can rerun commands and verify   results before finish.</li> </ul>"},{"location":"auto-qa/#ui-and-history","title":"UI and History","text":"<ul> <li><code>ObserverHistory</code> persists observer exchanges and reasoning frames.   Auto Threads overlay entries now include the observer mode in their   label.</li> <li>Banners surface milestones: \u201cObserver bootstrap completed.\u201d,   \u201cCross-check in progress.\u201d, \u201cCross-check successful.\u201d Failures include   guidance text or restart notices.</li> </ul>"},{"location":"auto-qa/#teardown-and-restart","title":"Teardown and Restart","text":"<ul> <li><code>auto_stop</code> and automatic restarts clear <code>ObserverHistory</code>, reset the   readiness flag, and send <code>ResetObserver</code> so the coordinator rebuilds   state before the next run.</li> <li>The QA orchestrator handle is stopped alongside the observer. All   cadence and cross-check state is reconstructed on the next launch.</li> </ul>"},{"location":"auto-qa/#qa-orchestrator-responsibilities","title":"QA Orchestrator Responsibilities","text":"<ul> <li>Emit <code>AppEvent::AutoQaUpdate { note }</code> every cadence window (default   three turns).</li> <li>Emit <code>AppEvent::AutoReviewRequest { summary }</code> when diff-bearing turns   satisfy the review cooldown. These events are now the sole trigger for   automated reviews.</li> <li>Reset cadence state on shutdown and send a final review request if   diffs remain when automation stops.</li> </ul>"},{"location":"auto-qa/#environment-knobs","title":"Environment Knobs","text":"<ul> <li><code>CODE_QA_CADENCE</code> \u2014 number of turns between observer cadence updates   (default three).</li> <li><code>CODE_QA_REVIEW_COOLDOWN_TURNS</code> \u2014 diff-bearing turns before   <code>AutoReviewRequest</code> fires (default one).</li> <li>(Future) expose tool-policy overrides if operators need to restrict   cross-check access.</li> </ul>"},{"location":"auto-qa/#future-work","title":"Future Work","text":"<ul> <li>Collapse multiple QA toggles into a single <code>qa_automation_enabled</code>   flag in <code>AutoDriveSettings</code>.</li> <li>Expand observer regression coverage when the VT100 harness exposes   observer fixtures (TODO).</li> </ul>"},{"location":"config/","title":"Config","text":"<p>Codex supports several mechanisms for setting config values:</p> <ul> <li>Config-specific command-line flags, such as <code>--model o3</code> (highest precedence).</li> <li>A generic <code>-c</code>/<code>--config</code> flag that takes a <code>key=value</code> pair, such as <code>--config model=\"o3\"</code>.</li> <li>The key can contain dots to set a value deeper than the root, e.g. <code>--config model_providers.openai.wire_api=\"chat\"</code>.</li> <li>For consistency with <code>config.toml</code>, values are a string in TOML format rather than JSON format, so use <code>key='{a = 1, b = 2}'</code> rather than <code>key='{\"a\": 1, \"b\": 2}'</code>.<ul> <li>The quotes around the value are necessary, as without them your shell would split the config argument on spaces, resulting in <code>codex</code> receiving <code>-c key={a</code> with (invalid) additional arguments <code>=</code>, <code>1,</code>, <code>b</code>, <code>=</code>, <code>2}</code>.</li> </ul> </li> <li>Values can contain any TOML object, such as <code>--config shell_environment_policy.include_only='[\"PATH\", \"HOME\", \"USER\"]'</code>.</li> <li>If <code>value</code> cannot be parsed as a valid TOML value, it is treated as a string value. This means that <code>-c model='\"o3\"'</code> and <code>-c model=o3</code> are equivalent.<ul> <li>In the first case, the value is the TOML string <code>\"o3\"</code>, while in the second the value is <code>o3</code>, which is not valid TOML and therefore treated as the TOML string <code>\"o3\"</code>.</li> <li>Because quotes are interpreted by one's shell, <code>-c key=\"true\"</code> will be correctly interpreted in TOML as <code>key = true</code> (a boolean) and not <code>key = \"true\"</code> (a string). If for some reason you needed the string <code>\"true\"</code>, you would need to use <code>-c key='\"true\"'</code> (note the two sets of quotes).</li> </ul> </li> <li>The <code>$CODE_HOME/config.toml</code> configuration file. <code>CODE_HOME</code> defaults to <code>~/.code</code>; Code also reads from <code>$CODEX_HOME</code>/<code>~/.codex</code> for backwards compatibility but only writes to <code>~/.code</code>. (Logs and other state use the same directory.)</li> </ul> <p>Both the <code>--config</code> flag and the <code>config.toml</code> file support the following options:</p>"},{"location":"config/#model","title":"model","text":"<p>The model that Codex should use.</p> <pre><code>model = \"o3\"  # overrides the default of \"gpt-5\"\n</code></pre>"},{"location":"config/#model_providers","title":"model_providers","text":"<p>This option lets you override and amend the default set of model providers bundled with Codex. This value is a map where the key is the value to use with <code>model_provider</code> to select the corresponding provider.</p> <p>For example, if you wanted to add a provider that uses the OpenAI 4o model via the chat completions API, then you could add the following configuration:</p> <pre><code># Recall that in TOML, root keys must be listed before tables.\nmodel = \"gpt-4o\"\nmodel_provider = \"openai-chat-completions\"\n\n[model_providers.openai-chat-completions]\n# Name of the provider that will be displayed in the Codex UI.\nname = \"OpenAI using Chat Completions\"\n# The path `/chat/completions` will be amended to this URL to make the POST\n# request for the chat completions.\nbase_url = \"https://api.openai.com/v1\"\n# If `env_key` is set, identifies an environment variable that must be set when\n# using Codex with this provider. The value of the environment variable must be\n# non-empty and will be used in the `Bearer TOKEN` HTTP header for the POST request.\nenv_key = \"OPENAI_API_KEY\"\n# Valid values for wire_api are \"chat\" and \"responses\". Defaults to \"chat\" if omitted.\nwire_api = \"chat\"\n# If necessary, extra query params that need to be added to the URL.\n# See the Azure example below.\nquery_params = {}\n</code></pre> <p>Note this makes it possible to use Codex CLI with non-OpenAI models, so long as they use a wire API that is compatible with the OpenAI chat completions API. For example, you could define the following provider to use Codex CLI with Ollama running locally:</p> <pre><code>[model_providers.ollama]\nname = \"Ollama\"\nbase_url = \"http://localhost:11434/v1\"\n</code></pre> <p>Or a third-party provider (using a distinct environment variable for the API key):</p> <pre><code>[model_providers.mistral]\nname = \"Mistral\"\nbase_url = \"https://api.mistral.ai/v1\"\nenv_key = \"MISTRAL_API_KEY\"\n</code></pre> <p>It is also possible to configure a provider to include extra HTTP headers with a request. These can be hardcoded values (<code>http_headers</code>) or values read from environment variables (<code>env_http_headers</code>):</p> <pre><code>[model_providers.example]\n# name, base_url, ...\n\n# This will add the HTTP header `X-Example-Header` with value `example-value`\n# to each request to the model provider.\nhttp_headers = { \"X-Example-Header\" = \"example-value\" }\n\n# This will add the HTTP header `X-Example-Features` with the value of the\n# `EXAMPLE_FEATURES` environment variable to each request to the model provider\n# _if_ the environment variable is set and its value is non-empty.\nenv_http_headers = { \"X-Example-Features\" = \"EXAMPLE_FEATURES\" }\n</code></pre>"},{"location":"config/#azure-model-provider-example","title":"Azure model provider example","text":"<p>Note that Azure requires <code>api-version</code> to be passed as a query parameter, so be sure to specify it as part of <code>query_params</code> when defining the Azure provider:</p> <pre><code>[model_providers.azure]\nname = \"Azure\"\n# Make sure you set the appropriate subdomain for this URL.\nbase_url = \"https://YOUR_PROJECT_NAME.openai.azure.com/openai\"\nenv_key = \"AZURE_OPENAI_API_KEY\"  # Or \"OPENAI_API_KEY\", whichever you use.\nquery_params = { api-version = \"2025-04-01-preview\" }\nwire_api = \"responses\"\n</code></pre> <p>Export your key before launching Codex: <code>export AZURE_OPENAI_API_KEY=\u2026</code></p>"},{"location":"config/#per-provider-network-tuning","title":"Per-provider network tuning","text":"<p>The following optional settings control retry behaviour and streaming idle timeouts per model provider. They must be specified inside the corresponding <code>[model_providers.&lt;id&gt;]</code> block in <code>config.toml</code>. (Older releases accepted top\u2011level keys; those are now ignored.)</p> <p>Example:</p> <pre><code>[model_providers.openai]\nname = \"OpenAI\"\nbase_url = \"https://api.openai.com/v1\"\nenv_key = \"OPENAI_API_KEY\"\n# network tuning overrides (all optional; falls back to built\u2011in defaults)\nrequest_max_retries = 4            # retry failed HTTP requests\nstream_max_retries = 10            # retry dropped SSE streams\nstream_idle_timeout_ms = 300000    # 5m idle timeout\n</code></pre>"},{"location":"config/#request_max_retries","title":"request_max_retries","text":"<p>How many times Codex will retry a failed HTTP request to the model provider. Defaults to <code>4</code>.</p>"},{"location":"config/#stream_max_retries","title":"stream_max_retries","text":"<p>Number of times Codex will attempt to reconnect when a streaming response is interrupted. Defaults to <code>5</code>.</p>"},{"location":"config/#stream_idle_timeout_ms","title":"stream_idle_timeout_ms","text":"<p>How long Codex will wait for activity on a streaming response before treating the connection as lost. Defaults to <code>300_000</code> (5 minutes).</p>"},{"location":"config/#model_provider","title":"model_provider","text":"<p>Identifies which provider to use from the <code>model_providers</code> map. Defaults to <code>\"openai\"</code>. You can override the <code>base_url</code> for the built-in <code>openai</code> provider via the <code>OPENAI_BASE_URL</code> environment variable and force the wire protocol (<code>\"responses\"</code> or <code>\"chat\"</code>) with <code>OPENAI_WIRE_API</code>.</p> <p>Note that if you override <code>model_provider</code>, then you likely want to override <code>model</code>, as well. For example, if you are running ollama with Mistral locally, then you would need to add the following to your config in addition to the new entry in the <code>model_providers</code> map:</p> <pre><code>model_provider = \"ollama\"\nmodel = \"mistral\"\n</code></pre>"},{"location":"config/#approval_policy","title":"approval_policy","text":"<p>Determines when the user should be prompted to approve whether Codex can execute a command:</p> <pre><code># Codex has hardcoded logic that defines a set of \"trusted\" commands.\n# Setting the approval_policy to `untrusted` means that Codex will prompt the\n# user before running a command not in the \"trusted\" set.\n#\n# See https://github.com/openai/codex/issues/1260 for the plan to enable\n# end-users to define their own trusted commands.\napproval_policy = \"untrusted\"\n</code></pre> <p>If you want to be notified whenever a command fails, use \"on-failure\":</p> <pre><code># If the command fails when run in the sandbox, Codex asks for permission to\n# retry the command outside the sandbox.\napproval_policy = \"on-failure\"\n</code></pre> <p>If you want the model to run until it decides that it needs to ask you for escalated permissions, use \"on-request\":</p> <pre><code># The model decides when to escalate\napproval_policy = \"on-request\"\n</code></pre> <p>Alternatively, you can have the model run until it is done, and never ask to run a command with escalated permissions:</p> <pre><code># User is never prompted: if the command fails, Codex will automatically try\n# something out. Note the `exec` subcommand always uses this mode.\napproval_policy = \"never\"\n</code></pre>"},{"location":"config/#agents","title":"agents","text":"<p>Use <code>[[agents]]</code> blocks to register additional CLI programs that Codex can launch as peers. Each block maps a short <code>name</code> (referenced elsewhere in the config) to the command to execute, optional default flags, and environment variables.</p> <p>Note: Built-in model slugs (for example <code>code-gpt-5-codex</code>, <code>claude-sonnet-4.5</code>) automatically inject the correct <code>--model</code> or <code>-m</code> flag. To avoid conflicting arguments, Codex strips any <code>--model</code>/<code>-m</code> flags you place in <code>args</code>, <code>args_read_only</code>, or <code>args_write</code> before launching the agent. If you need a new model variant, add a slug in <code>code-rs/core/src/agent_defaults.rs</code> (or set an environment variable consumed by the CLI) rather than pinning the flag here.</p> <pre><code>[[agents]]\nname = \"context-collector\"\ncommand = \"gemini\"\nenabled = true\nread-only = true\ndescription = \"Gemini long-context helper that summarizes large repositories\"\nargs = [\"-y\"]\nenv = { GEMINI_API_KEY = \"...\" }\n</code></pre> <p>When <code>enabled = true</code>, the agent is surfaced in the TUI picker and any sub-agent commands that reference it. Setting <code>read-only = true</code> forces the agent to request approval before modifying files even if the primary session permits writes.</p>"},{"location":"config/#profiles","title":"profiles","text":"<p>A profile is a collection of configuration values that can be set together. Multiple profiles can be defined in <code>config.toml</code> and you can specify the one you want to use at runtime via the <code>--profile</code> flag.</p> <p>Here is an example of a <code>config.toml</code> that defines multiple profiles:</p> <pre><code>model = \"o3\"\napproval_policy = \"untrusted\"\ndisable_response_storage = false\n\n# Setting `profile` is equivalent to specifying `--profile o3` on the command\n# line, though the `--profile` flag can still be used to override this value.\nprofile = \"o3\"\n\n[model_providers.openai-chat-completions]\nname = \"OpenAI using Chat Completions\"\nbase_url = \"https://api.openai.com/v1\"\nenv_key = \"OPENAI_API_KEY\"\nwire_api = \"chat\"\n\n[profiles.o3]\nmodel = \"o3\"\nmodel_provider = \"openai\"\napproval_policy = \"never\"\nmodel_reasoning_effort = \"high\"\nmodel_reasoning_summary = \"detailed\"\n\n[profiles.gpt3]\nmodel = \"gpt-3.5-turbo\"\nmodel_provider = \"openai-chat-completions\"\n\n[profiles.zdr]\nmodel = \"o3\"\nmodel_provider = \"openai\"\napproval_policy = \"on-failure\"\ndisable_response_storage = true\n</code></pre> <p>Users can specify config values at multiple levels. Order of precedence is as follows:</p> <ol> <li>custom command-line argument, e.g., <code>--model o3</code></li> <li>as part of a profile, where the <code>--profile</code> is specified via a CLI (or in the config file itself)</li> <li>as an entry in <code>config.toml</code>, e.g., <code>model = \"o3\"</code></li> <li>the default value that comes with Codex CLI (i.e., Codex CLI defaults to <code>gpt-5</code>)</li> </ol>"},{"location":"config/#model_reasoning_effort","title":"model_reasoning_effort","text":"<p>If the selected model is known to support reasoning (for example: <code>o3</code>, <code>o4-mini</code>, <code>codex-*</code>, <code>gpt-5</code>, <code>gpt-5-codex</code>), reasoning is enabled by default when using the Responses API. As explained in the OpenAI Platform documentation, this can be set to:</p> <ul> <li><code>\"minimal\"</code></li> <li><code>\"low\"</code></li> <li><code>\"medium\"</code> (default)</li> <li><code>\"high\"</code></li> </ul> <p>Note: to minimize reasoning, choose <code>\"minimal\"</code>.</p>"},{"location":"config/#model_reasoning_summary","title":"model_reasoning_summary","text":"<p>If the model name starts with <code>\"o\"</code> (as in <code>\"o3\"</code> or <code>\"o4-mini\"</code>) or <code>\"codex\"</code>, reasoning is enabled by default when using the Responses API. As explained in the OpenAI Platform documentation, this can be set to:</p> <ul> <li><code>\"auto\"</code> (default)</li> <li><code>\"concise\"</code></li> <li><code>\"detailed\"</code></li> </ul> <p>To disable reasoning summaries, set <code>model_reasoning_summary</code> to <code>\"none\"</code> in your config:</p> <pre><code>model_reasoning_summary = \"none\"  # disable reasoning summaries\n</code></pre>"},{"location":"config/#model_verbosity","title":"model_verbosity","text":"<p>Controls output length/detail on GPT\u20115 family models when using the Responses API. Supported values:</p> <ul> <li><code>\"low\"</code></li> <li><code>\"medium\"</code> (default when omitted)</li> <li><code>\"high\"</code></li> </ul> <p>When set, Codex includes a <code>text</code> object in the request payload with the configured verbosity, for example: <code>\"text\": { \"verbosity\": \"low\" }</code>.</p> <p>Example:</p> <pre><code>model = \"gpt-5\"\nmodel_verbosity = \"low\"\n</code></pre> <p>Note: This applies only to providers using the Responses API. Chat Completions providers are unaffected.</p>"},{"location":"config/#model_supports_reasoning_summaries","title":"model_supports_reasoning_summaries","text":"<p>By default, <code>reasoning</code> is only set on requests to OpenAI models that are known to support them. To force <code>reasoning</code> to set on requests to the current model, you can force this behavior by setting the following in <code>config.toml</code>:</p> <pre><code>model_supports_reasoning_summaries = true\n</code></pre>"},{"location":"config/#sandbox_mode","title":"sandbox_mode","text":"<p>Codex executes model-generated shell commands inside an OS-level sandbox.</p> <p>In most cases you can pick the desired behaviour with a single option:</p> <pre><code># same as `--sandbox read-only`\nsandbox_mode = \"read-only\"\n</code></pre> <p>The default policy is <code>read-only</code>, which means commands can read any file on disk, but attempts to write a file or access the network will be blocked.</p> <p>A more relaxed policy is <code>workspace-write</code>. When specified, the current working directory for the Codex task will be writable (as well as <code>$TMPDIR</code> on macOS). Note that the CLI defaults to using the directory where it was spawned as <code>cwd</code>, though this can be overridden using <code>--cwd/-C</code>.</p> <p>Historically, Codex allowed writes inside the top\u2011level <code>.git/</code> folder when using <code>workspace-write</code>. That permissive behavior is the default again. If you want to protect <code>.git</code> under <code>workspace-write</code>, you can opt out via <code>[sandbox_workspace_write].allow_git_writes = false</code>.</p> <pre><code># same as `--sandbox workspace-write`\nsandbox_mode = \"workspace-write\"\n\n# Extra settings that only apply when `sandbox = \"workspace-write\"`.\n[sandbox_workspace_write]\n# By default, the cwd for the Codex session will be writable as well as $TMPDIR\n# (if set) and /tmp (if it exists). Setting the respective options to `true`\n# will override those defaults.\nexclude_tmpdir_env_var = false\nexclude_slash_tmp = false\n\n# Optional list of _additional_ writable roots beyond $TMPDIR and /tmp.\n\n# Protect top-level .git under writable roots (default is true = allow writes)\nallow_git_writes = true\nwritable_roots = [\"/Users/YOU/.pyenv/shims\"]\n\n# Allow the command being run inside the sandbox to make outbound network\n# requests. Disabled by default.\nnetwork_access = false\n</code></pre> <p>To disable sandboxing altogether, specify <code>danger-full-access</code> like so:</p> <pre><code># same as `--sandbox danger-full-access`\nsandbox_mode = \"danger-full-access\"\n</code></pre> <p>This is reasonable to use if Codex is running in an environment that provides its own sandboxing (such as a Docker container) such that further sandboxing is unnecessary.</p> <p>Though using this option may also be necessary if you try to use Codex in environments where its native sandboxing mechanisms are unsupported, such as older Linux kernels or on Windows.</p>"},{"location":"config/#approval-presets","title":"Approval presets","text":"<p>Codex provides three main Approval Presets:</p> <ul> <li>Read Only: Codex can read files and answer questions; edits, running commands, and network access require approval.</li> <li>Auto: Codex can read files, make edits, and run commands in the workspace without approval; asks for approval outside the workspace or for network access.</li> <li>Full Access: Full disk and network access without prompts; extremely risky.</li> </ul> <p>You can further customize how Codex runs at the command line using the <code>--ask-for-approval</code> and <code>--sandbox</code> options.</p>"},{"location":"config/#mcp-servers","title":"MCP Servers","text":"<p>You can configure Codex to use MCP servers to give Codex access to external applications, resources, or services such as Playwright, Figma, documentation, and more.</p>"},{"location":"config/#server-transport-configuration","title":"Server transport configuration","text":"<p>Each server may set <code>startup_timeout_sec</code> to adjust how long Codex waits for it to start and respond to a tools listing. The default is <code>10</code> seconds. Similarly, <code>tool_timeout_sec</code> limits how long individual tool calls may run (default: <code>60</code> seconds), and Codex will fall back to the default when this value is omitted.</p> <p>This config option is comparable to how Claude and Cursor define <code>mcpServers</code> in their respective JSON config files, though because Codex uses TOML for its config language, the format is slightly different. For example, the following config in JSON:</p> <pre><code>{\n  \"mcpServers\": {\n    \"server-name\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"mcp-server\"],\n      \"env\": {\n        \"API_KEY\": \"value\"\n      }\n    }\n  }\n}\n</code></pre> <p>Should be represented as follows in <code>~/.code/config.toml</code> (Code will also read the legacy <code>~/.codex/config.toml</code> if it exists):</p> <pre><code># The top-level table name must be `mcp_servers`\n# The sub-table name (`server-name` in this example) can be anything you would like.\n[mcp_servers.server-name]\ncommand = \"npx\"\n# Optional\nargs = [\"-y\", \"mcp-server\"]\n# Optional: propagate additional env vars to the MVP server.\n# A default whitelist of env vars will be propagated to the MCP server.\n# https://github.com/openai/codex/blob/main/codex-rs/rmcp-client/src/utils.rs#L82\nenv = { \"API_KEY\" = \"value\" }\n</code></pre>"},{"location":"config/#streamable-http","title":"Streamable HTTP","text":"<pre><code># Streamable HTTP requires the experimental rmcp client\nexperimental_use_rmcp_client = true\n[mcp_servers.figma]\nurl = \"http://127.0.0.1:3845/mcp\"\n# Optional bearer token to be passed into an `Authorization: Bearer &lt;token&gt;` header\n# Use this with caution because the token is in plaintext.\nbearer_token = \"&lt;token&gt;\"\n</code></pre> <p>Refer to the MCP CLI commands for oauth login</p>"},{"location":"config/#other-configuration-options","title":"Other configuration options","text":"<pre><code># Optional: override the default 10s startup timeout\nstartup_timeout_sec = 20\n# Optional: override the default 60s per-tool timeout\ntool_timeout_sec = 30\n</code></pre>"},{"location":"config/#subagents","title":"subagents","text":"<p>Sub-agents are orchestrated helper workflows you can trigger with slash commands (for example <code>/plan</code>, <code>/solve</code>, <code>/code</code>). Each entry under <code>[[subagents.commands]]</code> defines the slash command name, whether spawned agents run in read-only mode, which <code>agents</code> to launch, and extra guidance for both the orchestrator (Code) and the individual agents.</p> <p>By default (when no <code>[[agents]]</code> are configured) Code advertises these model slugs for multi-agent runs: <code>code-gpt-5</code>, <code>claude-sonnet-4.5</code>, <code>claude-opus-4.1</code>, <code>gemini-2.5-pro</code>, <code>gemini-2.5-flash</code>, <code>qwen-3-coder</code>, and <code>code-gpt-5-codex</code>. The cloud counterpart, <code>cloud-gpt-5-codex</code>, only appears when <code>CODE_ENABLE_CLOUD_AGENT_MODEL=1</code> is set. You can override the list by defining <code>[[agents]]</code> entries or by specifying <code>agents = [ \u2026 ]</code> on a given <code>[[subagents.commands]]</code> entry.</p> <pre><code>[[subagents.commands]]\nname = \"context\"\nread-only = true\nagents = [\"context-collector\", \"code-gpt-5\"]\norchestrator-instructions = \"Coordinate a context sweep before coding. Ask each agent to emit concise, linked summaries of relevant files and tooling the primary task might need.\"\nagent-instructions = \"Summarize the repository areas most relevant to the user's request. List file paths, rationale, and suggested follow-up scripts to run. Keep the reply under 2,000 tokens.\"\n</code></pre> <p>With the example above you can run <code>/context</code> inside the TUI to create a summary cell that the main <code>/code</code> turn can reference later. Because <code>context-collector</code> is an ordinary agent, any command-line static analysis utilities it invokes (such as your blast radius tool) should be described in the <code>agent-instructions</code> so the orchestrator launches the right workflow. You can also customise the built-in commands by providing an entry with the same <code>name</code> (<code>plan</code>, <code>solve</code>, or <code>code</code>) and pointing their <code>agents</code> list at your long-context helper.</p>"},{"location":"config/#validation","title":"validation","text":"<p>Controls the quick validation harness that runs before applying patches. The harness now activates automatically whenever at least one validation group is enabled. Use <code>[validation.groups]</code> for high-level toggles and the nested <code>[validation.tools]</code> table for per-tool overrides:</p> <pre><code>[validation.groups]\nfunctional = true\nstylistic = false\n\n[validation.tools]\nshellcheck = true\nmarkdownlint = true\nhadolint = true\nyamllint = true\ncargo-check = true\ntsc = true\neslint = true\nmypy = true\npyright = true\nphpstan = true\npsalm = true\ngolangci-lint = true\nshfmt = true\nprettier = true\n</code></pre> <p>Functional checks stay enabled by default to catch regressions in the touched code, while stylistic linters default to off so teams can opt in when they want formatting feedback.</p> <p>With functional checks enabled, Codex automatically detects the languages affected by a patch and schedules the appropriate tools:</p> <ul> <li><code>cargo-check</code> for Rust workspaces (scoped to touched manifests)</li> <li><code>tsc --noEmit</code> and <code>eslint --max-warnings=0</code> for TypeScript/JavaScript files</li> <li><code>mypy</code> and <code>pyright</code> for Python modules</li> <li><code>phpstan</code>/<code>psalm</code> for PHP projects with matching config or Composer entries</li> <li><code>golangci-lint run ./...</code> for Go modules alongside the existing JSON/TOML/YAML   syntax checks</li> </ul> <p>Each entry under <code>[validation.tools]</code> can be toggled to disable a specific tool or to opt particular checks back in after disabling the entire group.</p> <p>When enabled, Codex can also run <code>actionlint</code> against modified workflows. This is configured under <code>[github]</code>:</p> <pre><code>[github]\nactionlint_on_patch = true\n# Optional: provide an explicit binary path\nactionlint_path = \"/usr/local/bin/actionlint\"\n</code></pre>"},{"location":"config/#disable_response_storage","title":"disable_response_storage","text":"<p>Currently, customers whose accounts are set to use Zero Data Retention (ZDR) must set <code>disable_response_storage</code> to <code>true</code> so that Codex uses an alternative to the Responses API that works with ZDR:</p> <pre><code>disable_response_storage = true\n</code></pre>"},{"location":"config/#managing-mcp-servers-from-cli-experimental","title":"Managing MCP servers from CLI (experimental)","text":"<p>You can also manage these entries from the CLI:</p> <pre><code># Add a server (env can be repeated; `--` separates the launcher command)\ncodex mcp add docs -- docs-server --port 4000\n\n# List configured servers (pretty table or JSON)\ncodex mcp list\ncodex mcp list --json\n\n# Show one server (table or JSON)\ncodex mcp get docs\ncodex mcp get docs --json\n\n# Remove a server\ncodex mcp remove docs\n\n# Log in to a streamable HTTP server that supports oauth\ncodex mcp login SERVER_NAME\n\n# Log out from a streamable HTTP server that supports oauth\ncodex mcp logout SERVER_NAME\n</code></pre>"},{"location":"config/#shell_environment_policy","title":"shell_environment_policy","text":"<p>Codex spawns subprocesses (e.g. when executing a <code>local_shell</code> tool-call suggested by the assistant). By default it now passes your full environment to those subprocesses. You can tune this behavior via the <code>shell_environment_policy</code> block in <code>config.toml</code>:</p> <pre><code>[shell_environment_policy]\n# inherit can be \"all\" (default), \"core\", or \"none\"\ninherit = \"core\"\n# set to true to *skip* the filter for `\"*KEY*\"` and `\"*TOKEN*\"`\nignore_default_excludes = false\n# exclude patterns (case-insensitive globs)\nexclude = [\"AWS_*\", \"AZURE_*\"]\n# force-set / override values\nset = { CI = \"1\" }\n# if provided, *only* vars matching these patterns are kept\ninclude_only = [\"PATH\", \"HOME\"]\n</code></pre> Field Type Default Description <code>inherit</code> string <code>all</code> Starting template for the environment:<code>all</code> (clone full parent env), <code>core</code> (<code>HOME</code>, <code>PATH</code>, <code>USER</code>, \u2026), or <code>none</code> (start empty). <code>ignore_default_excludes</code> boolean <code>false</code> When <code>false</code>, Codex removes any var whose name contains <code>KEY</code>, <code>SECRET</code>, or <code>TOKEN</code> (case-insensitive) before other rules run. <code>exclude</code> array <code>[]</code> Case-insensitive glob patterns to drop after the default filter.Examples: <code>\"AWS_*\"</code>, <code>\"AZURE_*\"</code>. <code>set</code> table <code>{}</code> Explicit key/value overrides or additions \u2013 always win over inherited values. <code>include_only</code> array <code>[]</code> If non-empty, a whitelist of patterns; only variables that match one pattern survive the final step. (Generally used with <code>inherit = \"all\"</code>.) <p>The patterns are glob style, not full regular expressions: <code>*</code> matches any number of characters, <code>?</code> matches exactly one, and character classes like <code>[A-Z]</code>/<code>[^0-9]</code> are supported. Matching is always case-insensitive. This syntax is documented in code as <code>EnvironmentVariablePattern</code> (see <code>core/src/config_types.rs</code>).</p> <p>If you just need a clean slate with a few custom entries you can write:</p> <pre><code>[shell_environment_policy]\ninherit = \"none\"\nset = { PATH = \"/usr/bin\", MY_FLAG = \"1\" }\n</code></pre> <p>Currently, <code>CODEX_SANDBOX_NETWORK_DISABLED=1</code> is also added to the environment, assuming network is disabled. This is not configurable.</p>"},{"location":"config/#otel","title":"otel","text":"<p>Codex can emit OpenTelemetry log events that describe each run: outbound API requests, streamed responses, user input, tool-approval decisions, and the result of every tool invocation. Export is disabled by default so local runs remain self-contained. Opt in by adding an <code>[otel]</code> table and choosing an exporter.</p> <pre><code>[otel]\nenvironment = \"staging\"   # defaults to \"dev\"\nexporter = \"none\"          # defaults to \"none\"; set to otlp-http or otlp-grpc to send events\nlog_user_prompt = false    # defaults to false; redact prompt text unless explicitly enabled\n</code></pre> <p>Codex tags every exported event with <code>service.name = $ORIGINATOR</code> (the same value sent in the <code>originator</code> header, <code>codex_cli_rs</code> by default), the CLI version, and an <code>env</code> attribute so downstream collectors can distinguish dev/staging/prod traffic. Only telemetry produced inside the <code>codex_otel</code> crate\u2014the events listed below\u2014is forwarded to the exporter.</p>"},{"location":"config/#event-catalog","title":"Event catalog","text":"<p>Every event shares a common set of metadata fields: <code>event.timestamp</code>, <code>conversation.id</code>, <code>app.version</code>, <code>auth_mode</code> (when available), <code>user.account_id</code> (when available), <code>terminal.type</code>, <code>model</code>, and <code>slug</code>.</p> <p>With OTEL enabled Codex emits the following event types (in addition to the metadata above):</p> <ul> <li><code>codex.conversation_starts</code></li> <li><code>provider_name</code></li> <li><code>reasoning_effort</code> (optional)</li> <li><code>reasoning_summary</code></li> <li><code>context_window</code> (optional)</li> <li><code>max_output_tokens</code> (optional)</li> <li><code>auto_compact_token_limit</code> (optional)</li> <li><code>approval_policy</code></li> <li><code>sandbox_policy</code></li> <li><code>mcp_servers</code> (comma-separated list)</li> <li><code>active_profile</code> (optional)</li> <li><code>codex.api_request</code></li> <li><code>attempt</code></li> <li><code>duration_ms</code></li> <li><code>http.response.status_code</code> (optional)</li> <li><code>error.message</code> (failures)</li> <li><code>codex.sse_event</code></li> <li><code>event.kind</code></li> <li><code>duration_ms</code></li> <li><code>error.message</code> (failures)</li> <li><code>input_token_count</code> (responses only)</li> <li><code>output_token_count</code> (responses only)</li> <li><code>cached_token_count</code> (responses only, optional)</li> <li><code>reasoning_token_count</code> (responses only, optional)</li> <li><code>tool_token_count</code> (responses only)</li> <li><code>codex.user_prompt</code></li> <li><code>prompt_length</code></li> <li><code>prompt</code> (redacted unless <code>log_user_prompt = true</code>)</li> <li><code>codex.tool_decision</code></li> <li><code>tool_name</code></li> <li><code>call_id</code></li> <li><code>decision</code> (<code>approved</code>, <code>approved_for_session</code>, <code>denied</code>, or <code>abort</code>)</li> <li><code>source</code> (<code>config</code> or <code>user</code>)</li> <li><code>codex.tool_result</code></li> <li><code>tool_name</code></li> <li><code>call_id</code> (optional)</li> <li><code>arguments</code> (optional)</li> <li><code>duration_ms</code> (execution time for the tool)</li> <li><code>success</code> (<code>\"true\"</code> or <code>\"false\"</code>)</li> <li><code>output</code></li> </ul> <p>These event shapes may change as we iterate.</p>"},{"location":"config/#choosing-an-exporter","title":"Choosing an exporter","text":"<p>Set <code>otel.exporter</code> to control where events go:</p> <ul> <li><code>none</code> \u2013 leaves instrumentation active but skips exporting. This is the   default.</li> <li><code>otlp-http</code> \u2013 posts OTLP log records to an OTLP/HTTP collector. Specify the   endpoint, protocol, and headers your collector expects:</li> </ul> <pre><code>[otel]\nexporter = { otlp-http = {\n  endpoint = \"https://otel.example.com/v1/logs\",\n  protocol = \"binary\",\n  headers = { \"x-otlp-api-key\" = \"${OTLP_TOKEN}\" }\n}}\n</code></pre> <ul> <li><code>otlp-grpc</code> \u2013 streams OTLP log records over gRPC. Provide the endpoint and any   metadata headers:</li> </ul> <pre><code>[otel]\nexporter = { otlp-grpc = {\n  endpoint = \"https://otel.example.com:4317\",\n  headers = { \"x-otlp-meta\" = \"abc123\" }\n}}\n</code></pre> <p>If the exporter is <code>none</code> nothing is written anywhere; otherwise you must run or point to your own collector. All exporters run on a background batch worker that is flushed on shutdown.</p> <p>If you build Codex from source the OTEL crate is still behind an <code>otel</code> feature flag; the official prebuilt binaries ship with the feature enabled. When the feature is disabled the telemetry hooks become no-ops so the CLI continues to function without the extra dependencies.</p>"},{"location":"config/#notify","title":"notify","text":"<p>Specify a program that will be executed to get notified about events generated by Codex. Note that the program will receive the notification argument as a string of JSON, e.g.:</p> <pre><code>{\n  \"type\": \"agent-turn-complete\",\n  \"turn-id\": \"12345\",\n  \"input-messages\": [\"Rename `foo` to `bar` and update the callsites.\"],\n  \"last-assistant-message\": \"Rename complete and verified `cargo build` succeeds.\"\n}\n</code></pre> <p>The <code>\"type\"</code> property will always be set. Currently, <code>\"agent-turn-complete\"</code> is the only notification type that is supported.</p> <p>As an example, here is a Python script that parses the JSON and decides whether to show a desktop push notification using terminal-notifier on macOS:</p> <pre><code>#!/usr/bin/env python3\n\nimport json\nimport subprocess\nimport sys\n\n\ndef main() -&gt; int:\n    if len(sys.argv) != 2:\n        print(\"Usage: notify.py &lt;NOTIFICATION_JSON&gt;\")\n        return 1\n\n    try:\n        notification = json.loads(sys.argv[1])\n    except json.JSONDecodeError:\n        return 1\n\n    match notification_type := notification.get(\"type\"):\n        case \"agent-turn-complete\":\n            assistant_message = notification.get(\"last-assistant-message\")\n            if assistant_message:\n                title = f\"Codex: {assistant_message}\"\n            else:\n                title = \"Codex: Turn Complete!\"\n            input_messages = notification.get(\"input_messages\", [])\n            message = \" \".join(input_messages)\n            title += message\n        case _:\n            print(f\"not sending a push notification for: {notification_type}\")\n            return 0\n\n    subprocess.check_output(\n        [\n            \"terminal-notifier\",\n            \"-title\",\n            title,\n            \"-message\",\n            message,\n            \"-group\",\n            \"codex\",\n            \"-ignoreDnD\",\n            \"-activate\",\n            \"com.googlecode.iterm2\",\n        ]\n    )\n\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>To have Codex use this script for notifications, you would configure it via <code>notify</code> in <code>~/.code/config.toml</code> (legacy <code>~/.codex/config.toml</code> is still read) using the appropriate path to <code>notify.py</code> on your computer:</p> <pre><code>notify = [\"python3\", \"/Users/mbolin/.codex/notify.py\"]\n</code></pre> <p>[!NOTE] Use <code>notify</code> for automation and integrations: Codex invokes your external program with a single JSON argument for each event, independent of the TUI. If you only want lightweight desktop notifications while using the TUI, prefer <code>tui.notifications</code>, which uses terminal escape codes and requires no external program. You can enable both; <code>tui.notifications</code> covers in\u2011TUI alerts (e.g., approval prompts), while <code>notify</code> is best for system\u2011level hooks or custom notifiers. Currently, <code>notify</code> emits only <code>agent-turn-complete</code>, whereas <code>tui.notifications</code> supports <code>agent-turn-complete</code> and <code>approval-requested</code> with optional filtering.</p>"},{"location":"config/#history","title":"history","text":"<p>By default, Codex CLI records messages sent to the model in <code>$CODEX_HOME/history.jsonl</code>. Note that on UNIX, the file permissions are set to <code>o600</code>, so it should only be readable and writable by the owner.</p> <p>To disable this behavior, configure <code>[history]</code> as follows:</p> <pre><code>[history]\npersistence = \"none\"  # \"save-all\" is the default value\n</code></pre>"},{"location":"config/#file_opener","title":"file_opener","text":"<p>Identifies the editor/URI scheme to use for hyperlinking citations in model output. If set, citations to files in the model output will be hyperlinked using the specified URI scheme so they can be ctrl/cmd-clicked from the terminal to open them.</p> <p>For example, if the model output includes a reference such as <code>\u3010F:/home/user/project/main.py\u2020L42-L50\u3011</code>, then this would be rewritten to link to the URI <code>vscode://file/home/user/project/main.py:42</code>.</p> <p>Note this is not a general editor setting (like <code>$EDITOR</code>), as it only accepts a fixed set of values:</p> <ul> <li><code>\"vscode\"</code> (default)</li> <li><code>\"vscode-insiders\"</code></li> <li><code>\"windsurf\"</code></li> <li><code>\"cursor\"</code></li> <li><code>\"none\"</code> to explicitly disable this feature</li> </ul> <p>Currently, <code>\"vscode\"</code> is the default, though Codex does not verify VS Code is installed. As such, <code>file_opener</code> may default to <code>\"none\"</code> or something else in the future.</p>"},{"location":"config/#hide_agent_reasoning","title":"hide_agent_reasoning","text":"<p>Codex intermittently emits \"reasoning\" events that show the model's internal \"thinking\" before it produces a final answer. Some users may find these events distracting, especially in CI logs or minimal terminal output.</p> <p>Setting <code>hide_agent_reasoning</code> to <code>true</code> suppresses these events in both the TUI as well as the headless <code>exec</code> sub-command:</p> <pre><code>hide_agent_reasoning = true   # defaults to false\n</code></pre>"},{"location":"config/#show_raw_agent_reasoning","title":"show_raw_agent_reasoning","text":"<p>Surfaces the model\u2019s raw chain-of-thought (\"raw reasoning content\") when available.</p> <p>Notes:</p> <ul> <li>Only takes effect if the selected model/provider actually emits raw reasoning content. Many models do not. When unsupported, this option has no visible effect.</li> <li>Raw reasoning may include intermediate thoughts or sensitive context. Enable only if acceptable for your workflow.</li> </ul> <p>Example:</p> <pre><code>show_raw_agent_reasoning = true  # defaults to false\n</code></pre>"},{"location":"config/#model_context_window","title":"model_context_window","text":"<p>The size of the context window for the model, in tokens.</p> <p>In general, Codex knows the context window for the most common OpenAI models, but if you are using a new model with an old version of the Codex CLI, then you can use <code>model_context_window</code> to tell Codex what value to use to determine how much context is left during a conversation.</p>"},{"location":"config/#model_max_output_tokens","title":"model_max_output_tokens","text":"<p>This is analogous to <code>model_context_window</code>, but for the maximum number of output tokens for the model.</p>"},{"location":"config/#project_doc_max_bytes","title":"project_doc_max_bytes","text":"<p>Maximum number of bytes to read from an <code>AGENTS.md</code> file to include in the instructions sent with the first turn of a session. Defaults to 32 KiB.</p>"},{"location":"config/#project_doc_fallback_filenames","title":"project_doc_fallback_filenames","text":"<p>Ordered list of additional filenames to look for when <code>AGENTS.md</code> is missing at a given directory level. The CLI always checks <code>AGENTS.md</code> first; the configured fallbacks are tried in the order provided. This lets monorepos that already use alternate instruction files (for example, <code>CLAUDE.md</code>) work out of the box while you migrate to <code>AGENTS.md</code> over time.</p> <pre><code>project_doc_fallback_filenames = [\"CLAUDE.md\", \".exampleagentrules.md\"]\n</code></pre> <p>We recommend migrating instructions to AGENTS.md; other filenames may reduce model performance.</p>"},{"location":"config/#tui","title":"tui","text":"<p>Options that are specific to the TUI.</p> <pre><code>[tui]\n# Send desktop notifications when approvals are required or a turn completes.\n# Defaults to false.\nnotifications = true\n\n# You can optionally filter to specific notification types.\n# Available types are \"agent-turn-complete\" and \"approval-requested\".\nnotifications = [ \"agent-turn-complete\", \"approval-requested\" ]\n</code></pre> <p>[!NOTE] Codex emits desktop notifications using terminal escape codes. Not all terminals support these (notably, macOS Terminal.app and VS Code's terminal do not support custom notifications. iTerm2, Ghostty and WezTerm do support these notifications).</p> <p>[!NOTE] <code>tui.notifications</code> is built\u2011in and limited to the TUI session. For programmatic or cross\u2011environment notifications\u2014or to integrate with OS\u2011specific notifiers\u2014use the top-level <code>notify</code> option to run an external program that receives event JSON. The two settings are independent and can be used together.</p>"},{"location":"config/#auto-drive-observer","title":"Auto Drive Observer","text":"<p>Codex keeps long-running Auto Drive sessions in check with a lightweight observer thread. Configure its cadence with the top-level <code>auto_drive_observer_cadence</code> key (default <code>5</code>). After every n completed requests the observer reviews the coordinator/CLI transcript, emits telemetry, and\u2014if necessary\u2014suggests a corrected prompt or follow-up guidance. Setting the value to <code>0</code> disables the observer entirely.</p> <pre><code># Run the observer after every third Auto Drive request\nauto_drive_observer_cadence = 3\n</code></pre> <p>When the observer reports <code>status = \"failing\"</code>, the TUI banner highlights the intervention, updates the pending prompt when provided, and records guidance for future coordinator turns.</p>"},{"location":"config/#project-hooks","title":"Project Hooks","text":"<p>Use the <code>[projects]</code> table to scope settings to a specific workspace path. In addition to <code>trust_level</code>, <code>approval_policy</code>, and <code>always_allow_commands</code>, you can attach lifecycle hooks that run commands automatically when notable events occur.</p> <pre><code>[projects.\"/Users/me/src/my-app\"]\ntrust_level = \"trusted\"\n\n[[projects.\"/Users/me/src/my-app\".hooks]]\nname = \"bootstrap\"\nevent = \"session.start\"\nrun = [\"./scripts/bootstrap.sh\"]\ntimeout_ms = 60000\n\n[[projects.\"/Users/me/src/my-app\".hooks]]\nevent = \"tool.after\"\nrun = \"npm run lint -- --changed\"\n</code></pre> <p>Supported hook events:</p> <ul> <li><code>session.start</code>: after the session is configured (once per launch)</li> <li><code>session.end</code>: before shutdown completes</li> <li><code>tool.before</code>: immediately before each exec/tool command runs</li> <li><code>tool.after</code>: once an exec/tool command finishes (regardless of exit code)</li> <li><code>file.before_write</code>: right before an <code>apply_patch</code> is applied</li> <li><code>file.after_write</code>: after an <code>apply_patch</code> completes and diffs are emitted</li> </ul> <p>Hook commands run inside the same sandbox mode as the session and appear in the TUI as their own exec cells. Failures are surfaced as background events but do not block the main task. Each invocation receives environment variables such as <code>CODE_HOOK_EVENT</code>, <code>CODE_HOOK_NAME</code>, <code>CODE_HOOK_INDEX</code>, <code>CODE_HOOK_CALL_ID</code>, <code>CODE_HOOK_PAYLOAD</code> (JSON describing the context), <code>CODE_SESSION_CWD</code>, and\u2014when applicable\u2014<code>CODE_HOOK_SOURCE_CALL_ID</code>. Hooks may also set <code>cwd</code>, provide additional <code>env</code> entries, and specify <code>timeout_ms</code>.</p> <p>Example <code>tool.after</code> payload:</p> <pre><code>{\n  \"event\": \"tool.after\",\n  \"call_id\": \"tool_12\",\n  \"cwd\": \"/Users/me/src/my-app\",\n  \"command\": [\"npm\", \"test\"],\n  \"exit_code\": 1,\n  \"duration_ms\": 1832,\n  \"stdout\": \"\u2026output truncated\u2026\",\n  \"stderr\": \"\u2026\",\n  \"timed_out\": false\n}\n</code></pre>"},{"location":"config/#project-commands","title":"Project Commands","text":"<p>Define project-scoped commands under <code>[[projects.\"&lt;path&gt;\".commands]]</code>. Each command needs a unique <code>name</code> and either an array (<code>command</code>) or string (<code>run</code>) describing how to invoke it. Optional fields include <code>description</code>, <code>cwd</code>, <code>env</code>, and <code>timeout_ms</code>.</p> <pre><code>[[projects.\"/Users/me/src/my-app\".commands]]\nname = \"setup\"\ndescription = \"Install dependencies\"\nrun = [\"pnpm\", \"install\"]\n\n[[projects.\"/Users/me/src/my-app\".commands]]\nname = \"unit\"\nrun = \"cargo test --lib\"\n</code></pre> <p>Project commands appear in the TUI via <code>/cmd &lt;name&gt;</code> and run through the standard execution pipeline. During execution Codex sets <code>CODE_PROJECT_COMMAND_NAME</code>, <code>CODE_PROJECT_COMMAND_DESCRIPTION</code> (when provided), and <code>CODE_SESSION_CWD</code> so scripts can tailor their behaviour.</p>"},{"location":"config/#config-reference","title":"Config reference","text":"Key Type / Values Notes <code>model</code> string Model to use (e.g., <code>gpt-5-codex</code>). <code>model_provider</code> string Provider id from <code>model_providers</code> (default: <code>openai</code>). <code>model_context_window</code> number Context window tokens. <code>model_max_output_tokens</code> number Max output tokens. <code>approval_policy</code> <code>untrusted</code> | <code>on-failure</code> | <code>on-request</code> | <code>never</code> When to prompt for approval. <code>sandbox_mode</code> <code>read-only</code> | <code>workspace-write</code> | <code>danger-full-access</code> OS sandbox policy. <code>sandbox_workspace_write.writable_roots</code> array Extra writable roots in workspace\u2011write. <code>sandbox_workspace_write.network_access</code> boolean Allow network in workspace\u2011write (default: false). <code>sandbox_workspace_write.exclude_tmpdir_env_var</code> boolean Exclude <code>$TMPDIR</code> from writable roots (default: false). <code>sandbox_workspace_write.exclude_slash_tmp</code> boolean Exclude <code>/tmp</code> from writable roots (default: false). <code>disable_response_storage</code> boolean Required for ZDR orgs. <code>notify</code> array External program for notifications. <code>instructions</code> string Currently ignored; use <code>experimental_instructions_file</code> or <code>AGENTS.md</code>. <code>mcp_servers.&lt;id&gt;.command</code> string MCP server launcher command. <code>mcp_servers.&lt;id&gt;.args</code> array MCP server args. <code>mcp_servers.&lt;id&gt;.env</code> map MCP server env vars. <code>mcp_servers.&lt;id&gt;.startup_timeout_sec</code> number Startup timeout in seconds (default: 10). Timeout is applied both for initializing MCP server and initially listing tools. <code>mcp_servers.&lt;id&gt;.tool_timeout_sec</code> number Per-tool timeout in seconds (default: 60). Accepts fractional values; omit to use the default. <code>model_providers.&lt;id&gt;.name</code> string Display name. <code>model_providers.&lt;id&gt;.base_url</code> string API base URL. <code>model_providers.&lt;id&gt;.env_key</code> string Env var for API key. <code>model_providers.&lt;id&gt;.wire_api</code> <code>chat</code> | <code>responses</code> Protocol used (default: <code>chat</code>). <code>model_providers.&lt;id&gt;.query_params</code> map Extra query params (e.g., Azure <code>api-version</code>). <code>model_providers.&lt;id&gt;.http_headers</code> map Additional static headers. <code>model_providers.&lt;id&gt;.env_http_headers</code> map Headers sourced from env vars. <code>model_providers.&lt;id&gt;.request_max_retries</code> number Per\u2011provider HTTP retry count (default: 4). <code>model_providers.&lt;id&gt;.stream_max_retries</code> number SSE stream retry count (default: 5). <code>model_providers.&lt;id&gt;.stream_idle_timeout_ms</code> number SSE idle timeout (ms) (default: 300000). <code>project_doc_max_bytes</code> number Max bytes to read from <code>AGENTS.md</code>. <code>projects.&lt;path&gt;.trust_level</code> string Mark project/worktree as trusted (only <code>\"trusted\"</code> is recognized). <code>projects.&lt;path&gt;.hooks</code> array Lifecycle hooks for that workspace (see \"Project Hooks\"). <code>projects.&lt;path&gt;.commands</code> array Project commands exposed via <code>/cmd</code>. <code>profile</code> string Active profile name. <code>profiles.&lt;name&gt;.*</code> various Profile\u2011scoped overrides of the same keys. <code>history.persistence</code> <code>save-all</code> | <code>none</code> History file persistence (default: <code>save-all</code>). <code>history.max_bytes</code> number Currently ignored (not enforced). <code>file_opener</code> <code>vscode</code> | <code>vscode-insiders</code> | <code>windsurf</code> | <code>cursor</code> | <code>none</code> URI scheme for clickable citations (default: <code>vscode</code>). <code>tui</code> table TUI\u2011specific options. <code>tui.notifications</code> boolean | array Enable desktop notifications in the tui (default: false). <code>hide_agent_reasoning</code> boolean Hide model reasoning events. <code>show_raw_agent_reasoning</code> boolean Show raw reasoning (when available). <code>model_reasoning_effort</code> <code>minimal</code> | <code>low</code> | <code>medium</code> | <code>high</code> Responses API reasoning effort. <code>model_reasoning_summary</code> <code>auto</code> | <code>concise</code> | <code>detailed</code> | <code>none</code> Reasoning summaries. <code>model_verbosity</code> <code>low</code> | <code>medium</code> | <code>high</code> GPT\u20115 text verbosity (Responses API). <code>model_supports_reasoning_summaries</code> boolean Force\u2011enable reasoning summaries. <code>chatgpt_base_url</code> string Base URL for ChatGPT auth flow. <code>experimental_resume</code> string (path) Resume JSONL path (internal/experimental). <code>experimental_instructions_file</code> string (path) Replace built\u2011in instructions (experimental). <code>experimental_use_exec_command_tool</code> boolean Use experimental exec command tool. <code>use_experimental_reasoning_summary</code> boolean Use experimental summary for reasoning chain. <code>responses_originator_header_internal_override</code> string Override <code>originator</code> header value. <code>tools.web_search</code> boolean Enable web search tool (alias: <code>web_search_request</code>) (default: false). <code>tools.web_search_allowed_domains</code> array Optional allow-list for web search (filters.allowed_domains)."},{"location":"contributing/","title":"Contributing to MARS","text":"<p>We welcome contributions! Here's how you can help.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-the-repository","title":"1. Clone the Repository","text":"<pre><code>git clone https://github.com/GeorgePearse/code.git\ncd code/code-rs/code-mars\n</code></pre>"},{"location":"contributing/#2-install-dependencies","title":"2. Install Dependencies","text":"<pre><code>rustup update\ncargo build\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run Tests","text":"<pre><code># Unit tests\ncargo test --lib\n\n# Integration tests\ncargo test --test '*'\n\n# All tests with output\ncargo test -- --nocapture\n</code></pre>"},{"location":"contributing/#code-structure","title":"Code Structure","text":"<pre><code>code-rs/code-mars/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 main.rs              # CLI entry point\n\u2502   \u251c\u2500\u2500 lib.rs               # Public API\n\u2502   \u251c\u2500\u2500 coordinator.rs        # Phase orchestration\n\u2502   \u251c\u2500\u2500 agent.rs             # Individual agent\n\u2502   \u251c\u2500\u2500 workspace.rs         # Solution storage\n\u2502   \u251c\u2500\u2500 verifier.rs          # Verification system\n\u2502   \u251c\u2500\u2500 aggregator.rs        # Aggregation routing\n\u2502   \u251c\u2500\u2500 moa.rs               # MOA implementation\n\u2502   \u251c\u2500\u2500 mcts.rs              # MCTS implementation\n\u2502   \u251c\u2500\u2500 strategy.rs          # Strategy network\n\u2502   \u251c\u2500\u2500 types.rs             # Core types\n\u2502   \u251c\u2500\u2500 config.rs            # Configuration\n\u2502   \u251c\u2500\u2500 prompts.rs           # Prompt templates\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 mcts_integration.rs\n\u2502   \u2514\u2500\u2500 multi_provider_integration.rs\n\u251c\u2500\u2500 examples/\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 README.md\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#1-create-a-branch","title":"1. Create a Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n</code></pre>"},{"location":"contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<p>Follow Rust conventions and run tests:</p> <pre><code>cargo fmt\ncargo clippy --all-targets --all-features -- -D warnings\ncargo test --lib\ncargo test --test '*'\n</code></pre>"},{"location":"contributing/#3-commit-and-push","title":"3. Commit and Push","text":"<pre><code>git add .\ngit commit -m \"feat: description of change\"\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"contributing/#4-create-a-pull-request","title":"4. Create a Pull Request","text":"<p>Push to GitHub and create a PR with clear description.</p>"},{"location":"contributing/#testing-guidelines","title":"Testing Guidelines","text":"<p>All public functions should have tests. Use:</p> <pre><code>#[test]\nfn test_feature() {\n    let input = setup();\n    let result = function_under_test(input);\n    assert_eq!(result, expected);\n}\n</code></pre> <p>For async code:</p> <pre><code>#[tokio::test]\nasync fn test_async_feature() {\n    // Your test\n}\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Functions: <code>snake_case</code></li> <li>Types: <code>PascalCase</code></li> <li>Constants: <code>SCREAMING_SNAKE_CASE</code></li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Check GitHub Issues or Documentation</p> <p>Thank you for contributing!</p>"},{"location":"exec/","title":"Execution","text":""},{"location":"exec/#non-interactive-mode","title":"Non-interactive mode","text":"<p>Use Codex in non-interactive mode to automate common workflows.</p> <pre><code>codex exec \"count the total number of lines of code in this project\"\n</code></pre> <p>In non-interactive mode, Codex does not ask for command or edit approvals. By default it runs in <code>read-only</code> mode, so it cannot edit files or run commands that require network access.</p> <p>Use <code>codex exec --full-auto</code> to allow file edits. Use <code>codex exec --sandbox danger-full-access</code> to allow edits and networked commands.</p>"},{"location":"exec/#default-output-mode","title":"Default output mode","text":"<p>By default, Codex streams its activity to stderr and only writes the final message from the agent to stdout. This makes it easier to pipe <code>codex exec</code> into another tool without extra filtering.</p> <p>To write the output of <code>codex exec</code> to a file, in addition to using a shell redirect like <code>&gt;</code>, there is also a dedicated flag to specify an output file: <code>-o</code>/<code>--output-last-message</code>.</p>"},{"location":"exec/#json-output-mode","title":"JSON output mode","text":"<p><code>codex exec</code> supports a <code>--json</code> mode that streams events to stdout as JSON Lines (JSONL) while the agent runs.</p> <p>Supported event types:</p> <ul> <li><code>thread.started</code> - when a thread is started or resumed.</li> <li><code>turn.started</code> - when a turn starts. A turn encompasses all events between the user message and the assistant response.</li> <li><code>turn.completed</code> - when a turn completes; includes token usage.</li> <li><code>turn.failed</code> - when a turn fails; includes error details.</li> <li><code>item.started</code>/<code>item.updated</code>/<code>item.completed</code> - when a thread item is added/updated/completed.</li> </ul> <p>Supported item types:</p> <ul> <li><code>assistant_message</code> - assistant message.</li> <li><code>reasoning</code> - a summary of the assistant's thinking.</li> <li><code>command_execution</code> - assistant executing a command.</li> <li><code>file_change</code> - assistant making file changes.</li> <li><code>mcp_tool_call</code> - assistant calling an MCP tool.</li> <li><code>web_search</code> - assistant performing a web search.</li> </ul> <p>Typically, an <code>assistant_message</code> is added at the end of the turn.</p> <p>Sample output:</p> <pre><code>{\"type\":\"thread.started\",\"thread_id\":\"0199a213-81c0-7800-8aa1-bbab2a035a53\"}\n{\"type\":\"turn.started\"}\n{\"type\":\"item.completed\",\"item\":{\"id\":\"item_0\",\"item_type\":\"reasoning\",\"text\":\"**Searching for README files**\"}}\n{\"type\":\"item.started\",\"item\":{\"id\":\"item_1\",\"item_type\":\"command_execution\",\"command\":\"bash -lc ls\",\"aggregated_output\":\"\",\"status\":\"in_progress\"}}\n{\"type\":\"item.completed\",\"item\":{\"id\":\"item_1\",\"item_type\":\"command_execution\",\"command\":\"bash -lc ls\",\"aggregated_output\":\"2025-09-11\\nAGENTS.md\\nCHANGELOG.md\\ncliff.toml\\ncodex-cli\\ncode-rs\\ncodex-rs\\ndocs\\nexamples\\nflake.lock\\nflake.nix\\nLICENSE\\nnode_modules\\nNOTICE\\npackage.json\\npnpm-lock.yaml\\npnpm-workspace.yaml\\nPNPM.md\\nREADME.md\\nscripts\\nsdk\\ntmp\\n\",\"exit_code\":0,\"status\":\"completed\"}}\n{\"type\":\"item.completed\",\"item\":{\"id\":\"item_2\",\"item_type\":\"reasoning\",\"text\":\"**Checking repository root for README**\"}}\n{\"type\":\"item.completed\",\"item\":{\"id\":\"item_3\",\"item_type\":\"assistant_message\",\"text\":\"Yep \u2014 there\u2019s a `README.md` in the repository root.\"}}\n{\"type\":\"turn.completed\",\"usage\":{\"input_tokens\":24763,\"cached_input_tokens\":24448,\"output_tokens\":122}}\n</code></pre>"},{"location":"exec/#structured-output","title":"Structured output","text":"<p>By default, the agent responds with natural language. Use <code>--output-schema</code> to provide a JSON Schema that defines the expected JSON output.</p> <p>The JSON Schema must follow the strict schema rules.</p> <p>Sample schema:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"project_name\": { \"type\": \"string\" },\n    \"programming_languages\": { \"type\": \"array\", \"items\": { \"type\": \"string\" } }\n  },\n  \"required\": [\"project_name\", \"programming_languages\"],\n  \"additionalProperties\": false\n}\n</code></pre> <pre><code>codex exec \"Extract details of the project\" --output-schema ~/schema.json\n...\n\n{\"project_name\":\"Codex CLI\",\"programming_languages\":[\"Rust\",\"TypeScript\",\"Shell\"]}\n</code></pre> <p>Combine <code>--output-schema</code> with <code>-o</code> to only print the final JSON output. You can also pass a file path to <code>-o</code> to save the JSON output to a file.</p>"},{"location":"exec/#git-repository-requirement","title":"Git repository requirement","text":"<p>Codex requires a Git repository to avoid destructive changes. To disable this check, use <code>codex exec --skip-git-repo-check</code>.</p>"},{"location":"exec/#resuming-non-interactive-sessions","title":"Resuming non-interactive sessions","text":"<p>Resume a previous non-interactive session with <code>codex exec resume &lt;SESSION_ID&gt;</code> or <code>codex exec resume --last</code>. This preserves conversation context so you can ask follow-up questions or give new tasks to the agent.</p> <pre><code>codex exec \"Review the change, look for use-after-free issues\"\ncodex exec resume --last \"Fix use-after-free issues\"\n</code></pre> <p>Only the conversation context is preserved; you must still provide flags to customize Codex behavior.</p> <pre><code>codex exec --model gpt-5-codex --json \"Review the change, look for use-after-free issues\"\ncodex exec --model gpt-5 --json resume --last \"Fix use-after-free issues\"\n</code></pre>"},{"location":"exec/#authentication","title":"Authentication","text":"<p>By default, <code>codex exec</code> will use the same authentication method as Codex CLI and VSCode extension. You can override the api key by setting the <code>CODEX_API_KEY</code> environment variable.</p> <pre><code>CODEX_API_KEY=your-api-key-here codex exec \"Fix merge conflict\"\n</code></pre> <p>NOTE: <code>CODEX_API_KEY</code> is only supported in <code>codex exec</code>.</p>"},{"location":"experimental/","title":"Experimental","text":""},{"location":"experimental/#experimental-technology-disclaimer","title":"Experimental technology disclaimer","text":"<p>Codex CLI is an experimental project under active development. It is not yet stable, may contain bugs, incomplete features, or undergo breaking changes. We're building it in the open with the community and welcome:</p> <ul> <li>Bug reports</li> <li>Feature requests</li> <li>Pull requests</li> <li>Good vibes</li> </ul> <p>Help us improve by filing issues or submitting PRs (see the section below for how to contribute)!</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#faq","title":"FAQ","text":""},{"location":"faq/#openai-released-a-model-called-codex-in-2021-is-this-related","title":"OpenAI released a model called Codex in 2021 - is this related?","text":"<p>In 2021, OpenAI released Codex, an AI system designed to generate code from natural language prompts. That original Codex model was deprecated as of March 2023 and is separate from the CLI tool.</p>"},{"location":"faq/#which-models-are-supported","title":"Which models are supported?","text":"<p>We recommend using Codex with GPT-5, our best coding model. The default reasoning level is medium, and you can upgrade to high for complex tasks with the <code>/model</code> command.</p> <p>You can also use older models by using API-based auth and launching codex with the <code>--model</code> flag.</p>"},{"location":"faq/#why-does-o3-or-o4-mini-not-work-for-me","title":"Why does <code>o3</code> or <code>o4-mini</code> not work for me?","text":"<p>It's possible that your API account needs to be verified in order to start streaming responses and seeing chain of thought summaries from the API. If you're still running into issues, please let us know!</p>"},{"location":"faq/#how-do-i-stop-codex-from-editing-my-files","title":"How do I stop Codex from editing my files?","text":"<p>By default, Codex can modify files in your current working directory (Auto mode). To prevent edits, run <code>codex</code> in read-only mode with the CLI flag <code>--sandbox read-only</code>. Alternatively, you can change the approval level mid-conversation with <code>/approvals</code>.</p>"},{"location":"faq/#does-it-work-on-windows","title":"Does it work on Windows?","text":"<p>Running Codex directly on Windows may work, but is not officially supported. We recommend using Windows Subsystem for Linux (WSL2).</p>"},{"location":"fork-enhancements/","title":"Fork Enhancements (Initial, Not Exhaustive)","text":"<p>This fork extends upstream <code>openai/codex</code> in several areas. These bullets are a starting point \u2014 not a complete list. Before merging, scan the codebase and history (CHANGELOG.md, recent commits) to discover additional or newer fork\u2011only behavior and preserve it.</p> <ul> <li>Browser Integration (code-rs/browser, TUI + core wiring)</li> <li>Internal CDP browser manager with global access and /browser command.</li> <li>Unified <code>browser</code> tool with actions (open, status, close, click, move, type, key, javascript, scroll, history, inspect, console, cleanup, cdp, fetch).</li> <li>Screenshot capture with segmentation, cursor overlay, asset storage, and per\u2011turn injection/queueing; TUI rendering with friendly titles.</li> <li> <p>External Chrome attach via <code>/chrome</code> and headless profile management.</p> </li> <li> <p>Multi\u2011Agent Orchestration (core/agent_tool.rs, TUI panel)</p> </li> <li>Unified <code>agent</code> tool with actions (create, status, result, cancel, wait, list) and persisted outputs under <code>.code/agents/&lt;id&gt;</code> plus the TUI agent panel.</li> <li> <p>Batch and per\u2011agent status updates; file emission (result/error/status logs).</p> </li> <li> <p>Tooling Extensions and Policy Integration</p> </li> <li><code>browser</code> action <code>fetch</code> (formerly <code>web_fetch</code>) with markdown\u2011aware TUI rendering and UA override.</li> <li><code>view_image</code> tool to attach local images.</li> <li>Local shell + sandbox policy with escalation request (<code>with_escalated_permissions</code>, <code>justification</code>) and WorkspaceWrite flags (network access, allow_git_writes, tmpdir controls).</li> <li> <p>Streamable exec tool support kept off by default; preserved compatibility with classic shell tool.</p> </li> <li> <p>Protocol/Model Compatibility Tweaks</p> </li> <li>Responses/Chat Completions parity for tools; MCP tool schema sanitization to a safe subset.</li> <li>FunctionCallOutput serialization kept as a plain string for Responses API compatibility.</li> <li> <p>Web search event mapping and <code>WebSearchCall</code> rendering in TUI; optional upstream web_search tool gated by policy.</p> </li> <li> <p>TUI Enhancements</p> </li> <li>Strict streaming ordering invariants (request_ordinal, output_index, sequence_number) and delta rendering.</li> <li>Markdown renderer improvements and code-block snapshots; theme selection view; bottom pane chrome selection.</li> <li>History cells with richer tool-specific titles and previews (browser actions, fetch, agent actions).</li> <li> <p>Fully state-driven history refactor: <code>HistoryState</code> + <code>HistoryDomainEvent</code>, shared renderer cache, and serialized snapshots. See <code>docs/tui-chatwidget-refactor.md</code>, <code>docs/history_state_schema.md</code>, and <code>docs/history_render_cache_bridge.md</code>.</p> </li> <li> <p>Version and User\u2011Agent Handling</p> </li> <li> <p>Consistent <code>codex_version::version()</code> usage; <code>get_codex_user_agent(_default)</code> helper; MCP server/client UA tests.</p> </li> <li> <p>Build/CI/Workflow</p> </li> <li><code>build-fast.sh</code> portability (exec bin autodetect, deterministic link tweaks) and zero\u2011warning policy.</li> <li>Upstream merge guards and policy to prefer our core files; static verify checks (handlers\u2194tools parity, UA/version) to prevent regressions.</li> </ul> <p>Note: These are representative areas. You must still scan for newer fork\u2011only behavior (e.g., additional TUI UX, new tools, rollout metrics, protocol fields) and preserve them when resolving conflicts.</p>"},{"location":"getting-started/","title":"Quick Start","text":""},{"location":"getting-started/#getting-started","title":"Getting started","text":""},{"location":"getting-started/#cli-usage","title":"CLI usage","text":"Command Purpose Example <code>codex</code> Interactive TUI <code>codex</code> <code>codex \"...\"</code> Initial prompt for interactive TUI <code>codex \"fix lint errors\"</code> <code>codex exec \"...\"</code> Non-interactive \"automation mode\" <code>codex exec \"explain utils.ts\"</code> <p>Key flags: <code>--model/-m</code>, <code>--ask-for-approval/-a</code>.</p>"},{"location":"getting-started/#running-with-a-prompt-as-input","title":"Running with a prompt as input","text":"<p>You can also run Codex CLI with a prompt as input:</p> <pre><code>codex \"explain this codebase to me\"\n</code></pre> <pre><code>codex --full-auto \"create the fanciest todo-list app\"\n</code></pre> <p>That's it - Codex will scaffold a file, run it inside a sandbox, install any missing dependencies, and show you the live result. Approve the changes and they'll be committed to your working directory.</p>"},{"location":"getting-started/#example-prompts","title":"Example prompts","text":"<p>Below are a few bite-size examples you can copy-paste. Replace the text in quotes with your own task.</p> \u2728 What you type What happens 1 <code>codex \"Refactor the Dashboard component to React Hooks\"</code> Codex rewrites the class component, runs <code>npm test</code>, and shows the diff. 2 <code>codex \"Generate SQL migrations for adding a users table\"</code> Infers your ORM, creates migration files, and runs them in a sandboxed DB. 3 <code>codex \"Write unit tests for utils/date.ts\"</code> Generates tests, executes them, and iterates until they pass. 4 <code>codex \"Bulk-rename *.jpeg -&gt; *.jpg with git mv\"</code> Safely renames files and updates imports/usages. 5 <code>codex \"Explain what this regex does: ^(?=.*[A-Z]).{8,}$\"</code> Outputs a step-by-step human explanation. 6 <code>codex \"Carefully review this repo, and propose 3 high impact well-scoped PRs\"</code> Suggests impactful PRs in the current codebase. 7 <code>codex \"Look for vulnerabilities and create a security review report\"</code> Finds and explains security bugs."},{"location":"getting-started/#memory-with-agentsmd","title":"Memory with AGENTS.md","text":"<p>You can give Codex extra instructions and guidance using <code>AGENTS.md</code> files. Codex looks for <code>AGENTS.md</code> files in the following places, and merges them top-down:</p> <ol> <li><code>~/.code/AGENTS.md</code> - personal global guidance (Code will also read a legacy <code>~/.codex/AGENTS.md</code> if present)</li> <li><code>AGENTS.md</code> at repo root - shared project notes</li> <li><code>AGENTS.md</code> in the current working directory - sub-folder/feature specifics</li> </ol> <p>For more information on how to use AGENTS.md, see the official AGENTS.md documentation.</p>"},{"location":"getting-started/#tips-shortcuts","title":"Tips &amp; shortcuts","text":""},{"location":"getting-started/#use-for-file-search","title":"Use <code>@</code> for file search","text":"<p>Typing <code>@</code> triggers a fuzzy-filename search over the workspace root. Use up/down to select among the results and Tab or Enter to replace the <code>@</code> with the selected path. You can use Esc to cancel the search.</p>"},{"location":"getting-started/#image-input","title":"Image input","text":"<p>Paste images directly into the composer (Ctrl+V / Cmd+V) to attach them to your prompt. You can also attach files via the CLI using <code>-i/--image</code> (comma\u2011separated):</p> <pre><code>codex -i screenshot.png \"Explain this error\"\ncodex --image img1.png,img2.jpg \"Summarize these diagrams\"\n</code></pre>"},{"location":"getting-started/#escesc-to-edit-a-previous-message","title":"Esc\u2013Esc to edit a previous message","text":"<p>When the chat composer is empty, press Esc to prime \u201cbacktrack\u201d mode. Press Esc again to open a transcript preview highlighting the last user message; press Esc repeatedly to step to older user messages. Press Enter to confirm and Codex will fork the conversation from that point, trim the visible transcript accordingly, and pre\u2011fill the composer with the selected user message so you can edit and resubmit it.</p> <p>In the transcript preview, the footer shows an <code>Esc edit prev</code> hint while editing is active.</p>"},{"location":"getting-started/#shell-completions","title":"Shell completions","text":"<p>Generate shell completion scripts via:</p> <pre><code>code completion bash\ncode completion zsh\ncode completion fish\n</code></pre>"},{"location":"getting-started/#-cd-c-flag","title":"<code>--cd</code>/<code>-C</code> flag","text":"<p>Sometimes it is not convenient to <code>cd</code> to the directory you want Codex to use as the \"working root\" before running Codex. Fortunately, <code>codex</code> supports a <code>--cd</code> option so you can specify whatever folder you want. You can confirm that Codex is honoring <code>--cd</code> by double-checking the workdir it reports in the TUI at the start of a new session.</p>"},{"location":"history_render_cache_bridge/","title":"History Render Cache Bridge","text":"<p>This document captures the current bridge between the new <code>HistoryState</code> data model and the legacy per-cell layout caches that still live inside a few history cells (<code>ExecCell</code>, streaming/assistant cells, and diff cells). The bridge allows <code>ChatWidget</code> to render the unified history vector without re-computing expensive layouts every frame while we complete the Step\u202f6 renderer work.</p>"},{"location":"history_render_cache_bridge/#rendering-pipeline-overview","title":"Rendering pipeline overview","text":"<ol> <li>The draw loop in <code>ChatWidget::render_history</code> assembles a list of    <code>RenderRequest</code> values. Each request contains the <code>HistoryId</code> of the record    being rendered, the existing <code>HistoryCell</code> cache if one exists, and a set of    \"fallback\" lines derived on demand from the semantic <code>HistoryRecord</code>.</li> <li><code>HistoryRenderState::visible_cells()</code> receives those requests along with the    current <code>RenderSettings</code> (width, theme epoch, and reasoning visibility    toggle). The render state stores cached layouts in a <code>HashMap&lt;CacheKey,    CachedLayout&gt;</code>, where <code>CacheKey</code> is <code>(HistoryId, width, theme_epoch,    reasoning_visible)</code>.</li> <li>When a cache entry exists, <code>visible_cells()</code> returns the layout immediately.    Otherwise it invokes the builder closure supplied in the request. The    closure uses the <code>HistoryCell</code> if one exists, and falls back to the semantic    lines so the renderer can still make forward progress even when we no longer    materialize a trait object.</li> <li>The resulting <code>VisibleCell</code> bundles the resolved layout (plus the assistant    streaming plan, if applicable). <code>ChatWidget</code> uses these <code>VisibleCell</code>    snapshots to copy pre-rendered buffer rows into the terminal buffer.</li> </ol>"},{"location":"history_render_cache_bridge/#cache-invalidation","title":"Cache invalidation","text":"<p><code>HistoryRenderState</code> exposes several targeted invalidation helpers:</p> <ul> <li><code>invalidate_history_id(HistoryId)</code> removes cached layouts for a single   record. We call this whenever a record\u2019s semantic state changes (for example   when streaming assistant output arrives).</li> <li><code>handle_width_change(width)</code> prunes cache entries whose width no longer   matches the viewport. Width changes also clear prefix sums so we rebuild the   height map.</li> <li><code>invalidate_all()</code> clears the layout cache and prefix sums. We use this when   restoring snapshots or performing operations that massively reshuffle   history.</li> </ul> <p>The prefix sum cache (<code>prefix_sums</code>, <code>last_prefix_*</code> fields) is separate from the layout cache. It stores cumulative heights so that scrolling computations are O(1) per frame.</p>"},{"location":"history_render_cache_bridge/#fallback-behaviors","title":"Fallback behaviors","text":"<ul> <li><code>RenderRequest::fallback_lines</code> are derived from the semantic   <code>HistoryRecord</code>. This ensures that even if a legacy <code>HistoryCell</code> cache is   missing (for example, after deserializing a snapshot from disk) we still have   content to render.</li> <li><code>ChatWidget::draw_history</code> keeps the old <code>HistoryCell</code> objects around only as   an optimization; any cache miss rebuilds the layout from semantic state and   stores the result in the shared cache.</li> <li>Cells that still own per-width caches (<code>ExecCell</code>, diff/assistant streaming)   remain responsible for invalidating their internal caches. The history render   cache treats those cells as opaque; once Step\u202f6 lands we can remove the   per-cell caches and rely exclusively on <code>HistoryRenderState</code>.</li> </ul>"},{"location":"history_render_cache_bridge/#removal-roadmap","title":"Removal roadmap","text":"<p>The interim bridge exists solely to prevent regressions while we finish Step\u202f6 of the plan. Once all cells render from structured state and the shared cache proves stable, we can:</p> <ol> <li>Delete the per-cell layout caches (<code>ExecCell::cached_layout</code>, assistant    streaming wrappers, diff caches).</li> <li>Convert all render paths to supply semantic state only, removing    <code>HistoryCell</code> trait objects from the steady-state draw loop.</li> <li>Simplify <code>ChatWidget::draw_history</code> so it only iterates over    <code>HistoryState.records</code>.</li> </ol> <p>Tracking this work in the plan keeps the team aligned on why the bridge exists and what remains before it can be removed.</p>"},{"location":"history_state_schema/","title":"History State JSON Schema","text":"<p>This document captures the canonical JSON schema for the serialized history state used by the TUI. Each record stored in <code>HistoryState.records</code> is a tagged <code>HistoryRecord</code> enum. The schema follows the common structure below:</p> <pre><code>{\n  \"id\": &lt;number&gt;,\n  \"type\": &lt;string&gt;,\n  \"payload\": &lt;variant specific object&gt;\n}\n</code></pre> <ul> <li><code>id</code> \u2014 64-bit unsigned integer derived from <code>HistoryId</code>.</li> <li><code>type</code> \u2014 string enum discriminant (e.g. <code>\"plain_message\"</code>).</li> <li><code>payload</code> \u2014 variant data described in the sections that follow.</li> </ul> <p>All timestamps are serialized as RFC\u202f3339 strings, durations as ISO\u202f8601 duration strings, and enums as snake_case strings unless otherwise noted.</p>"},{"location":"history_state_schema/#variant-reference","title":"Variant Reference","text":""},{"location":"history_state_schema/#plainmessage","title":"PlainMessage","text":"<ul> <li><code>type</code>: <code>\"plain_message\"</code></li> <li><code>payload</code>:</li> <li><code>role</code>: <code>\"system\" | \"user\" | \"assistant\" | \"tool\" | \"error\" | \"background_event\"</code></li> <li><code>kind</code>: <code>\"plain\" | \"user\" | \"assistant\" | \"tool\" | \"error\" | \"background\" | \"notice\"</code></li> <li><code>header</code>: optional object <code>{ \"label\": string, \"badge\": string? }</code></li> <li><code>lines</code>: array of <code>MessageLine</code> objects</li> <li><code>metadata</code>: optional <code>MessageMetadata</code></li> </ul> <p><code>MessageLine</code> objects: - <code>kind</code>: <code>\"paragraph\" | \"bullet\" | \"code\" | \"quote\" | \"separator\" | \"metadata\" | \"blank\"</code> - <code>spans</code>: array of <code>{ \"text\": string, \"tone\": TextTone, \"emphasis\": Emphasis, \"entity\": Entity? }</code></p> <p><code>MessageMetadata</code>: - <code>citations</code>: array of citation strings - <code>token_usage</code>: optional <code>{ \"prompt\": number, \"completion\": number, \"total\": number }</code></p> <p><code>TextTone</code>: <code>\"default\" | \"dim\" | \"primary\" | \"success\" | \"warning\" | \"error\" | \"info\"</code></p> <p><code>Emphasis</code>: <code>{ \"bold\": bool, \"italic\": bool, \"dim\": bool, \"strike\": bool, \"underline\": bool }</code></p> <p><code>Entity</code>: <code>null | { \"type\": \"link\", \"href\": string } | { \"type\": \"code\" }</code></p>"},{"location":"history_state_schema/#waitstatus","title":"WaitStatus","text":"<ul> <li><code>type</code>: <code>\"wait_status\"</code></li> <li><code>payload</code>:</li> <li><code>header</code>: <code>{ \"title\": string, \"title_tone\": TextTone, \"summary\": string?, \"summary_tone\": TextTone }</code></li> <li><code>details</code>: array of <code>{ \"label\": string, \"value\": string?, \"tone\": TextTone }</code></li> </ul>"},{"location":"history_state_schema/#loading","title":"Loading","text":"<ul> <li><code>type</code>: <code>\"loading\"</code></li> <li><code>payload</code>: <code>{ \"message\": string }</code></li> </ul>"},{"location":"history_state_schema/#runningtool","title":"RunningTool","text":"<ul> <li><code>type</code>: <code>\"running_tool\"</code></li> <li><code>payload</code>:</li> <li><code>title</code>: string</li> <li><code>started_at</code>: timestamp</li> <li><code>arguments</code>: <code>ToolArgument[]</code></li> <li><code>wait_cap_ms</code>: number?</li> <li><code>wait_has_target</code>: bool</li> <li><code>wait_has_call_id</code>: bool</li> </ul>"},{"location":"history_state_schema/#toolcall","title":"ToolCall","text":"<ul> <li><code>type</code>: <code>\"tool_call\"</code></li> <li><code>payload</code>:</li> <li><code>status</code>: <code>\"running\" | \"success\" | \"failed\"</code></li> <li><code>title</code>: string</li> <li><code>duration_ms</code>: number?</li> <li><code>arguments</code>: <code>ToolArgument[]</code></li> <li><code>result_preview</code>: <code>{ \"lines\": string[], \"truncated\": bool }?</code></li> <li><code>error_message</code>: string?</li> </ul> <p><code>ToolArgument</code>: - <code>{ \"name\": string, \"value\": Text | Json | Secret }</code> - <code>Text</code>: <code>{ \"type\": \"text\", \"text\": string }</code> - <code>Json</code>: <code>{ \"type\": \"json\", \"value\": any }</code> - <code>Secret</code>: <code>{ \"type\": \"secret\" }</code></p>"},{"location":"history_state_schema/#planupdate","title":"PlanUpdate","text":"<ul> <li><code>type</code>: <code>\"plan_update\"</code></li> <li><code>payload</code>:</li> <li><code>name</code>: string</li> <li><code>icon</code>: <code>\"light_bulb\" | \"rocket\" | \"clipboard\" | { \"custom\": string }</code></li> <li><code>progress</code>: <code>{ \"completed\": number, \"total\": number }</code></li> <li><code>steps</code>: array of <code>{ \"description\": string, \"status\": StepStatus }</code></li> </ul> <p><code>StepStatus</code>: <code>\"pending\" | \"in_progress\" | \"complete\" | \"skipped\"</code></p>"},{"location":"history_state_schema/#upgradenotice","title":"UpgradeNotice","text":"<ul> <li><code>type</code>: <code>\"upgrade_notice\"</code></li> <li><code>payload</code>: <code>{ \"current_version\": string, \"latest_version\": string, \"message\": string }</code></li> </ul>"},{"location":"history_state_schema/#reasoning","title":"Reasoning","text":"<ul> <li><code>type</code>: <code>\"reasoning\"</code></li> <li><code>payload</code>:</li> <li><code>in_progress</code>: bool</li> <li><code>sections</code>: array of <code>{ \"heading\": string?, \"summary\": InlineSpan[], \"blocks\": ReasoningBlock[] }</code></li> <li><code>hide_when_collapsed</code>: bool</li> </ul> <p><code>ReasoningBlock</code>: one of - <code>{ \"type\": \"paragraph\", \"spans\": InlineSpan[] }</code> - <code>{ \"type\": \"bullet\", \"indent\": number, \"marker\": string, \"spans\": InlineSpan[] }</code> - <code>{ \"type\": \"code\", \"language\": string?, \"content\": string }</code> - <code>{ \"type\": \"quote\", \"spans\": InlineSpan[] }</code> - <code>{ \"type\": \"separator\" }</code></p>"},{"location":"history_state_schema/#exec","title":"Exec","text":"<ul> <li><code>type</code>: <code>\"exec\"</code></li> <li><code>payload</code>:</li> <li><code>call_id</code>: string?</li> <li><code>command</code>: string[]</li> <li><code>parsed</code>: array of <code>ParsedCommand</code></li> <li><code>action</code>: <code>\"read\" | \"search\" | \"list\" | \"run\"</code></li> <li><code>status</code>: <code>\"running\" | \"success\" | \"error\"</code></li> <li><code>stdout_chunks</code>: array of <code>{ \"offset\": number, \"content\": string }</code></li> <li><code>stderr_chunks</code>: array of <code>{ \"offset\": number, \"content\": string }</code></li> <li><code>exit_code</code>: number?</li> <li><code>wait_total_ms</code>: number?</li> <li><code>wait_active</code>: bool</li> <li><code>wait_notes</code>: array of <code>{ \"message\": string, \"tone\": TextTone, \"timestamp\": timestamp }</code></li> <li><code>started_at</code>: timestamp</li> <li><code>completed_at</code>: timestamp?</li> </ul> <p><code>ParsedCommand</code> mirrors the parsed shell command structure emitted by <code>codex_core::parse_command</code>.</p>"},{"location":"history_state_schema/#assistantstream","title":"AssistantStream","text":"<ul> <li><code>type</code>: <code>\"assistant_stream\"</code></li> <li><code>payload</code>:</li> <li><code>stream_id</code>: string</li> <li><code>preview_markdown</code>: string</li> <li><code>deltas</code>: array of <code>{ \"delta\": string, \"sequence\": number?, \"received_at\": timestamp }</code></li> <li><code>citations</code>: string[]</li> <li><code>metadata</code>: <code>MessageMetadata?</code></li> <li><code>in_progress</code>: bool</li> <li><code>last_updated_at</code>: timestamp</li> </ul>"},{"location":"history_state_schema/#assistantmessage","title":"AssistantMessage","text":"<ul> <li><code>type</code>: <code>\"assistant_message\"</code></li> <li><code>payload</code>:</li> <li><code>stream_id</code>: string?</li> <li><code>markdown</code>: string</li> <li><code>citations</code>: string[]</li> <li><code>metadata</code>: <code>MessageMetadata?</code></li> <li><code>token_usage</code>: <code>{ \"prompt\": number, \"completion\": number, \"total\": number }?</code></li> <li><code>created_at</code>: timestamp</li> </ul>"},{"location":"history_state_schema/#diff","title":"Diff","text":"<ul> <li><code>type</code>: <code>\"diff\"</code></li> <li><code>payload</code>:</li> <li><code>hunks</code>: array of <code>{ \"header\": string, \"lines\": DiffLine[] }</code></li> </ul> <p><code>DiffLine</code>: <code>{ \"kind\": \"context\" | \"added\" | \"removed\", \"text\": string }</code></p>"},{"location":"history_state_schema/#image","title":"Image","text":"<ul> <li><code>type</code>: <code>\"image\"</code></li> <li><code>payload</code>: <code>{ \"path\": string, \"caption\": string?, \"metadata\": object? }</code></li> </ul>"},{"location":"history_state_schema/#explore","title":"Explore","text":"<ul> <li><code>type</code>: <code>\"explore\"</code></li> <li><code>payload</code>: <code>{ \"entries\": ExploreEntry[], \"trailing\": bool }</code></li> </ul> <p><code>ExploreEntry</code>: <code>{ \"command\": string[], \"cwd\": string?, \"status\": ExploreStatus }</code></p> <p><code>ExploreStatus</code>: <code>\"running\" | \"success\" | \"not_found\" | { \"error\": { \"exit_code\": number? } }</code></p>"},{"location":"history_state_schema/#ratelimits","title":"RateLimits","text":"<ul> <li><code>type</code>: <code>\"rate_limits\"</code></li> <li><code>payload</code>: mirrors <code>RateLimitSnapshotEvent</code> with primary/secondary usage percentages   and reset timestamps.</li> </ul>"},{"location":"history_state_schema/#patch","title":"Patch","text":"<ul> <li><code>type</code>: <code>\"patch\"</code></li> <li><code>payload</code>:</li> <li><code>event</code>: <code>\"apply_begin\" | \"apply_success\" | \"apply_failure\" | \"proposed\"</code></li> <li><code>auto_approved</code>: bool?</li> <li><code>changes</code>: map of path \u2192 <code>FileChange</code></li> <li><code>failure</code>: <code>{ \"message\": string, \"stdout_excerpt\": string?, \"stderr_excerpt\": string? }?</code></li> </ul>"},{"location":"history_state_schema/#backgroundevent","title":"BackgroundEvent","text":"<ul> <li><code>type</code>: <code>\"background_event\"</code></li> <li><code>payload</code>: <code>{ \"title\": string, \"description\": string }</code></li> </ul>"},{"location":"history_state_schema/#notice","title":"Notice","text":"<ul> <li><code>type</code>: <code>\"notice\"</code></li> <li><code>payload</code>: <code>{ \"title\": string?, \"body\": MessageLine[] }</code></li> </ul>"},{"location":"history_state_schema/#snapshot-structure","title":"Snapshot Structure","text":"<p>A serialized history snapshot is the object returned by <code>HistoryState::snapshot()</code>:</p> <pre><code>{\n  \"records\": [ &lt;HistoryRecord&gt;... ],\n  \"next_id\": &lt;number&gt;,\n  \"exec_call_lookup\": { \"call_id\": HistoryId },\n  \"tool_call_lookup\": { \"call_id\": HistoryId },\n  \"stream_lookup\": { \"stream_id\": HistoryId }\n}\n</code></pre> <p>Applications consuming the schema should treat unknown fields as forward compatible extensions and must not rely on enum ordering.</p>"},{"location":"install/","title":"Installation","text":""},{"location":"install/#install-build","title":"Install &amp; build","text":""},{"location":"install/#system-requirements","title":"System requirements","text":"Requirement Details Operating systems macOS 12+, Ubuntu 20.04+/Debian 10+, or Windows 11 via WSL2 Git (optional, recommended) 2.23+ for built-in PR helpers RAM 4-GB minimum (8-GB recommended)"},{"location":"install/#dotslash","title":"DotSlash","text":"<p>The GitHub Release also contains a DotSlash file for the Codex CLI named <code>codex</code>. Using a DotSlash file makes it possible to make a lightweight commit to source control to ensure all contributors use the same version of an executable, regardless of what platform they use for development.</p>"},{"location":"install/#build-from-source","title":"Build from source","text":"<pre><code># Clone the repository and navigate to the root of the Cargo workspace.\ngit clone https://github.com/openai/codex.git\ncd codex/code-rs\n\n# Install the Rust toolchain, if necessary.\ncurl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y\nsource \"$HOME/.cargo/env\"\nrustup component add rustfmt\nrustup component add clippy\n\n# Build Code.\ncargo build\n\n# Launch the TUI with a sample prompt.\ncargo run --bin code -- \"explain this codebase to me\"\n\n# After making changes, ensure the code is clean.\ncargo fmt -- --config imports_granularity=Item\ncargo clippy --tests\n\n# Run the tests.\ncargo test\n</code></pre>"},{"location":"integration-zed/","title":"Zed Integration","text":"<p>To point Zed at Code's ACP server, add this block to <code>settings.json</code>:</p> <pre><code>{\n  \"agent_servers\": {\n    \"Code\": {\n      \"command\": \"npx\",\n      \"args\": [\"-y\", \"@just-every/code\", \"acp\"]\n    }\n  }\n}\n</code></pre> <p>Adjust the <code>command</code> or <code>args</code> only if you pin a different version or use a globally installed binary.</p>"},{"location":"integration-zed/#zed-prerequisites","title":"Zed prerequisites","text":"<ul> <li>Zed Stable <code>0.201.5</code> (released August 27, 2025) or newer adds ACP support with the Agent Panel. Update via <code>Zed \u2192 Check for Updates</code> before wiring Code in. Zed\u2019s docs call out ACP as the mechanism powering Gemini CLI and other external agents.</li> <li>External agents live inside the Agent Panel (<code>cmd-?</code>). Use the <code>+</code> button to start a new thread and pick <code>Code</code> from the external agent list. Zed runs our CLI as a subprocess over JSON\u2011RPC, so all prompts and diff previews stay local.</li> <li>Zed installs dependencies per entry automatically. If you keep <code>command = \"npx\"</code>, Zed will download the published <code>@just-every/code</code> package the first time you trigger the integration.</li> </ul>"},{"location":"integration-zed/#how-code-implements-acp","title":"How Code implements ACP","text":"<ul> <li>The Rust MCP server exposes ACP tools: <code>session/new</code>, <code>session/prompt</code>, and fast interrupts via <code>session/cancel</code>. These are backed by the same conversation manager that powers the TUI, so approvals, confirm guards, and sandbox policies remain intact.</li> <li>Streaming <code>session/update</code> notifications bridge Codex events into Zed. You get Answer/Reasoning updates, shell command progress, approvals, and apply_patch diffs in the Zed UI without losing terminal parity.</li> <li>MCP configuration stays centralized in <code>CODEX_HOME/config.toml</code>. Use <code>[experimental_client_tools]</code> to delegate file read/write and permission requests back to Zed when you want its UI to handle approvals.</li> <li>MCP configuration stays centralized in <code>CODEX_HOME/config.toml</code>. Use <code>[experimental_client_tools]</code> to delegate file read/write and permission requests back to Zed when you want its UI to handle approvals. A minimal setup looks like:</li> </ul> <pre><code>[experimental_client_tools]\nrequest_permission = { mcp_server = \"zed\", tool_name = \"requestPermission\" }\nread_text_file = { mcp_server = \"zed\", tool_name = \"readTextFile\" }\nwrite_text_file = { mcp_server = \"zed\", tool_name = \"writeTextFile\" }\n</code></pre> <p>Zed wires these tools automatically when you add the Code agent, so the identifiers above match the defaults. - The CLI entry point (<code>npx @just-every/code acp</code>) is a thin wrapper over the Rust binary (<code>cargo run -p code-mcp-server -- --stdio</code>) that ships alongside the rest of Code. Build-from-source workflows plug in by swapping <code>command</code> for an absolute path to that binary.</p>"},{"location":"integration-zed/#tips-and-troubleshooting","title":"Tips and troubleshooting","text":"<ul> <li>Need to inspect the handshake? Run Zed\u2019s <code>dev: open acp logs</code> command from the Command Palette; the log shows JSON\u2011RPC requests and Codex replies.</li> <li>If prompts hang, make sure no other process is bound to the same MCP port and that your <code>CODEX_HOME</code> points to the intended config directory. The ACP server inherits all of Code\u2019s sandbox settings, so restrictive policies (e.g., <code>approval_policy = \"never\"</code>) still apply.</li> <li>Zed currently skips history restores and checkpoint UI for third-party agents. Stick to the TUI if you rely on those features; ACP support is still evolving upstream.</li> <li>After a session starts, the model selector in Zed lists Code\u2019s built-in presets (e.g., <code>gpt-5-codex</code>, <code>gpt-5</code> high/medium/low). Picking a new preset updates the running Codex session immediately, so you don\u2019t have to restart the agent to change models.</li> </ul>"},{"location":"interrupt-resume-postmortem/","title":"Interrupt \u2192 Resume Postmortem","text":"<p>This document summarizes the root cause, symptoms, and fixes for the \"Esc then send never resumes\" issue in the Code (Codex CLI, Rust) TUI/core stack.</p>"},{"location":"interrupt-resume-postmortem/#summary","title":"Summary","text":"<p>Pressing Esc (or Ctrl+C) to interrupt a running turn, then immediately entering a new message, frequently resulted in no new model turn starting. The UI showed the user message, but there was no <code>TaskStarted</code>, and sometimes the system appeared to hang until forcibly terminated.</p> <p>Two independent problems contributed to this behavior:</p> <p>1) A deadlock in <code>Session::abort()</code> in the core. 2) The TUI queuing policy after interrupts, which could strand messages.</p>"},{"location":"interrupt-resume-postmortem/#symptoms","title":"Symptoms","text":"<ul> <li>After Esc, the next user message shows in history but no assistant output or   <code>TaskStarted</code> appears.</li> <li>A second Ctrl+C exits the TUI, but the background task may linger.</li> <li>In earlier runs, messages sent while a task was running were queued waiting   for <code>TaskComplete</code> (which never arrives after an <code>Interrupt</code>).</li> </ul>"},{"location":"interrupt-resume-postmortem/#root-cause","title":"Root Cause","text":""},{"location":"interrupt-resume-postmortem/#1-deadlock-in-sessionabort","title":"1) Deadlock in <code>Session::abort()</code>","text":"<p><code>Session::abort()</code> held the session mutex (<code>self.state.lock()</code>) and then called <code>agent.abort()</code>. The <code>AgentAgent::abort()</code> emitted a protocol event using <code>Session::make_event()</code>, which also attempted to lock <code>self.state</code>. This re-entrant lock acquisition resulted in a deadlock:</p> <ul> <li><code>Session::abort()</code> acquires the lock.</li> <li>Calls <code>agent.abort()</code> while still holding the lock.</li> <li><code>agent.abort()</code> \u2192 <code>make_event()</code> tries to acquire the same lock \u2192 blocks   forever.</li> </ul> <p>Result: the submission loop remained stuck in the interrupt path and never got to process the next <code>UserInput</code> turn.</p>"},{"location":"interrupt-resume-postmortem/#2-stranded-messages-after-interrupts","title":"2) Stranded messages after interrupts","text":"<p>Historically the TUI queued new user messages while a task was running, and dispatched them on <code>TaskComplete</code>. After an <code>Interrupt</code>, there is no <code>TaskComplete</code>, so the queued message could be left stranded indefinitely.</p>"},{"location":"interrupt-resume-postmortem/#fixes","title":"Fixes","text":"<p>1) Remove the deadlock in <code>Session::abort()</code>:    - Take (<code>take()</code>) the <code>current_agent</code> while holding the mutex, then drop the      lock before calling <code>agent.abort()</code>. This ensures <code>make_event()</code> can safely      acquire the lock again and avoids the self-deadlock.</p> <p>2) Stronger atomicity around new input after interrupts:    - For <code>Op::UserInput</code>, abort synchronously first (to ensure the prior agent      is gone), then spawn and set the new agent. This prevents races where an      async abort could kill the newly spawned agent.</p> <p>3) TUI sending policy:    - Always send user messages immediately, even if a task appears to be      running. The core\u2019s <code>UserInput</code> path aborts the prior agent and starts a      fresh turn, so messages are never stranded waiting for a <code>TaskComplete</code>.</p> <p>4) UX guards (non-functional):    - Ignore late deltas after an interrupt so partial output does not trickle in      post-cancel.    - Drop late <code>ExecEnd</code> events for commands that were already marked as      cancelled to avoid duplicate cells.</p>"},{"location":"interrupt-resume-postmortem/#lessons-learned","title":"Lessons Learned","text":"<ul> <li>Never call back into code that may re-acquire a mutex you currently hold.   Move side-effects (like emitting events) outside the lock\u2019s critical section.</li> <li>Prefer a simple, predictable policy after interrupts: abort first, then spawn   the next turn. Avoid queuing by frontends unless they can prove delivery.</li> <li>When diagnosing event-ordered systems, add short-lived diagnostics at the   critical edges (submit, loop recv, abort begin/end, agent spawn, run begin)   and then remove them once the root cause is fixed.</li> </ul>"},{"location":"interrupt-resume-postmortem/#filesareas-changed","title":"Files/Areas Changed","text":"<ul> <li><code>core/src/codex.rs</code></li> <li><code>Session::abort()</code> releases the lock before calling <code>agent.abort()</code>.</li> <li> <p><code>Op::UserInput</code> aborts synchronously before spawning the next agent.</p> </li> <li> <p><code>tui/src/chatwidget.rs</code></p> </li> <li>Always send user input immediately (no queuing after interrupts).</li> <li>Ignore late deltas after cancel; drop late ExecEnd for cancelled calls.</li> </ul> <p>These changes resolved the \u201cEsc then send never resumes\u201d issue and restored a reliable cancel\u2192resume experience.</p>"},{"location":"license/","title":"License","text":""},{"location":"license/#license","title":"License","text":"<p>This repository is licensed under the Apache-2.0 License.</p>"},{"location":"markdown_showcase/","title":"Markdown Showcase","text":"<p>A quick tour of many common Markdown elements so you can verify rendering.</p>"},{"location":"markdown_showcase/#headings","title":"Headings","text":""},{"location":"markdown_showcase/#h1","title":"H1","text":""},{"location":"markdown_showcase/#h2","title":"H2","text":""},{"location":"markdown_showcase/#h3","title":"H3","text":""},{"location":"markdown_showcase/#h4","title":"H4","text":""},{"location":"markdown_showcase/#h5","title":"H5","text":""},{"location":"markdown_showcase/#h6","title":"H6","text":""},{"location":"markdown_showcase/#emphasis-inline","title":"Emphasis &amp; Inline","text":"<ul> <li>Plain: text</li> <li>Italic: text or text</li> <li>Bold: text or text</li> <li>Bold + Italic: text</li> <li>Strikethrough: ~~text~~</li> <li>Code: <code>let answer = 42;</code></li> <li>Escaping: *literal asterisks*, _underscores_, `backticks`</li> </ul>"},{"location":"markdown_showcase/#blockquotes","title":"Blockquotes","text":"<p>Note: This is a single-level quote.</p> <ul> <li>It can include lists</li> <li>And <code>inline code</code></li> </ul> <p>Nested quote level two.</p> <ul> <li>Deep thought here</li> </ul>"},{"location":"markdown_showcase/#lists","title":"Lists","text":"<ul> <li>Unordered list item</li> <li>Nested item<ul> <li>Third level</li> </ul> </li> <li> <p>Another item</p> </li> <li> <p>Ordered list item</p> </li> <li>Nested item</li> <li>Another nested item</li> <li> <p>Second item</p> </li> <li> <p>Mixed list</p> </li> <li>Ordered inside unordered</li> <li>Still works</li> </ul>"},{"location":"markdown_showcase/#task-lists","title":"Task Lists","text":"<ul> <li> Render checkboxes</li> <li> Support unchecked state</li> <li> Handle nested tasks</li> <li> Child complete</li> <li> Child pending</li> </ul>"},{"location":"markdown_showcase/#links","title":"Links","text":"<ul> <li>Inline: OpenAI</li> <li>Title: Rust</li> <li>Reference: Codex CLI and CommonMark</li> </ul>"},{"location":"markdown_showcase/#images","title":"Images","text":"<p> Caption: Placeholder banner to test image rendering.</p> <p></p>"},{"location":"markdown_showcase/#code-blocks","title":"Code Blocks","text":"<pre><code>// Rust example\nfn greet(name: &amp;str) -&gt; String {\n    format!(\"Hello, {}!\", name)\n}\n\nfn main() {\n    println!(\"{}\", greet(\"Codex\"));\n}\n</code></pre> <pre><code># Shell example\nset -euo pipefail\n./build-fast.sh\necho \"Build finished.\"\n</code></pre> <pre><code>{\n  \"name\": \"codex-demo\",\n  \"version\": \"1.0.0\",\n  \"private\": true\n}\n</code></pre> <pre><code># Cargo-like TOML\n[package]\nname = \"codex-demo\"\nversion = \"0.1.0\"\nedition = \"2021\"\n</code></pre> <pre><code>diff --git a/hello.txt b/hello.txt\n--- a/hello.txt\n+++ b/hello.txt\n@@\n-Hello Wordl\n+Hello World\n</code></pre>"},{"location":"markdown_showcase/#simple-tables","title":"Simple Tables","text":"First Header Second Header Content Cell Content Cell Content Cell Content Cell Command Description git status List all new or modified files git diff Show file differences that haven't been staged"},{"location":"markdown_showcase/#complex-table","title":"Complex Table","text":"Name Type Stars Notes codex-core library 4,512 Core execution logic codex-tui app 1,203 Terminal UI codex-web app 987 Browser front-end <p>Alignment: left, center, right verified above.</p> Command Description <code>git status</code> List all new or modified files <code>git diff</code> Show file differences that haven't been staged <p>Contains markdown</p>"},{"location":"markdown_showcase/#footnotes","title":"Footnotes","text":"<p>Here\u2019s a sentence with a footnote marker.[^1] And another one right here.[^ref]</p> <p>[^1]: This is the first footnote with more detail. [^ref]: A second footnote to test multiple references.</p>"},{"location":"markdown_showcase/#horizontal-rules","title":"Horizontal Rules","text":""},{"location":"markdown_showcase/#collapsible","title":"Collapsible","text":"Click to expand  - This content is hidden by default. - It can include lists and code:  <pre><code>Hidden block content.\n</code></pre>"},{"location":"markdown_showcase/#definition-list","title":"Definition List","text":"<p>Term : The thing being defined.</p> <p>Another Term : A second definition with more words for testing.</p>"},{"location":"markdown_showcase/#callouts-quote-style","title":"Callouts (Quote-style)","text":"<p>[!NOTE] This is a helpful note for readers.</p> <p>[!TIP] Use format!(\"{value}\") to inline variables.</p> <p>[!WARNING] Treat all warnings as failures during builds.</p> <p>[!IMPORTANT] Never modify sandbox env var handling in code.</p>"},{"location":"markdown_showcase/#mermaid-diagrams","title":"Mermaid (Diagrams)","text":"<pre><code>flowchart TD\n    A[Start] --&gt; B{Build?}\n    B -- yes --&gt; C[Run ./build-fast.sh]\n    B -- no --&gt; D[Edit code]\n    C --&gt; E[Success]\n    D --&gt; B</code></pre>"},{"location":"markdown_showcase/#mixed-content","title":"Mixed Content","text":"<ul> <li>Emoji: \u2705 \ud83d\ude80 \ud83d\udca1</li> <li>Math (literal text): <code>E = mc^2</code></li> <li>Monospace paths: <code>/usr/local/bin</code>, <code>src/main.rs</code></li> <li>HTML inline: <sup>sup</sup> and <sub>sub</sub></li> </ul> <p>If you want more of any specific element (e.g., longer tables, nested lists, or additional code languages), tell me what to stress-test next.</p>"},{"location":"open-source-fund/","title":"Open Source Fund","text":""},{"location":"open-source-fund/#codex-open-source-fund","title":"Codex open source fund","text":"<p>We're excited to launch a $1 million initiative supporting open source projects that use Codex CLI and other OpenAI models.</p> <ul> <li>Grants are awarded up to $25,000 API credits.</li> <li>Applications are reviewed on a rolling basis.</li> </ul> <p>Interested? Apply here.</p>"},{"location":"platform-sandboxing/","title":"Platform Sandboxing","text":""},{"location":"platform-sandboxing/#platform-sandboxing-details","title":"Platform sandboxing details","text":"<p>The mechanism Codex uses to implement the sandbox policy depends on your OS:</p> <ul> <li>macOS 12+ uses Apple Seatbelt and runs commands using <code>sandbox-exec</code> with a profile (<code>-p</code>) that corresponds to the <code>--sandbox</code> that was specified.</li> <li>Linux uses a combination of Landlock/seccomp APIs to enforce the <code>sandbox</code> configuration.</li> </ul> <p>Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running <code>codex</code> with <code>--sandbox danger-full-access</code> (or, more simply, the <code>--dangerously-bypass-approvals-and-sandbox</code> flag) within your container.</p>"},{"location":"prompts/","title":"Prompts","text":""},{"location":"prompts/#custom-prompts","title":"Custom Prompts","text":"<p>Save frequently used prompts as Markdown files and reuse them quickly from the slash menu.</p> <ul> <li>Location: Put files in <code>$CODE_HOME/prompts/</code> (defaults to <code>~/.code/prompts/</code>; Code also reads legacy <code>~/.codex/prompts/</code>).</li> <li>File type: Only Markdown files with the <code>.md</code> extension are recognized.</li> <li>Name: The filename without the <code>.md</code> extension becomes the slash entry. For a file named <code>my-prompt.md</code>, type <code>/my-prompt</code>.</li> <li>Content: The file contents are sent as your message when you select the item in the slash popup and press Enter.</li> <li>Arguments: Local prompts support placeholders in their content:</li> <li><code>$1..$9</code> expand to the first nine positional arguments typed after the slash name</li> <li><code>$ARGUMENTS</code> expands to all arguments joined by a single space</li> <li><code>$$</code> is preserved literally</li> <li>Quoted args: Wrap a single argument in double quotes to include spaces, e.g. <code>/review \"docs/My File.md\"</code>.</li> <li>How to use:</li> <li>Start a new session (Codex loads custom prompts on session start).</li> <li>In the composer, type <code>/</code> to open the slash popup and begin typing your prompt name.</li> <li>Use Up/Down to select it. Press Enter to submit its contents, or Tab to autocomplete the name.</li> <li>Notes:</li> <li>Files with names that collide with built\u2011in commands (e.g. <code>/init</code>) are ignored and won\u2019t appear.</li> <li>New or changed files are discovered on session start. If you add a new prompt while Codex is running, start a new session to pick it up.</li> </ul>"},{"location":"references/","title":"References and Research","text":""},{"location":"references/#academic-papers","title":"Academic Papers","text":""},{"location":"references/#aggregation-methods","title":"Aggregation Methods","text":""},{"location":"references/#moa-mixture-of-agents","title":"MOA (Mixture of Agents)","text":"<p>Mixture of Agents: Enhancing LLM Capabilities through Collaborative Specialization</p> <ul> <li>Authors: Anonymous (arXiv submission)</li> <li>Year: 2025</li> <li>URL: https://arxiv.org/abs/2502.04913</li> <li>Citation: Introduces the MOA aggregation method for horizontal diversity in LLM reasoning</li> <li>Key Concepts:</li> <li>Diverse completions via temperature sampling</li> <li>Critique and synthesis workflow</li> <li>Collaborative refinement of solutions</li> </ul>"},{"location":"references/#mcts-monte-carlo-tree-search","title":"MCTS (Monte Carlo Tree Search)","text":"<p>Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</p> <ul> <li>Authors: R\u00e9mi Coulom</li> <li>Year: 2006</li> <li>URL: https://dblp.org/rec/journals/cg/Coulom06.html</li> <li>Citation: Foundational work on UCB selection and efficient tree search</li> <li>Key Concepts:</li> <li>Upper Confidence Bounds (UCB) formula</li> <li>Selection, expansion, simulation, backpropagation phases</li> <li>Efficient leaf node backup strategies</li> </ul>"},{"location":"references/#related-work","title":"Related Work","text":""},{"location":"references/#tree-of-thoughts","title":"Tree of Thoughts","text":"<p>Tree of Thoughts: Deliberate Problem Solving with Large Language Models</p> <ul> <li>Authors: Yao et al.</li> <li>Year: 2023</li> <li>URL: https://arxiv.org/abs/2305.10601</li> <li>Relevance: Tree-based LLM reasoning with explicit state exploration</li> <li>Related to: MCTS implementation in MARS</li> </ul>"},{"location":"references/#chain-of-thought-prompting","title":"Chain-of-Thought Prompting","text":"<p>Chain-of-Thought Prompting Elicits Reasoning in Large Language Models</p> <ul> <li>Authors: Wei et al.</li> <li>Year: 2023</li> <li>URL: https://arxiv.org/abs/2201.11903</li> <li>Relevance: Foundational reasoning technique used in all phases</li> <li>Related to: Base prompting strategy in MARS</li> </ul>"},{"location":"references/#ensemble-methods","title":"Ensemble Methods","text":""},{"location":"references/#mixture-of-experts-moe","title":"Mixture of Experts (MoE)","text":"<p>Outrageously Large Neural Networks for Efficient Conditional Computation</p> <ul> <li>Authors: Shazeer et al.</li> <li>Year: 2017</li> <li>URL: https://arxiv.org/abs/1701.06538</li> <li>Relevance: Inspired population-based approaches in MARS</li> <li>Related to: RSA aggregation strategy</li> </ul>"},{"location":"references/#ensemble-learning","title":"Ensemble Learning","text":"<p>Ensemble Methods in Machine Learning</p> <ul> <li>Authors: Zhou, Z. H.</li> <li>Year: 2012</li> <li>URL: https://ieeexplore.ieee.org/abstract/document/6271705</li> <li>Relevance: Theoretical foundation for multi-agent approaches</li> <li>Related to: Cross-agent verification and voting</li> </ul>"},{"location":"references/#optimization-techniques","title":"Optimization Techniques","text":""},{"location":"references/#temperature-based-sampling","title":"Temperature-Based Sampling","text":"<p>Temperature controls randomness in language model outputs:</p> <ul> <li>Low (0.3): Deterministic, focused responses</li> <li>Medium (0.6): Balanced exploration</li> <li>High (1.0): Creative, diverse responses</li> </ul> <p>Reference: OpenAI API documentation on temperature parameter</p>"},{"location":"references/#verification-strategies","title":"Verification Strategies","text":""},{"location":"references/#consensus-based-verification","title":"Consensus-Based Verification","text":"<ul> <li>Multiple agents verify each solution</li> <li>Threshold-based consensus (default: 2 passes)</li> <li>Feedback-driven improvement</li> </ul> <p>Related to: Phase 3 in MARS pipeline</p>"},{"location":"references/#multi-agent-systems","title":"Multi-Agent Systems","text":""},{"location":"references/#multi-agent-reinforcement-learning","title":"Multi-Agent Reinforcement Learning","text":"<ul> <li>Coordination mechanisms</li> <li>Shared reward structures</li> <li>Convergence properties</li> </ul> <p>Reference: Busoniu et al. \"Multi-agent reinforcement learning: An overview\"</p>"},{"location":"references/#framework-references","title":"Framework References","text":""},{"location":"references/#rust-async-ecosystem","title":"Rust Async Ecosystem","text":"<ul> <li>Tokio: Async runtime</li> <li>Async-trait: Trait-based async abstractions</li> <li>serde: Data serialization</li> </ul>"},{"location":"references/#benchmarking","title":"Benchmarking","text":""},{"location":"references/#aime-2025","title":"AIME 2025","text":"<p>American Invitational Mathematics Examination</p> <ul> <li>Baseline (GPT-4): 43.3%</li> <li>MARS Result: 73.3%</li> <li>Improvement: +30 percentage points (+69% relative)</li> </ul>"},{"location":"references/#imo-2025","title":"IMO 2025","text":"<p>International Mathematical Olympiad</p> <ul> <li>Baseline: 16.7%</li> <li>MARS Result: 33.3%</li> <li>Improvement: +16.7 percentage points (+100% relative)</li> </ul>"},{"location":"references/#livecodebench-v5v6","title":"LiveCodeBench v5/v6","text":"<p>Code generation and debugging benchmark</p> <ul> <li>Baseline: 39.05%</li> <li>MARS Result: 50.48%</li> <li>Improvement: +11.43 percentage points (+29% relative)</li> </ul>"},{"location":"references/#recommended-reading-order","title":"Recommended Reading Order","text":""},{"location":"references/#beginner","title":"Beginner","text":"<ol> <li>Start: Getting Started</li> <li>Quick Start: 5-Minute Setup</li> <li>Aggregation: Overview</li> </ol>"},{"location":"references/#intermediate","title":"Intermediate","text":"<ol> <li>Architecture: System Design</li> <li>MOA: Mixture of Agents</li> <li>MCTS: Tree Search</li> </ol>"},{"location":"references/#advanced","title":"Advanced","text":"<ol> <li>Configuration: Full Guide</li> <li>API: Type Reference</li> <li>Papers: Research Links (this page)</li> </ol>"},{"location":"references/#contributing-citations","title":"Contributing &amp; Citations","text":""},{"location":"references/#how-to-cite-mars","title":"How to Cite MARS","text":"<pre><code>@software{mars-2025,\n  title={MARS: Multi-Agent Reasoning System},\n  author={Code Project},\n  year={2025},\n  url={https://github.com/GeorgePearse/code},\n  note={Rust implementation of advanced LLM optimization}\n}\n</code></pre>"},{"location":"references/#how-to-cite-individual-components","title":"How to Cite Individual Components","text":"<p>For MOA aggregation: <pre><code>@arXiv{mixture_of_agents_2025,\n  title={Mixture of Agents: Enhancing LLM Capabilities through Collaborative Specialization},\n  year={2025},\n  url={https://arxiv.org/abs/2502.04913}\n}\n</code></pre></p> <p>For MCTS: <pre><code>@article{coulom2006efficient,\n  title={Efficient selectivity and backup operators in Monte-Carlo tree search},\n  author={Coulom, R{\\'e}mi},\n  journal={Computers and Games},\n  pages={72--83},\n  year={2006},\n  publisher={Springer}\n}\n</code></pre></p>"},{"location":"references/#key-concepts-explained","title":"Key Concepts Explained","text":""},{"location":"references/#ucb-upper-confidence-bounds","title":"UCB (Upper Confidence Bounds)","text":"<p>Formula used in MCTS node selection:</p> <pre><code>UCB = (value/visits) + C * sqrt(ln(parent_visits) / visits)\n</code></pre> <ul> <li>value/visits: Exploitation - how good is this node?</li> <li>C * sqrt(...): Exploration - how uncertain are we?</li> <li>Higher UCB = more promising node</li> </ul>"},{"location":"references/#consensus-verification","title":"Consensus Verification","text":"<p>Solution is marked verified when:</p> <ol> <li>Passes verification check (is_correct = true)</li> <li>Receives a score (0.0-1.0)</li> <li>Gets 2 consecutive passes with no failures</li> </ol>"},{"location":"references/#temperature-sampling","title":"Temperature Sampling","text":"<p>Controls randomness in LLM outputs:</p> <ul> <li>Temperature = 0: Deterministic</li> <li>Temperature = 1.0: Standard randomness</li> <li>Temperature &gt; 1.0: More random</li> </ul> <p>MARS uses: 0.3 (conservative), 0.6 (balanced), 1.0 (creative)</p>"},{"location":"references/#related-projects","title":"Related Projects","text":""},{"location":"references/#optimllm","title":"OptimLLM","text":"<p>Repository: https://github.com/coohom/optillm</p> <p>The Python implementation that inspired MARS. Contains: - Original MOA implementation - MCTS algorithm reference - Prompt templates - Benchmark code</p>"},{"location":"references/#litellm","title":"LiteLLM","text":"<p>Repository: https://github.com/BerriAI/litellm</p> <p>Multi-model LLM interface. MARS uses litellm-rs for: - Provider routing - Model abstraction - Cost tracking</p>"},{"location":"references/#code-project","title":"Code Project","text":"<p>Repository: https://github.com/GeorgePearse/code</p> <p>Full context codebase with: - MARS (this project) - ModelClient for LLM access - CLI integration - Additional optimization techniques</p>"},{"location":"references/#learning-resources","title":"Learning Resources","text":""},{"location":"references/#understanding-tree-search","title":"Understanding Tree Search","text":"<ul> <li>AlphaGo paper for MCTS in games</li> <li>AlphaZero for pure tree search RL</li> <li>Sutton &amp; Barto for RL foundations</li> </ul>"},{"location":"references/#language-model-reasoning","title":"Language Model Reasoning","text":"<ul> <li>GPT-4 technical report</li> <li>Constitutional AI papers</li> <li>Prompt engineering guides</li> </ul>"},{"location":"references/#rust-systems-programming","title":"Rust Systems Programming","text":"<ul> <li>Tokio documentation</li> <li>async-trait patterns</li> <li>Arc concurrency"},{"location":"references/#staying-updated","title":"Staying Updated","text":""},{"location":"references/#follow-research","title":"Follow Research","text":"<ul> <li>arXiv (cs.CL, cs.AI sections)</li> <li>OpenAI blog</li> <li>Anthropic research papers</li> <li>Papers with Code</li> </ul>"},{"location":"references/#community","title":"Community","text":"<ul> <li>GitHub discussions</li> <li>Rust communities</li> <li>ML research forums</li> </ul>"},{"location":"references/#benchmarks","title":"Benchmarks","text":"<ul> <li>AIME, IMO official sites</li> <li>LiveCodeBench updates</li> <li>HumanEval variants</li> </ul>"},{"location":"references/#quick-links","title":"Quick Links","text":"Topic Link MOA Paper https://arxiv.org/abs/2502.04913 MCTS Paper https://dblp.org/rec/journals/cg/Coulom06.html OptimLLM https://github.com/coohom/optillm This Project https://github.com/GeorgePearse/code AIME 2025 https://www.maa.org/aime IMO 2025 https://www.imo-official.org"},{"location":"release_management/","title":"Release Management","text":"<p>Currently, we made Codex binaries available in three places:</p> <ul> <li>GitHub Releases https://github.com/openai/codex/releases/</li> <li><code>@openai/codex</code> on npm: https://www.npmjs.com/package/@openai/codex</li> <li><code>codex</code> on Homebrew: https://formulae.brew.sh/formula/codex</li> </ul>"},{"location":"release_management/#cutting-a-release","title":"Cutting a Release","text":"<p>Currently, choosing the version number for the next release is a manual process. In general, just go to https://github.com/openai/codex/releases/latest and see what the latest release is and increase the minor version by <code>1</code>, so if the current release is <code>0.20.0</code>, then the next release should be <code>0.21.0</code>.</p> <p>Assuming you are trying to publish <code>0.21.0</code>, first you would run:</p> <pre><code>VERSION=0.21.0\n./code-rs/scripts/create_github_release.sh \"$VERSION\"\n</code></pre> <p>This will kick off a GitHub Action to build the release, so go to https://github.com/openai/codex/actions/workflows/rust-release.yml to find the corresponding workflow. (Note: we should automate finding the workflow URL with <code>gh</code>.)</p> <p>When the workflow finishes, the GitHub Release is \"done,\" but you still have to consider npm and Homebrew.</p>"},{"location":"release_management/#publishing-to-npm","title":"Publishing to npm","text":"<p>The GitHub Action is responsible for publishing to npm.</p>"},{"location":"release_management/#publishing-to-homebrew","title":"Publishing to Homebrew","text":"<p>For Homebrew, we are properly set up with their automation system, so every few hours or so it will check our GitHub repo to see if there is a new release. When it finds one, it will put up a PR to create the equivalent Homebrew release, which entails building Codex CLI from source on various versions of macOS.</p> <p>Inevitably, you just have to refresh this page periodically to see if the release has been picked up by their automation system:</p> <p>https://github.com/Homebrew/homebrew-core/pulls?q=%3Apr+codex</p> <p>Once everything builds, a Homebrew admin has to approve the PR. Again, the whole process takes several hours and we don't have total control over it, but it seems to work pretty well.</p> <p>For reference, our Homebrew formula lives at:</p> <p>https://github.com/Homebrew/homebrew-core/blob/main/Formula/c/codex.rb</p>"},{"location":"sandbox/","title":"Sandbox Mode","text":""},{"location":"sandbox/#sandbox-approvals","title":"Sandbox &amp; approvals","text":""},{"location":"sandbox/#approval-modes","title":"Approval modes","text":"<p>We've chosen a powerful default for how Codex works on your computer: <code>Auto</code>. In this approval mode, Codex can read files, make edits, and run commands in the working directory automatically. However, Codex will need your approval to work outside the working directory or access network.</p> <p>When you just want to chat, or if you want to plan before diving in, you can switch to <code>Read Only</code> mode with the <code>/approvals</code> command.</p> <p>If you need Codex to read files, make edits, and run commands with network access, without approval, you can use <code>Full Access</code>. Exercise caution before doing so.</p>"},{"location":"sandbox/#defaults-and-recommendations","title":"Defaults and recommendations","text":"<ul> <li>Codex runs in a sandbox by default with strong guardrails: it prevents editing files outside the workspace and blocks network access unless enabled.</li> <li>On launch, Codex detects whether the folder is version-controlled and recommends:</li> <li>Version-controlled folders: <code>Auto</code> (workspace write + on-request approvals)</li> <li>Non-version-controlled folders: <code>Read Only</code></li> <li>The workspace includes the current directory and temporary directories like <code>/tmp</code>. Use the <code>/status</code> command to see which directories are in the workspace.</li> <li>You can set these explicitly:</li> <li><code>codex --sandbox workspace-write --ask-for-approval on-request</code></li> <li><code>codex --sandbox read-only --ask-for-approval on-request</code></li> </ul>"},{"location":"sandbox/#can-i-run-without-any-approvals","title":"Can I run without ANY approvals?","text":"<p>Yes, you can disable all approval prompts with <code>--ask-for-approval never</code>. This option works with all <code>--sandbox</code> modes, so you still have full control over Codex's level of autonomy. It will make its best attempt with whatever contrainsts you provide.</p>"},{"location":"sandbox/#common-sandbox-approvals-combinations","title":"Common sandbox + approvals combinations","text":"Intent Flags Effect Safe read-only browsing <code>--sandbox read-only --ask-for-approval on-request</code> Codex can read files and answer questions. Codex requires approval to make edits, run commands, or access network. Read-only non-interactive (CI) <code>--sandbox read-only --ask-for-approval never</code> Reads only; never escalates Let it edit the repo, ask if risky <code>--sandbox workspace-write --ask-for-approval on-request</code> Codex can read files, make edits, and run commands in the workspace. Codex requires approval for actions outside the workspace or for network access. Auto (preset) <code>--full-auto</code> (equivalent to <code>--sandbox workspace-write</code> + <code>--ask-for-approval on-failure</code>) Codex can read files, make edits, and run commands in the workspace. Codex requires approval when a sandboxed command fails or needs escalation. YOLO (not recommended) <code>--dangerously-bypass-approvals-and-sandbox</code> (alias: <code>--yolo</code>) No sandbox; no prompts <p>Note: In <code>workspace-write</code>, network is disabled by default unless enabled in config (<code>[sandbox_workspace_write].network_access = true</code>).</p>"},{"location":"sandbox/#fine-tuning-in-configtoml","title":"Fine-tuning in <code>config.toml</code>","text":"<pre><code># approval mode\napproval_policy = \"untrusted\"\nsandbox_mode    = \"read-only\"\n\n# full-auto mode\napproval_policy = \"on-request\"\nsandbox_mode    = \"workspace-write\"\n\n# Optional: allow network in workspace-write mode\n[sandbox_workspace_write]\nnetwork_access = true\n</code></pre> <p>You can also save presets as profiles:</p> <pre><code>[profiles.full_auto]\napproval_policy = \"on-request\"\nsandbox_mode    = \"workspace-write\"\n\n[profiles.readonly_quiet]\napproval_policy = \"never\"\nsandbox_mode    = \"read-only\"\n</code></pre>"},{"location":"sandbox/#experimenting-with-the-codex-sandbox","title":"Experimenting with the Codex Sandbox","text":"<p>To test to see what happens when a command is run under the sandbox provided by Codex, we provide the following subcommands in Codex CLI:</p> <pre><code># macOS\ncodex sandbox macos [--full-auto] [COMMAND]...\n\n# Linux\ncodex sandbox linux [--full-auto] [COMMAND]...\n\n# Legacy aliases\ncodex debug seatbelt [--full-auto] [COMMAND]...\ncodex debug landlock [--full-auto] [COMMAND]...\n</code></pre>"},{"location":"sandbox/#platform-sandboxing-details","title":"Platform sandboxing details","text":"<p>The mechanism Codex uses to implement the sandbox policy depends on your OS:</p> <ul> <li>macOS 12+ uses Apple Seatbelt and runs commands using <code>sandbox-exec</code> with a profile (<code>-p</code>) that corresponds to the <code>--sandbox</code> that was specified.</li> <li>Linux uses a combination of Landlock/seccomp APIs to enforce the <code>sandbox</code> configuration.</li> </ul> <p>Note that when running Linux in a containerized environment such as Docker, sandboxing may not work if the host/container configuration does not support the necessary Landlock/seccomp APIs. In such cases, we recommend configuring your Docker container so that it provides the sandbox guarantees you are looking for and then running <code>codex</code> with <code>--sandbox danger-full-access</code> (or, more simply, the <code>--dangerously-bypass-approvals-and-sandbox</code> flag) within your container.</p>"},{"location":"slash-commands/","title":"Slash Commands","text":"<p>Codex CLI supports a set of slash commands you can type at the start of the composer input. These commands provide quick actions, toggles, or expand into full prompts. This document lists all built\u2011in commands and what they do.</p> <p>Notes</p> <ul> <li>Commands are shown in the TUI\u2019s slash popup; the order below matches the UI.</li> <li>Commands marked \u201cprompt\u2011expanding\u201d transform your input into a full prompt and   typically kick off multi\u2011agent flows.</li> <li>Some commands accept arguments; if required, usage is shown in parentheses.</li> </ul>"},{"location":"slash-commands/#navigation-session","title":"Navigation &amp; Session","text":"<ul> <li><code>/browser</code>: open internal browser.</li> <li><code>/chrome</code>: connect to your Chrome browser.</li> <li><code>/new</code>: start a new chat during a conversation.</li> <li><code>/resume</code>: resume a past session for this folder.</li> <li><code>/quit</code>: exit Codex.</li> <li><code>/logout</code>: log out of Codex.</li> <li><code>/login</code>: manage Code sign-ins (select, add, or disconnect accounts).</li> <li><code>/settings [section]</code>: open the settings panel. Optional section argument   jumps directly to <code>model</code>, <code>theme</code>, <code>agents</code>, <code>limits</code>, <code>chrome</code>, <code>mcp</code>, or   <code>notifications</code>.</li> </ul>"},{"location":"slash-commands/#workspace-git","title":"Workspace &amp; Git","text":"<ul> <li><code>/init</code>: create an <code>AGENTS.md</code> file with instructions for Codex.</li> <li><code>/diff</code>: show <code>git diff</code> (including untracked files).</li> <li><code>/undo</code>: open a snapshot picker so you can restore workspace files to a   previous Code snapshot and optionally rewind the conversation to that point.</li> <li><code>/branch [task]</code>: create a worktree branch and switch to it. If a   task/description is provided, it is used when naming the branch. Must be run   from the repository root (not inside another branch worktree). Set   <code>CODE_BRANCH_COPY_CACHES=1</code> (legacy: <code>CODEX_BRANCH_COPY_CACHES=1</code>) to mirror   <code>node_modules</code> and Rust build caches into the worktree; otherwise no cache   directories are copied automatically.</li> <li><code>/merge</code>: merge the current worktree branch back into the default branch and   remove the worktree. Run this from inside the worktree created by <code>/branch</code>.</li> <li><code>/review [focus]</code>: without arguments, opens a review picker so you can audit   the workspace, a specific commit, compare against another branch, or enter   custom instructions. With a focus argument, skips the picker and uses your   text directly. Use the Review options dialog to toggle Auto Resolve if you   want Codex to rerun fixes and review checks automatically.</li> <li><code>/cloud</code>: browse Codex Cloud tasks, view details, apply patches, and create   new tasks from the TUI.</li> <li><code>/cmd &lt;name&gt;</code>: run a project command defined for the current workspace.</li> </ul>"},{"location":"slash-commands/#ux-display","title":"UX &amp; Display","text":"<ul> <li><code>/theme</code>: customize the app theme.</li> <li><code>/verbosity (high|medium|low)</code>: change text verbosity.</li> <li><code>/model</code>: choose your default model.</li> <li><code>/reasoning (minimal|low|medium|high)</code>: change reasoning effort.</li> <li><code>/prompts</code>: show example prompts.</li> <li><code>/status</code>: show current session configuration and token usage.</li> <li><code>/limits</code>: adjust session limits and visualize hourly and weekly rate-limit   usage.</li> <li><code>/update</code>: check the installed version, detect available upgrades, and open a   guided upgrade terminal that runs the installer interactively when possible.</li> <li><code>/notifications [status|on|off]</code>: manage notification settings. Without   arguments, shows the notifications panel. With arguments: <code>status</code> shows   current config, <code>on</code> enables all, <code>off</code> disables all.</li> <li><code>/mcp [status|on|off &lt;name&gt;|add]</code>: manage MCP servers. Without arguments,   shows all servers with toggle controls. With arguments: <code>status</code> lists   servers, <code>on &lt;name&gt;</code> enables, <code>off &lt;name&gt;</code> disables, and <code>add</code> starts the new   server workflow.</li> <li><code>/validation [status|on|off|&lt;tool&gt; (on|off)]</code>: inspect or toggle validation   harness settings.</li> </ul>"},{"location":"slash-commands/#search-mentions","title":"Search &amp; Mentions","text":"<ul> <li><code>/mention</code>: mention a file (opens the file search for quick insertion).</li> </ul>"},{"location":"slash-commands/#performance-agents","title":"Performance &amp; Agents","text":"<ul> <li><code>/perf (on|off|show|reset)</code>: performance tracing controls.</li> <li><code>/agents</code>: configure agents and subagent commands (including autonomous   follow-ups and observer status; available in dev, dev-fast, and perf builds).</li> <li><code>/auto [goal]</code>: start the maintainer-style auto coordinator. If no goal is   provided it defaults to \"review the git log for recent changes and come up   with sensible follow up work\".</li> </ul>"},{"location":"slash-commands/#promptexpanding-multiagent","title":"Prompt\u2011Expanding (Multi\u2011Agent)","text":"<p>These commands expand into full prompts (generated by <code>codex-core</code>) and typically start multiple agents. They require a task/problem description.</p> <ul> <li><code>/plan &lt;task&gt;</code>: create a comprehensive plan (multiple agents). Prompt\u2011expanding.</li> <li><code>/solve &lt;problem&gt;</code>: solve a challenging problem (multiple agents). Prompt\u2011expanding.</li> <li><code>/code &lt;task&gt;</code>: perform a coding task (multiple agents). Prompt\u2011expanding.</li> </ul>"},{"location":"slash-commands/#developmentonly","title":"Development\u2011Only","text":"<ul> <li><code>/demo</code>: populate the chat history with assorted sample cells (available in   dev and perf builds for UI testing).</li> <li><code>/test-approval</code>: test approval request (available in debug builds only).</li> </ul> <p>Implementation Notes</p> <ul> <li>The authoritative list of commands is defined in   <code>code-rs/tui/src/slash_command.rs</code> (the <code>SlashCommand</code> enum). When adding a   new command, please update this document to keep the UI and docs in sync.</li> <li>Prompt formatting for <code>/plan</code>, <code>/solve</code>, and <code>/code</code> lives in   <code>code-rs/core/src/slash_commands.rs</code>.   When no <code>[[agents]]</code> are configured, the orchestrator advertises the   following model slugs to the LLM for multi-agent runs: <code>code-gpt-5</code>,   <code>claude-sonnet-4.5</code>, <code>claude-opus-4.1</code>, <code>gemini-2.5-pro</code>,   <code>gemini-2.5-flash</code>, <code>qwen-3-coder</code>, and <code>code-gpt-5-codex</code> (with <code>cloud-gpt-5-codex</code> gated by   <code>CODE_ENABLE_CLOUD_AGENT_MODEL</code>). You can replace or pin this set via   <code>[[agents]]</code> or per-command <code>[[subagents.commands]].agents</code>.</li> </ul>"},{"location":"tui-card-themes/","title":"TUI Card Theme System","text":"<p>The TUI now exposes a shared card theme catalog that any surface (Agent, Browser, Auto Drive, Search, etc.) can use to present consistent gradients, palettes, and reveal animations.</p>"},{"location":"tui-card-themes/#1-import-the-theme-helpers","title":"1. Import the theme helpers","text":"<pre><code>use crate::card_theme::{self, CardThemeDefinition};\nuse crate::gradient_background::{GradientBackground, RevealRender};\n</code></pre> <p><code>card_theme</code> owns the data model (gradients, palettes, animation metadata). <code>gradient_background</code> handles all gradient and reveal rendering.</p>"},{"location":"tui-card-themes/#2-pick-a-theme","title":"2. Pick a theme","text":"<p>Built-in helpers return <code>CardThemeDefinition</code> values that carry the display name and styling bundle:</p> <pre><code>let search_dark = card_theme::search_dark_theme();\nlet auto_drive_light = card_theme::auto_drive_light_theme();\nlet all = card_theme::theme_catalog();\n</code></pre> <p>Use <code>dark_theme_catalog()</code>, <code>light_theme_catalog()</code>, or <code>auto_drive_theme_catalog()</code> when you need grouped collections (for example, to build preview pickers).</p> <p>To attach body content, convert the definition into a <code>CardPreviewSpec</code>:</p> <pre><code>const BODY: &amp;[&amp;str] = &amp;[\"LLM output in a card\", \"Second paragraph...\"];\nlet preview = theme.preview(BODY);\n</code></pre>"},{"location":"tui-card-themes/#3-render-a-gradient-card","title":"3. Render a gradient card","text":"<p><code>GradientBackground::render</code> applies either a static gradient or an animated reveal. Pass the palette\u2019s text color as the foreground; the helper keeps text legible while animating.</p> <pre><code>let reveal = preview\n    .theme\n    .reveal\n    .map(|config| RevealRender {\n        progress: animation.progress(),\n        variant: config.variant,\n        intro_light: preview.name.contains(\"Light\"),\n    });\n\nGradientBackground::render(\n    buf,\n    area,\n    &amp;preview.theme.gradient,\n    preview.theme.palette.text,\n    reveal,\n);\n</code></pre> <p>After painting the background, write title/body/footer text with the palette colors (<code>palette.border</code>, <code>palette.title</code>, etc.).</p>"},{"location":"tui-card-themes/#4-drive-animations-when-available","title":"4. Drive animations when available","text":"<p>Themes that define a <code>RevealConfig</code> opt into animated reveals. Track an animation progress value (<code>0.0..=1.0</code>) of your choosing and pass it through <code>RevealRender</code>. Themes with <code>reveal: None</code> render statically; supply <code>None</code> to <code>GradientBackground::render</code> in that case.</p>"},{"location":"tui-card-themes/#5-examples-by-surface","title":"5. Examples by surface","text":"<ul> <li>Search \u2013 <code>search_dark_theme()</code> / <code>search_light_theme()</code></li> <li>Auto Drive \u2013 <code>auto_drive_dark_theme()</code> / <code>auto_drive_light_theme()</code> (the only themes with <code>reveal</code> animation data)</li> <li>Agent (read-only) \u2013 <code>agent_read_only_dark_theme()</code> / <code>agent_read_only_light_theme()</code></li> <li>Agent (write) \u2013 <code>agent_write_dark_theme()</code> / <code>agent_write_light_theme()</code></li> <li>Browser \u2013 <code>browser_dark_theme()</code> / <code>browser_light_theme()</code></li> </ul>"},{"location":"tui-card-themes/#tips","title":"Tips","text":"<ul> <li>Keep body copy narrow (<code>textwrap</code> width \u2248 <code>area.width - padding</code>).</li> <li>When you introduce a new theme, add it to <code>card_theme.rs</code> so every surface can reuse it.</li> <li>Prefer <code>CardThemeDefinition::preview</code> over ad-hoc palette wiring; this keeps names, palettes, and animations aligned.</li> </ul>"},{"location":"tui-chatwidget-refactor/","title":"TUI ChatWidget Refactor Plan","text":"<p>This document tracks the refactor of <code>code-rs/tui/src/chatwidget.rs</code> into smaller, maintainable modules and state bundles. It also captures what has been completed, and what remains so another engineer can continue seamlessly.</p>"},{"location":"tui-chatwidget-refactor/#goals","title":"Goals","text":"<ul> <li>Break the 389kB <code>chatwidget.rs</code> into cohesive modules by responsibility.</li> <li>Group related state into small structs to reduce field sprawl and accidental misuse.</li> <li>Centralize common history operations (push/replace/remove) behind a tiny API.</li> <li>Keep behavior identical and builds green at every step (<code>./build-fast.sh</code>).</li> <li>Maintain zero warnings policy.</li> </ul>"},{"location":"tui-chatwidget-refactor/#status-summary-as-of-this-commit","title":"Status Summary (as of this commit)","text":"<p>Completed extractions (modules): - <code>chatwidget/perf.rs</code>: <code>PerfStats</code> (data + helpers) - <code>chatwidget/diff_ui.rs</code>: <code>DiffOverlay</code>, <code>DiffBlock</code>, <code>DiffConfirm</code> - <code>chatwidget/message.rs</code>: <code>UserMessage</code>, <code>create_initial_user_message</code> - <code>chatwidget/streaming.rs</code>: streaming deltas and finalize helpers (<code>on_commit_tick</code>, <code>is_write_cycle_active</code>, <code>handle_streaming_delta</code>, <code>finalize_active_stream</code>) - <code>chatwidget/exec_tools.rs</code>: exec lifecycle, tool merge helpers; move-before-assistant helpers - <code>chatwidget/tools.rs</code>: Web Search and MCP tool lifecycle - <code>chatwidget/layout_scroll.rs</code>: HUD toggles, layout_areas, PageUp/PageDown, mouse wheel scroll - <code>chatwidget/diff_handlers.rs</code>: all Diff overlay key handling</p> <p>State groupings: - <code>StreamState</code> on <code>ChatWidget</code>   - <code>current_kind</code>, <code>closed_answer_ids</code>, <code>closed_reasoning_ids</code>, <code>next_seq</code>, <code>seq_answer_final</code>, <code>drop_streaming</code> - <code>LayoutState</code> on <code>ChatWidget</code>   - <code>scroll_offset</code>, <code>last_max_scroll</code>, <code>last_history_viewport_height</code>, <code>vertical_scrollbar_state</code>, <code>scrollbar_visible_until</code>, <code>last_hud_present</code>, <code>browser_hud_expanded</code>, <code>agents_hud_expanded</code>, <code>last_frame_height</code> - <code>DiffsState</code> on <code>ChatWidget</code>   - <code>session_patch_sets</code>, <code>baseline_file_contents</code>, <code>overlay</code>, <code>confirm</code>, <code>body_visible_rows</code> - <code>PerfState</code> on <code>ChatWidget</code>   - <code>enabled</code>, <code>stats: RefCell&lt;PerfStats&gt;</code> replacing <code>perf_enabled</code> and <code>perf</code></p> <p>History helper shim: - Added to <code>ChatWidget</code>:   - <code>history_push(cell)</code>: wrapper for <code>add_to_history</code></p> <p>Additional references: - Renderer cache bridge rationale: <code>history_render_cache_bridge.md</code> - History record schema + snapshot details: <code>history_state_schema.md</code> - Migrated usages in <code>exec_tools.rs</code> and <code>tools.rs</code> (some hotspots) to use the shim.</p> <p>Build + policy: - All steps validated with <code>./build-fast.sh</code>. - Zero compiler warnings kept intact.</p>"},{"location":"tui-chatwidget-refactor/#new-filemodule-map","title":"New File/Module Map","text":"<ul> <li><code>tui/src/chatwidget.rs</code> (shrinking coordinator)</li> <li>owns <code>ChatWidget</code> and wires events \u2192 submodules</li> <li>houses grouped states: <code>StreamState</code>, <code>LayoutState</code>, <code>DiffsState</code>, <code>PerfState</code></li> <li>history helper shim (temporary; see Future consolidation)</li> <li><code>tui/src/chatwidget/perf.rs</code> \u2192 performance structs</li> <li><code>tui/src/chatwidget/diff_ui.rs</code> \u2192 diff overlay data types</li> <li><code>tui/src/chatwidget/message.rs</code> \u2192 user message building</li> <li><code>tui/src/chatwidget/streaming.rs</code> \u2192 unified streaming operations</li> <li><code>tui/src/chatwidget/exec_tools.rs</code> \u2192 exec lifecycle + merges</li> <li><code>tui/src/chatwidget/tools.rs</code> \u2192 Web Search + MCP lifecycle</li> <li><code>tui/src/chatwidget/layout_scroll.rs</code> \u2192 layout + scrolling + HUD controls</li> <li><code>tui/src/chatwidget/diff_handlers.rs</code> \u2192 diff overlay key handling</li> </ul>"},{"location":"tui-chatwidget-refactor/#behavior-preserving-changes-made","title":"Behavior-Preserving Changes Made","text":"<ul> <li>No intentional UX/logic changes. Extracted code calls the same operations as before.</li> <li>Replaced direct field access with grouped state access (e.g., <code>self.stream_state.current_kind</code>).</li> <li>Centralized common history mutations and used them in a few hotspots to reduce duplication.</li> </ul>"},{"location":"tui-chatwidget-refactor/#next-steps-recommended-order","title":"Next Steps (Recommended Order)","text":"<p>1) Finish History helper adoption \u2014 DONE - Replaced remaining ad-hoc <code>history_cells[idx] = \u2026</code>, <code>remove(i)</code>, and <code>add_to_history</code> sites with <code>history_replace_at</code>, <code>history_remove_at</code>, <code>history_push</code> (kept direct remove+insert only for move-before-assistant helpers where the removed cell is reused). - Swept: <code>chatwidget.rs</code>, <code>chatwidget/tools.rs</code>, <code>chatwidget/exec_tools.rs</code>, <code>chatwidget/interrupts.rs</code>.</p> <p>2) History merge API consolidation \u2014 DONE - Added <code>history_push_and_maybe_merge</code>, <code>history_replace_and_maybe_merge</code>, and <code>history_maybe_merge_tool_with_previous</code> on <code>ChatWidget</code>. - Adopted across exec completion and web search completion paths; explicit merge code removed from call sites.</p> <p>3) Enum-ize exec actions and newtype IDs \u2014 DONE - Added <code>ExecAction</code> enum + <code>action_enum_from_parsed()</code> and adopted across <code>history_cell</code>, <code>exec_tools</code>, and <code>chatwidget</code>. - Introduced ID newtypes: <code>ExecCallId</code>, <code>ToolCallId</code>, <code>StreamId</code>; migrated state maps/sets and call sites. - Removed legacy <code>action_from_parsed</code> helper.</p> <p>4) Streaming API cleanup \u2014 DONE - In <code>streaming.rs</code>, added <code>begin(kind,id)</code>, <code>delta_text(kind,id,text)</code>, <code>finalize(kind, follow_bottom)</code> facades. - Replaced direct <code>current_kind</code> mutations at call sites with the facade; kept internal control inside streaming module.</p> <p>5) Tests (valuable quick wins) - Unit tests   - Exec merges: combinations for Read/Search/List/Run and exit statuses.   - Parse/paste: <code>[image: \u2026]</code>, <code>[file: \u2026]</code>, direct path handling, size thresholds, non\u2011UTF8 case.   - Diff selection math: selecting block by offset and generating undo/explain prompts. - Replay tests (vt100)   - Cancel mid-stream (drop deltas immediately, then finalize when backend completes).   - \u201cFinal answer closes lingering tools/execs\u201d (already coded; test it). </p> <p>6) Optional: rename for clarity - Consider <code>ChatWidget</code> \u2192 <code>ChatView</code> (render/controller) once responsibilities are clearer.</p>"},{"location":"tui-chatwidget-refactor/#tips-for-the-next-engineer","title":"Tips for the Next Engineer","text":"<ul> <li>Always validate with <code>./build-fast.sh</code>; warnings are treated as failures.</li> <li>Avoid touching any code related to <code>CODEX_SANDBOX_*</code> env vars.</li> <li>Keep steps small and behavior\u2011preserving; prefer many tiny commits over one big one.</li> <li>When moving code, first copy the function into a submodule, compile, then replace the original with a delegate call. Only then remove the original body.</li> <li>For scrolling math: all offsets are measured from the bottom; <code>LayoutState</code> centralizes the values you need.</li> <li>For diffs popup: <code>DiffsState</code> now owns overlay/confirm and the session patch context.</li> <li>For perf: read/write via <code>self.perf_state.enabled</code> and <code>self.perf_state.stats.borrow_mut()</code>.</li> </ul>"},{"location":"tui-chatwidget-refactor/#quick-pointers","title":"Quick Pointers","text":"<ul> <li>Stream state: <code>StreamState</code> on <code>ChatWidget</code> (search for <code>stream_state</code>)</li> <li>Layout state: <code>LayoutState</code> on <code>ChatWidget</code> (search for <code>layout.</code>)</li> <li>Diff state: <code>DiffsState</code> on <code>ChatWidget</code> (search for <code>diffs.</code>)</li> <li>History helpers: <code>history_push</code>, <code>history_replace_at</code>, <code>history_remove_at</code> in <code>chatwidget.rs</code></li> <li>Web search lifecycle: <code>chatwidget/tools.rs</code></li> <li>Exec lifecycle + merges: <code>chatwidget/exec_tools.rs</code></li> <li>Streaming functions: <code>chatwidget/streaming.rs</code></li> <li>Diff overlay keys: <code>chatwidget/diff_handlers.rs</code></li> <li>HUD/layout: <code>chatwidget/layout_scroll.rs</code></li> </ul>"},{"location":"tui-chatwidget-refactor/#done-checklist","title":"Done Checklist","text":"<ul> <li> Extract perf/diff/message data types</li> <li> Extract streaming and exec/tool handlers</li> <li> Extract layout/scroll + diff key handling</li> <li> Group StreamState, LayoutState, DiffsState, PerfState</li> <li> Add history helper shim and adopt in hotspots</li> <li> Keep builds green with zero warnings at each step</li> </ul>"},{"location":"tui-chatwidget-refactor/#todo-checklist","title":"Todo Checklist","text":"<ul> <li> Sweep remaining history mutations to use the shim</li> <li> Consolidate history merge API (adopt helpers across call sites)</li> <li> Replace all action string checks with <code>ExecAction</code></li> <li> Add ID newtypes (Exec/Tool/Stream) and migrate maps</li> <li> Tighten streaming API (begin/delta/finalize facade)</li> <li> Add state-driven renderer/unit tests for exec, assistant, streaming, diff</li> </ul>"},{"location":"tui-chatwidget-refactor/#how-to-validate-locally","title":"How to Validate Locally","text":"<pre><code>./build-fast.sh\n</code></pre> <p>Expect: success with zero warnings. The script builds the Rust workspace in a fast profile and maintains symlinks required by the CLI wrapper.</p> <p>If anything is unclear or you hit conflicts, start by grepping for the state bundle or module name and follow the delegate calls from <code>ChatWidget</code> into the submodule.</p>"},{"location":"upstream-acp-merge/","title":"Upstream ACP Integration \u2013 Design Notes","text":""},{"location":"upstream-acp-merge/#context","title":"Context","text":"<p>Upstream PR openai/codex#1707 introduces an experimental Agent Client Protocol (ACP) bridge so Zed can drive Codex via the <code>session/new</code> and <code>session/prompt</code> tool calls. The branch (fetched locally as <code>upstream-pr-1707</code>) rewrites large portions of <code>code-rs/core</code>, the MCP server, and the TypeScript CLI/TUI to accommodate the new workflow.</p>"},{"location":"upstream-acp-merge/#current-status-2025-09-22","title":"Current Status (2025-09-22)","text":"<ul> <li>Core \u2013 Apply-patch execution now runs in-process with the ACP filesystem   shim, preserving Code\u2019s validation harness and approval prompts while   emitting ACP tool-call metadata for downstream consumers.</li> <li>MCP Server \u2013 <code>session/new</code> and <code>session/prompt</code> are available alongside   existing Codex tools. The new <code>acp_tool_runner</code> bridges Codex events to ACP   <code>session/update</code> notifications and reuses the existing conversation manager to track live   sessions.</li> <li>Configuration \u2013 <code>ConfigOverrides</code> and the TOML schema understand   <code>experimental_client_tools</code> plus inline MCP server definitions, allowing IDE   clients to announce their ACP capabilities without dropping legacy settings.</li> <li>Validation \u2013 <code>./build-fast.sh</code> passes after the integration, and new MCP   tests cover ACP list-tool discovery plus a prompt roundtrip.</li> </ul> <p>Code diverges substantially from upstream in these same areas: we retain the Rust TUI, trimmed CLI scripts, and extended core features such as confirm guards, queued user input, plan tool updates, and sandbox policies tuned for our workflow. Directly rebasing onto upstream would drop or regress many of these capabilities.</p>"},{"location":"upstream-acp-merge/#what-upstream-added","title":"What Upstream Added","text":"<ul> <li>Core (<code>code-rs/core</code>)</li> <li>New <code>acp.rs</code> module with helpers for translating Codex exec/patch events     into ACP <code>ToolCall</code> updates, including an ACP-backed <code>FileSystem</code> shim and a     permission-request flow (<code>acp::request_permission</code>).</li> <li><code>codex.rs</code> rewritten around <code>agent_client_protocol</code>, removing many existing     operations (<code>QueueUserInput</code>, <code>RegisterApprovedCommand</code>, validation toggles,     etc.) and introducing new event wiring for ACP tool calls.</li> <li><code>protocol.rs</code> pared down to match the simplified upstream surface (fewer     <code>Op</code> variants, different <code>AskForApproval</code> options, new MCP call events).</li> <li>MCP server (<code>code-rs/mcp-server</code>)</li> <li>New <code>acp_tool_runner.rs</code> that spawns Codex sessions on demand, relays MCP     notifications, and surfaces ACP updates.</li> <li><code>message_processor.rs</code> extended to expose <code>session/new</code> and     <code>session/prompt</code>, and to translate Codex events into ACP notifications (<code>session/update</code>).</li> <li>CLI/TUI (Node + Rust)</li> <li>TypeScript terminal UI completely replaced with ACP-first experience.</li> <li>Rust TUI and associated tests removed.</li> </ul>"},{"location":"upstream-acp-merge/#code-specific-functionality-we-must-preserve","title":"Code-Specific Functionality We Must Preserve","text":"<ul> <li>Protocol &amp; Ops \u2013 <code>QueueUserInput</code>, validation toggles, confirm guards,   rich approval semantics (<code>ReviewDecision::ApprovedForSession</code>), and sandbox   policy options currently in <code>code-rs/core/src/protocol.rs:1</code>.</li> <li>Execution Safety &amp; Logging \u2013 Confirm-guard enforcement and the richer   <code>EventMsg</code> variants emitted from our <code>code-rs/core/src/codex.rs:1</code>.</li> <li>TUI \u2013 Entire Rust TUI stack (<code>code-rs/tui/src/chatwidget.rs:1</code>,   <code>code-rs/tui/src/history_cell.rs:1</code>, etc.) must remain functional and ignore   unknown ACP events gracefully.</li> <li>CLI Footprint \u2013 Our trimmed <code>codex-cli</code> structure and scripts differ from   upstream\u2019s wholesale replacement; we will not adopt the TypeScript overhaul.</li> <li>Config Schema \u2013 Existing TOML fields (confirm guards, validation toggles,   sandbox defaults) must stay intact.</li> </ul>"},{"location":"upstream-acp-merge/#integration-approach","title":"Integration Approach","text":"<ol> <li>Introduce ACP Helpers Without Regressions</li> <li>Port <code>code-rs/core/src/acp.rs:1</code> (and dependent structs) into Code, but      adapt it to reuse the current <code>FileChange</code> representations and respect      confirm guard / approval flows.</li> <li>Extend <code>code-rs/core/src/codex.rs:1</code> to emit ACP events while preserving      the existing <code>Op</code> variants, queueing logic, and validation toggles.</li> <li> <p>Update <code>code-rs/core/src/util.rs:1</code>, <code>code-rs/core/src/apply_patch.rs:1</code>,      and related modules so ACP tool-call generators can derive the same      metadata our TUI already consumes.</p> </li> <li> <p>MCP Server Wiring</p> </li> <li>Add <code>code-rs/mcp-server/src/acp_tool_runner.rs:1</code> and integrate it with      <code>message_processor.rs:1</code>, ensuring we keep Code-specific auth/sandbox setup      and error reporting.</li> <li> <p>Maintain existing MCP tools and ensure ACP tools are opt-in (guarded by      config or feature flag) so Code retains current behavior when ACP is      unused.</p> </li> <li> <p>Event &amp; Frontend Compatibility</p> </li> <li>Extend our event enums (<code>code-rs/core/src/protocol.rs:1</code>) with any new      variants required by ACP while keeping deprecated ones for backward      compatibility.</li> <li> <p>Teach the Rust TUI to ignore or minimally display ACP-specific events so      terminal UX does not panic when ACP notifications flow through.</p> </li> <li> <p>Config &amp; Build</p> </li> <li>Add <code>agent-client-protocol</code> dependency where needed, updating      <code>code-rs/core/Cargo.toml:1</code>, <code>code-rs/mcp-server/Cargo.toml:1</code>, and      <code>Cargo.lock</code>.</li> <li>Introduce configuration toggles (if any) in <code>code-rs/core/src/config.rs:1</code>      and <code>code-rs/core/src/config_types.rs:1</code> without breaking existing TOML      files.</li> <li> <p>Update documentation (targeted sections in <code>docs/experimental.md:1</code> or a      new page) to describe ACP/Zed support plus Code-specific caveats.</p> </li> <li> <p>Testing &amp; Validation</p> </li> <li>Add focused tests covering ACP request flow (unit-level in core and MCP      server). Reuse existing harnesses (e.g., <code>code-rs/mcp-server/tests</code>) to      simulate a session.</li> <li>Validate end-to-end via <code>./build-fast.sh</code> only, honoring our policy against      running additional formatters automatically.</li> </ol>"},{"location":"upstream-acp-merge/#open-questions","title":"Open Questions","text":"<ul> <li>How should ACP be surfaced in Code\u2019s configuration (auto-enabled vs per   server flag)?</li> <li>Do we expose ACP status in the Rust TUI, or treat it as headless-only? (Lean   toward headless-first with optional UI indicators.)</li> <li>Are there additional permission flows (e.g., review approvals) required for   Zed beyond what <code>ReviewDecision</code> already provides?</li> </ul> <p>Document last updated: 2025-09-22.</p>"},{"location":"upstream-mcp-implementation-guide/","title":"Upstream MCP Client Implementation Guide","text":"<p>Quick reference for implementing the upstream reuse strategy.</p>"},{"location":"upstream-mcp-implementation-guide/#quick-start-migrate-to-upstream-dependencies","title":"Quick Start: Migrate to Upstream Dependencies","text":""},{"location":"upstream-mcp-implementation-guide/#1-update-workspace-cargotoml","title":"1. Update Workspace Cargo.toml","text":"<pre><code># code-rs/Cargo.toml\n[workspace.dependencies]\n# Replace fork dependencies with upstream\ncodex-mcp-client = { path = \"../codex-rs/mcp-client\" }\ncodex-responses-api-proxy = { path = \"../codex-rs/responses-api-proxy\" }\ncodex-process-hardening = { path = \"../codex-rs/process-hardening\" }\n\n# Optional: Keep re-export aliases for gradual migration\ncode-mcp-client = { path = \"../codex-rs/mcp-client\", package = \"codex-mcp-client\" }\ncode-responses-api-proxy = { path = \"../codex-rs/responses-api-proxy\", package = \"codex-responses-api-proxy\" }\ncode-process-hardening = { path = \"../codex-rs/process-hardening\", package = \"codex-process-hardening\" }\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#2-update-core-dependencies","title":"2. Update Core Dependencies","text":"<pre><code># code-rs/core/Cargo.toml\n[dependencies]\n# Option A: Direct migration\ncodex-mcp-client = { workspace = true }\n\n# Option B: Gradual migration with alias\ncode-mcp-client = { workspace = true }\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#3-handle-binary-naming-if-required","title":"3. Handle Binary Naming (If Required)","text":""},{"location":"upstream-mcp-implementation-guide/#option-a-build-script-rename","title":"Option A: Build Script Rename","text":"<pre><code>// code-rs/cli/build.rs\nuse std::env;\nuse std::fs;\nuse std::path::PathBuf;\n\nfn main() {\n    // If embedding binary\n    let out_dir = PathBuf::from(env::var(\"OUT_DIR\").unwrap());\n\n    // Copy upstream binary with code- prefix\n    let upstream_bin = \"../target/release/codex-responses-api-proxy\";\n    let renamed_bin = out_dir.join(\"code-responses-api-proxy\");\n\n    if PathBuf::from(upstream_bin).exists() {\n        fs::copy(upstream_bin, renamed_bin)\n            .expect(\"Failed to copy responses-api-proxy binary\");\n    }\n}\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#option-b-accept-upstream-naming","title":"Option B: Accept Upstream Naming","text":"<pre><code>// code-rs/cli/src/whatever_uses_the_binary.rs\n// Simply use codex- prefix everywhere\nconst PROXY_BINARY: &amp;str = \"codex-responses-api-proxy\";\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#4-create-buffer-size-wrapper-if-needed","title":"4. Create Buffer Size Wrapper (If Needed)","text":"<p>Test first if default buffer is sufficient. If not:</p> <pre><code>// code-rs/core/src/mcp/client_wrapper.rs\nuse codex_mcp_client::McpClient;\nuse std::collections::HashMap;\nuse std::ffi::OsString;\n\n/// Creates MCP client optimized for large responses (code-rs specific)\npub async fn create_large_response_client(\n    program: OsString,\n    args: Vec&lt;OsString&gt;,\n    env: Option&lt;HashMap&lt;String, String&gt;&gt;,\n) -&gt; std::io::Result&lt;McpClient&gt; {\n    // For now, upstream implementation may already handle this well\n    // TODO: Benchmark with actual large responses before customizing\n    McpClient::new_stdio_client(program, args, env).await\n}\n\n// If buffer customization proves necessary, contribute upstream PR\n// See: upstream-mcp-reuse-strategy.md Strategy B\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#5-update-imports","title":"5. Update Imports","text":"<pre><code>// Before (code-rs fork)\nuse code_mcp_client::McpClient;\nuse code_process_hardening;\n\n// After (upstream direct)\nuse codex_mcp_client::McpClient;\nuse codex_process_hardening;\n\n// Or with re-export alias (no code changes needed)\nuse code_mcp_client::McpClient;  // Still works via Cargo.toml alias\nuse code_process_hardening;       // Still works via Cargo.toml alias\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#testing-the-migration","title":"Testing the Migration","text":""},{"location":"upstream-mcp-implementation-guide/#1-unit-tests","title":"1. Unit Tests","text":"<pre><code>cd code-rs\ncargo test -p core  # Test MCP client integration\ncargo test -p cli   # Test proxy binary handling\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#2-integration-tests-with-large-payloads","title":"2. Integration Tests with Large Payloads","text":"<pre><code>// code-rs/core/tests/mcp_large_response_test.rs\n#[tokio::test]\nasync fn test_large_tool_response() {\n    // Generate 2MB JSON response\n    let large_payload = generate_large_json_payload(2 * 1024 * 1024);\n\n    // Test MCP client handles it without truncation\n    let client = create_test_mcp_client().await.unwrap();\n    let result = client.call_tool(\n        \"large_data_tool\".to_string(),\n        Some(large_payload),\n        Some(Duration::from_secs(30))\n    ).await;\n\n    assert!(result.is_ok());\n    // Validate full payload received\n}\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#3-security-validation","title":"3. Security Validation","text":"<pre><code># Verify process hardening still active\ncargo build --release\n./target/release/code-responses-api-proxy --help\n\n# On Linux: Verify non-dumpable\ncat /proc/$(pgrep code-responses)/status | grep Dumpable\n# Should show: Dumpable: 0\n\n# On macOS: Verify ptrace protection\n# Attempt: lldb -p $(pgrep code-responses)\n# Should fail: \"Operation not permitted\"\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#upstream-contribution-workflow","title":"Upstream Contribution Workflow","text":""},{"location":"upstream-mcp-implementation-guide/#proposing-buffer-configuration-feature","title":"Proposing Buffer Configuration Feature","text":"<pre><code>// Proposed upstream change for codex-rs/mcp-client\n// File: codex-rs/mcp-client/src/mcp_client.rs\n\n/// Configuration options for MCP client\n#[derive(Debug, Clone)]\npub struct McpClientConfig {\n    /// Buffer capacity for reading server responses.\n    /// Default: 8KB (Tokio default)\n    /// Use larger values (e.g., 1MB) for servers with large tool responses\n    pub buffer_capacity: Option&lt;usize&gt;,\n}\n\nimpl Default for McpClientConfig {\n    fn default() -&gt; Self {\n        Self {\n            buffer_capacity: None,\n        }\n    }\n}\n\nimpl McpClient {\n    // Existing method unchanged for compatibility\n    pub async fn new_stdio_client(\n        program: OsString,\n        args: Vec&lt;OsString&gt;,\n        env: Option&lt;HashMap&lt;String, String&gt;&gt;,\n    ) -&gt; std::io::Result&lt;Self&gt; {\n        Self::new_stdio_client_with_config(\n            program,\n            args,\n            env,\n            McpClientConfig::default()\n        ).await\n    }\n\n    // New method with configuration\n    pub async fn new_stdio_client_with_config(\n        program: OsString,\n        args: Vec&lt;OsString&gt;,\n        env: Option&lt;HashMap&lt;String, String&gt;&gt;,\n        config: McpClientConfig,\n    ) -&gt; std::io::Result&lt;Self&gt; {\n        // ... existing setup code ...\n\n        let reader_handle = {\n            let pending = pending.clone();\n            let mut lines = match config.buffer_capacity {\n                Some(capacity) =&gt; BufReader::with_capacity(capacity, stdout).lines(),\n                None =&gt; BufReader::new(stdout).lines(),\n            };\n\n            // ... rest of implementation ...\n        };\n\n        // ... rest of implementation ...\n    }\n}\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#pr-description-template","title":"PR Description Template","text":"<pre><code>## Add buffer configuration to MCP client\n\n### Motivation\nWhen working with MCP servers that return large tool responses (&gt;100KB),\nthe default buffer size can impact performance. This PR adds optional\nbuffer configuration while maintaining backward compatibility.\n\n### Changes\n- Add `McpClientConfig` struct with optional buffer_capacity\n- Add `new_stdio_client_with_config` method\n- Keep existing `new_stdio_client` method unchanged (uses default config)\n\n### Testing\n- Existing tests pass (backward compatibility verified)\n- New test: large response handling with 1MB buffer\n- Benchmark: 2MB response ~30% faster with larger buffer\n\n### Breaking Changes\nNone - existing API unchanged, new method is additive\n\n### Use Case (code-rs)\nWe use this in code-rs for MCP servers that return large file contents\nor extensive analysis results. Setting buffer to 1MB eliminates multiple\nread syscalls for these responses.\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#rollback-plan","title":"Rollback Plan","text":"<p>If migration causes issues:</p> <pre><code># Revert to fork in code-rs/Cargo.toml\n[workspace.dependencies]\ncode-mcp-client = { path = \"mcp-client\" }\ncode-responses-api-proxy = { path = \"responses-api-proxy\" }\ncode-process-hardening = { path = \"process-hardening\" }\n</code></pre> <pre><code># Restore fork code\ngit checkout main -- code-rs/mcp-client\ngit checkout main -- code-rs/responses-api-proxy\ngit checkout main -- code-rs/process-hardening\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Before migration, establish baseline:</p> <pre><code>// code-rs/benches/mcp_client_bench.rs\nuse criterion::{black_box, criterion_group, criterion_main, Criterion};\n\nfn benchmark_large_response(c: &amp;mut Criterion) {\n    c.bench_function(\"mcp_client_1mb_response\", |b| {\n        b.iter(|| {\n            // Test current fork implementation\n            let client = create_fork_client();\n            let result = client.call_tool_with_large_response();\n            black_box(result)\n        })\n    });\n}\n\ncriterion_group!(benches, benchmark_large_response);\ncriterion_main!(benches);\n</code></pre> <p>After migration, compare:</p> <pre><code>cargo bench --bench mcp_client_bench &gt; before.txt\n# Apply migration\ncargo bench --bench mcp_client_bench &gt; after.txt\n# Compare results\ndiff before.txt after.txt\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#monitoring-post-migration","title":"Monitoring Post-Migration","text":""},{"location":"upstream-mcp-implementation-guide/#cicd-checks","title":"CI/CD Checks","text":"<pre><code># .github/workflows/mcp-upstream-health.yml\nname: MCP Upstream Health Check\non:\n  schedule:\n    - cron: '0 8 * * 1'  # Weekly Monday 8am\n  workflow_dispatch:\n\njobs:\n  check-upstream:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n        with:\n          submodules: true\n\n      - name: Check for upstream changes\n        run: |\n          cd codex-rs\n          git fetch origin\n          git diff origin/main -- mcp-client/ responses-api-proxy/ process-hardening/\n\n      - name: Run MCP integration tests\n        run: |\n          cargo test -p core -- mcp::\n          cargo test -p cli -- proxy::\n\n      - name: Security audit\n        run: cargo audit\n</code></pre>"},{"location":"upstream-mcp-implementation-guide/#alerting","title":"Alerting","text":"<p>Set up GitHub notifications: 1. Watch codex-rs repository 2. Custom notification rules:    - All activity in <code>mcp-client/</code>    - All activity in <code>responses-api-proxy/</code>    - All activity in <code>process-hardening/</code>    - Security advisories</p>"},{"location":"upstream-mcp-implementation-guide/#summary-checklist","title":"Summary Checklist","text":"<ul> <li> Update workspace dependencies to point to codex-rs</li> <li> Test with default buffer size first</li> <li> Implement wrapper only if needed (benchmark first)</li> <li> Update imports or use Cargo.toml aliases</li> <li> Run full test suite</li> <li> Validate process hardening on all platforms</li> <li> Benchmark performance before/after</li> <li> Set up upstream monitoring</li> <li> Prepare upstream PR for buffer config (if needed)</li> <li> Document migration in CHANGELOG</li> <li> Update internal documentation references</li> </ul>"},{"location":"upstream-mcp-implementation-guide/#next-steps","title":"Next Steps","text":"<ol> <li>Immediate: Test upstream <code>codex-mcp-client</code> with code-rs workloads</li> <li>Week 1: Migrate mcp-client dependency if tests pass</li> <li>Week 2: Migrate responses-api-proxy with binary rename</li> <li>Week 3: Propose upstream buffer config PR</li> <li>Ongoing: Monitor upstream changes weekly</li> </ol> <p>See upstream-mcp-reuse-strategy.md for detailed analysis and rationale.</p>"},{"location":"upstream-mcp-reuse-strategy/","title":"Upstream MCP Client Reuse Strategy","text":""},{"location":"upstream-mcp-reuse-strategy/#executive-summary","title":"Executive Summary","text":"<p>This document analyzes the feasibility of reusing <code>codex-rs/mcp-client</code> and <code>codex-rs/responses-api-proxy</code> directly from code-rs without maintaining separate forks. The investigation reveals minimal divergence between implementations, making upstream reuse highly feasible with a thin wrapper/patch strategy.</p>"},{"location":"upstream-mcp-reuse-strategy/#analysis-of-current-divergence","title":"Analysis of Current Divergence","text":""},{"location":"upstream-mcp-reuse-strategy/#1-mcp-client-mcp-client","title":"1. MCP Client (<code>mcp-client</code>)","text":"<p>Differences Found: - Buffer Size: code-rs uses 1MB buffer (<code>BufReader::with_capacity(1024 * 1024, stdout)</code>) vs codex-rs default buffer   - Location: <code>mcp-client/src/mcp_client.rs:145</code>   - Reason: Handle large tool responses without truncation   - Impact: Performance optimization for specific use cases</p> <p>Similarities: - Identical package structure and dependencies - Same API surface and MCP protocol implementation - Same JSON-RPC message handling - Same environment variable filtering logic - File sizes: 476 vs 475 lines (virtually identical)</p>"},{"location":"upstream-mcp-reuse-strategy/#2-responses-api-proxy-responses-api-proxy","title":"2. Responses API Proxy (<code>responses-api-proxy</code>)","text":"<p>Differences Found: - None - The lib.rs files are byte-for-byte identical</p> <p>Naming Differences: - Package name: <code>code-responses-api-proxy</code> vs <code>codex-responses-api-proxy</code> - Binary name: <code>code-responses-api-proxy</code> vs <code>codex-responses-api-proxy</code> - Library name: <code>code_responses_api_proxy</code> vs <code>codex_responses_api_proxy</code> - Process hardening dependency: <code>code-process-hardening</code> vs <code>codex-process-hardening</code></p>"},{"location":"upstream-mcp-reuse-strategy/#3-process-hardening-process-hardening","title":"3. Process Hardening (<code>process-hardening</code>)","text":"<p>Differences Found: - Minor style difference in filter_map closure (code-rs uses compact <code>.then_some()</code>, codex-rs uses verbose if/else)   - Location: <code>process-hardening/src/lib.rs:44-50</code> (Linux), <code>77-84</code> (macOS)   - Functional equivalence: Identical behavior   - Comment difference: Line 113 has <code>TODO(mbolin):</code> in codex-rs vs <code>TODO:</code> in code-rs</p> <p>Similarities: - Identical hardening strategy (core dumps, ptrace, env vars) - Same platform-specific implementations - Same error handling and exit codes</p>"},{"location":"upstream-mcp-reuse-strategy/#proposed-wrapperpatch-strategy","title":"Proposed Wrapper/Patch Strategy","text":""},{"location":"upstream-mcp-reuse-strategy/#strategy-a-direct-upstream-dependency-recommended","title":"Strategy A: Direct Upstream Dependency (Recommended)","text":"<p>Use codex-rs crates directly with minimal wrapper to handle fork-specific needs.</p> <p>Implementation:</p> <ol> <li> <p>MCP Client: Add configuration parameter for buffer size    <pre><code>// In code-rs workspace Cargo.toml\n[dependencies]\ncodex-mcp-client = { path = \"../codex-rs/mcp-client\" }\n\n// Create thin wrapper in code-rs if buffer config needed\npub fn create_mcp_client_large_responses(\n    program: OsString,\n    args: Vec&lt;OsString&gt;,\n    env: Option&lt;HashMap&lt;String, String&gt;&gt;,\n) -&gt; std::io::Result&lt;codex_mcp_client::McpClient&gt; {\n    // Option 1: Use as-is (upstream buffer may be sufficient)\n    codex_mcp_client::McpClient::new_stdio_client(program, args, env)\n\n    // Option 2: Propose buffer size config upstream\n    // codex_mcp_client::McpClient::with_buffer_capacity(1024*1024)\n    //     .new_stdio_client(program, args, env)\n}\n</code></pre></p> </li> <li> <p>Responses API Proxy: Use upstream with renamed binary    <pre><code>// In code-rs/cli binary embedding\n#[cfg(feature = \"embed-binaries\")]\nconst RESPONSES_PROXY_BINARY: &amp;[u8] =\n    include_bytes!(concat!(env!(\"OUT_DIR\"), \"/codex-responses-api-proxy\"));\n\n// Or: Copy/symlink binary during build with code- prefix\n</code></pre></p> </li> <li> <p>Process Hardening: Direct dependency, re-export if needed    <pre><code>// In code-rs workspace\npub use codex_process_hardening as code_process_hardening;\n</code></pre></p> </li> </ol> <p>Advantages: - Zero maintenance burden for core logic - Automatic upstream security fixes and improvements - Minimal code duplication</p> <p>Disadvantages: - Dependency on external codebase structure - Binary naming requires build script handling</p>"},{"location":"upstream-mcp-reuse-strategy/#strategy-b-feature-flag-in-upstream","title":"Strategy B: Feature-Flag in Upstream","text":"<p>Contribute buffer configuration as optional feature to codex-rs.</p> <p>Upstream PR Proposal: <pre><code>// In codex-rs/mcp-client/src/mcp_client.rs\npub struct McpClientConfig {\n    pub buffer_capacity: Option&lt;usize&gt;,\n    // Future: timeout configs, retry logic, etc.\n}\n\nimpl Default for McpClientConfig {\n    fn default() -&gt; Self {\n        Self { buffer_capacity: None }\n    }\n}\n\nimpl McpClient {\n    pub async fn new_stdio_client_with_config(\n        program: OsString,\n        args: Vec&lt;OsString&gt;,\n        env: Option&lt;HashMap&lt;String, String&gt;&gt;,\n        config: McpClientConfig,\n    ) -&gt; std::io::Result&lt;Self&gt; {\n        // ... existing code ...\n\n        let mut lines = if let Some(capacity) = config.buffer_capacity {\n            BufReader::with_capacity(capacity, stdout).lines()\n        } else {\n            BufReader::new(stdout).lines()\n        };\n\n        // ... rest of implementation ...\n    }\n}\n</code></pre></p> <p>Advantages: - Cleanest long-term solution - Benefits both codebases - No wrapper needed</p> <p>Disadvantages: - Requires upstream acceptance - Timeline depends on upstream review</p>"},{"location":"upstream-mcp-reuse-strategy/#strategy-c-minimal-fork-with-automated-sync","title":"Strategy C: Minimal Fork with Automated Sync","text":"<p>Maintain current fork structure but automate sync from upstream.</p> <p>Implementation: <pre><code># .github/workflows/sync-upstream-mcp.yml\nname: Sync Upstream MCP Components\non:\n  schedule:\n    - cron: '0 0 * * 1'  # Weekly\n  workflow_dispatch:\n\njobs:\n  sync:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Sync mcp-client\n        run: |\n          rsync -av --exclude=Cargo.toml \\\n            ../codex-rs/mcp-client/src/ \\\n            ./code-rs/mcp-client/src/\n          # Apply code-rs specific patches\n          patch -p1 &lt; patches/mcp-client-buffer-size.patch\n</code></pre></p> <p>Patch file (<code>patches/mcp-client-buffer-size.patch</code>): <pre><code>--- a/code-rs/mcp-client/src/mcp_client.rs\n+++ b/code-rs/mcp-client/src/mcp_client.rs\n@@ -141,7 +141,8 @@\n         let reader_handle = {\n             let pending = pending.clone();\n-            let mut lines = BufReader::new(stdout).lines();\n+            // Use a larger buffer size (1MB) to handle large tool responses\n+            let mut lines = BufReader::with_capacity(1024 * 1024, stdout).lines();\n</code></pre></p> <p>Advantages: - Automated tracking of upstream changes - Clear patch management - Fork-specific customization preserved</p> <p>Disadvantages: - Still maintains separate crate - Patch conflicts require manual resolution</p>"},{"location":"upstream-mcp-reuse-strategy/#fork-specific-modifications-required","title":"Fork-Specific Modifications Required","text":""},{"location":"upstream-mcp-reuse-strategy/#1-binarypackage-naming","title":"1. Binary/Package Naming","text":"<p>Current Naming: | Component | codex-rs | code-rs | |-----------|----------|---------| | MCP client package | <code>codex-mcp-client</code> | <code>code-mcp-client</code> | | Proxy package | <code>codex-responses-api-proxy</code> | <code>code-responses-api-proxy</code> | | Proxy binary | <code>codex-responses-api-proxy</code> | <code>code-responses-api-proxy</code> | | Process hardening | <code>codex-process-hardening</code> | <code>code-process-hardening</code> |</p> <p>Resolution Strategies: 1. Build-time rename: Copy and rename binaries during build 2. Cargo alias: Use <code>[[bin]]</code> section with custom name 3. Accept upstream names: Use <code>codex-*</code> binaries in code-rs (simplest)</p>"},{"location":"upstream-mcp-reuse-strategy/#2-buffer-size-configuration","title":"2. Buffer Size Configuration","text":"<p>Options: 1. Environment variable: <code>MCP_BUFFER_SIZE=1048576</code> 2. Config file parameter: Add to MCP server config 3. Upstream contribution: Add to McpClientConfig (Strategy B) 4. Accept default: Test if upstream buffer is sufficient</p>"},{"location":"upstream-mcp-reuse-strategy/#3-process-hardening-integration","title":"3. Process Hardening Integration","text":"<p>Current approach: <pre><code>// code-rs binaries\nuse code_process_hardening;\n\n#[ctor::ctor]\nfn pre_main() {\n    code_process_hardening::pre_main_hardening();\n}\n</code></pre></p> <p>Upstream reuse: <pre><code>// Option 1: Re-export\npub use codex_process_hardening as code_process_hardening;\n\n// Option 2: Direct use\nuse codex_process_hardening;\n\n#[ctor::ctor]\nfn pre_main() {\n    codex_process_hardening::pre_main_hardening();\n}\n</code></pre></p>"},{"location":"upstream-mcp-reuse-strategy/#recommended-approach","title":"Recommended Approach","text":"<p>Phase 1: Immediate (Low-Risk Migration) 1. Depend on <code>codex-mcp-client</code> directly from code-rs 2. Test with default buffer size - may already be sufficient 3. If buffer issues arise, implement thin wrapper (Strategy A, Option 1) 4. Use <code>codex-process-hardening</code> via re-export</p> <p>Phase 2: Short-Term (1-2 weeks) 1. Propose upstream PR for buffer configuration (Strategy B) 2. Migrate responses-api-proxy to use upstream with build-time binary rename 3. Document any remaining customization needs</p> <p>Phase 3: Long-Term Maintenance 1. Establish weekly automated checks for upstream changes 2. Participate in upstream development for shared needs 3. Maintain minimal patch set (ideally zero) for fork-specific requirements</p>"},{"location":"upstream-mcp-reuse-strategy/#upstream-parity-maintenance-checklist","title":"Upstream Parity Maintenance Checklist","text":""},{"location":"upstream-mcp-reuse-strategy/#weekly-tasks","title":"Weekly Tasks","text":"<ul> <li> Check codex-rs commits to mcp-client, responses-api-proxy, process-hardening</li> <li> Review upstream issues/PRs that may affect code-rs usage</li> <li> Run integration tests with latest upstream commits</li> <li> Update dependency pins if changes detected</li> </ul>"},{"location":"upstream-mcp-reuse-strategy/#per-upstream-release-tasks","title":"Per-Upstream-Release Tasks","text":"<ul> <li> Review changelog for breaking changes</li> <li> Test buffer size behavior with new release</li> <li> Validate process hardening still meets security requirements</li> <li> Update code-rs documentation if API changes</li> <li> Run full test suite with new upstream version</li> </ul>"},{"location":"upstream-mcp-reuse-strategy/#monthly-tasks","title":"Monthly Tasks","text":"<ul> <li> Measure and document any performance differences</li> <li> Review need for fork-specific customizations</li> <li> Propose upstream contributions for shared needs</li> <li> Audit dependency tree for security updates</li> </ul>"},{"location":"upstream-mcp-reuse-strategy/#monitoring-triggers","title":"Monitoring Triggers","text":"<ul> <li> Set up GitHub notifications for codex-rs MCP-related commits</li> <li> Create alerts for upstream security advisories</li> <li> Track upstream issue tracker for relevant bug reports</li> <li> Monitor codex-rs release notes for MCP changes</li> </ul>"},{"location":"upstream-mcp-reuse-strategy/#testing-requirements","title":"Testing Requirements","text":"<ul> <li> Verify large tool response handling (&gt;1MB payloads)</li> <li> Test MCP client with various server implementations</li> <li> Validate process hardening on all target platforms (Linux, macOS, Windows)</li> <li> Benchmark buffer size impact on performance</li> <li> Security audit of responses-api-proxy authentication</li> </ul>"},{"location":"upstream-mcp-reuse-strategy/#documentation-maintenance","title":"Documentation Maintenance","text":"<ul> <li> Keep this document updated with any new divergences</li> <li> Document reasons for any fork-specific patches</li> <li> Maintain migration guide for upstream API changes</li> <li> Track technical debt and sunset timeline for workarounds</li> </ul>"},{"location":"upstream-mcp-reuse-strategy/#risk-assessment","title":"Risk Assessment","text":"Risk Likelihood Impact Mitigation Upstream breaks API Low High Pin versions, automated testing Buffer size regression Medium Medium Integration tests with large payloads Binary rename conflicts Low Low Build script validation Security patch delay Low High Automated weekly sync, subscribe to advisories Upstream abandonment Very Low High Fork maintained as fallback (current state)"},{"location":"upstream-mcp-reuse-strategy/#conclusion","title":"Conclusion","text":"<p>The investigation reveals that code-rs can effectively reuse codex-rs MCP components with minimal overhead. The primary difference (1MB buffer) is a trivial configuration change that can be: 1. Tested to determine if even necessary 2. Implemented as a thin wrapper 3. Proposed upstream for mutual benefit</p> <p>Recommended Action: Proceed with Strategy A (Direct Upstream Dependency) for immediate benefits, while pursuing Strategy B (Feature-Flag in Upstream) for long-term sustainability.</p> <p>Expected Outcome: Reduced maintenance burden, automatic security updates, and stronger collaboration with upstream codex-rs development.</p>"},{"location":"zdr/","title":"ZDR","text":""},{"location":"zdr/#zero-data-retention-zdr-usage","title":"Zero data retention (ZDR) usage","text":"<p>Codex CLI does support OpenAI organizations with Zero Data Retention (ZDR) enabled. If your OpenAI organization has Zero Data Retention enabled and you still encounter errors such as:</p> <pre><code>OpenAI rejected the request. Error details: Status: 400, Code: unsupported_parameter, Type: invalid_request_error, Message: 400 Previous response cannot be used for this organization due to Zero Data Retention.\n</code></pre> <p>Ensure you are running <code>codex</code> with <code>--config disable_response_storage=true</code> or add this line to <code>~/.code/config.toml</code> (Code still reads legacy <code>~/.codex/config.toml</code>) to avoid specifying the command line option each time:</p> <pre><code>disable_response_storage = true\n</code></pre> <p>See the configuration documentation on <code>disable_response_storage</code> for details. </p>"},{"location":"aggregation/mcts/","title":"MCTS: Monte Carlo Tree Search","text":"<p>Monte Carlo Tree Search (MCTS) uses UCB-based node selection, rollout simulations, and backpropagation to strategically explore the tree of reasoning possibilities.</p>"},{"location":"aggregation/mcts/#paper","title":"Paper","text":"<p>Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</p>"},{"location":"aggregation/mcts/#algorithm","title":"Algorithm","text":"<p>MCTS has four core phases that repeat:</p>"},{"location":"aggregation/mcts/#phase-1-selection","title":"Phase 1: Selection","text":"<p>Use the UCB formula to select the most promising child node:</p> <pre><code>UCB = (value/visits) + C * sqrt(ln(parent_visits) / visits)\n</code></pre> <p>where: - <code>value/visits</code> = exploitation (average quality of node) - <code>C * sqrt(...)</code> = exploration (uncertainty bonus) - Higher UCB = more promising node</p>"},{"location":"aggregation/mcts/#phase-2-expansion","title":"Phase 2: Expansion","text":"<p>When reaching an unvisited node, generate N possible actions:</p> <pre><code>Current State\n    \u2193\nGenerate N actions via LLM completion\n    \u251c\u2500 Action 1: Next step variation 1\n    \u251c\u2500 Action 2: Next step variation 2\n    \u251c\u2500 ...\n    \u2514\u2500 Action N: Next step variation N\n    \u2193\nCreate N child nodes in tree\n</code></pre>"},{"location":"aggregation/mcts/#phase-3-simulation","title":"Phase 3: Simulation","text":"<p>Rollout to depth D using random action selection:</p> <pre><code>From new node:\n    \u2193\nRepeat D times:\n    \u251c\u2500 Randomly select action\n    \u251c\u2500 Apply to current state\n    \u2514\u2500 Move forward\n    \u2193\nTerminal state reached\n    \u2193\nEvaluate state quality (0.0-1.0)\n</code></pre>"},{"location":"aggregation/mcts/#phase-4-backpropagation","title":"Phase 4: Backpropagation","text":"<p>Update all ancestor nodes with simulation result:</p> <pre><code>Leaf node: value = 0.8, visits = 1\n    \u2193\nParent node: value += 0.8, visits += 1\n    \u2193\nGrandparent node: value += 0.8, visits += 1\n    \u2193\n... up to root\n</code></pre>"},{"location":"aggregation/mcts/#complete-algorithm-pseudocode","title":"Complete Algorithm Pseudocode","text":"<pre><code>MCTS(initial_state, num_simulations):\n    root = create_node(initial_state)\n\n    for i in 1 to num_simulations:\n        node = root\n\n        // Phase 1: Select\n        while not is_terminal(node.state):\n            if node has unvisited children:\n                break\n            node = select_child_with_highest_ucb(node)\n\n        // Phase 2: Expand\n        if not is_terminal(node.state) and node.visits &gt; 0:\n            actions = generate_actions(node.state)\n            node = create_random_child(node, actions)\n\n        // Phase 3: Simulate\n        state = node.state\n        for depth in 1 to simulation_depth:\n            if is_terminal(state):\n                break\n            actions = generate_actions(state)\n            action = select_random(actions)\n            state = apply_action(state, action)\n\n        value = evaluate_state(state)\n\n        // Phase 4: Backpropagate\n        while node != null:\n            node.visits += 1\n            node.value += value\n            node = node.parent\n\n    return best_child(root)  // Most visited child\n</code></pre>"},{"location":"aggregation/mcts/#when-to-use-mcts","title":"When to Use MCTS","text":""},{"location":"aggregation/mcts/#good-for","title":"\u2705 Good For:","text":"<ul> <li>Multi-step reasoning with complex decision trees</li> <li>Strategy problems requiring lookahead</li> <li>Logic puzzles with branching paths</li> <li>Planning problems with multiple options</li> <li>Code design with architectural choices</li> <li>Scientific problems with hypothesis testing</li> </ul>"},{"location":"aggregation/mcts/#avoid-for","title":"\u274c Avoid For:","text":"<ul> <li>Simple single-step questions</li> <li>Time-critical applications (MCTS is slow)</li> <li>Problems with few decision points</li> <li>Tasks where all paths are equivalent</li> </ul>"},{"location":"aggregation/mcts/#configuration","title":"Configuration","text":"<pre><code>use code_mars::{MarsConfig, MarsCoordinator, types::AggregationMethod};\n\n// Enable MCTS aggregation\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n    .with_mcts_simulation_depth(2)\n    .with_mcts_exploration_weight(0.3)\n    .with_mcts_num_simulations(4)\n    .with_mcts_num_actions(3);\n\nlet mut coordinator = MarsCoordinator::new(config);\n</code></pre>"},{"location":"aggregation/mcts/#configuration-options","title":"Configuration Options","text":"<pre><code>pub struct MCTSConfig {\n    // How deep to simulate (default: 1)\n    pub simulation_depth: usize,\n\n    // UCB exploration coefficient (default: 0.2)\n    pub exploration_weight: f32,\n\n    // Number of simulations per search (default: 2)\n    pub num_simulations: usize,\n\n    // Number of actions to generate per expansion (default: 3)\n    pub num_actions: usize,\n\n    // Temperature for action generation (default: 1.0)\n    pub generation_temperature: f32,\n\n    // Temperature for state evaluation (default: 0.1)\n    pub evaluation_temperature: f32,\n\n    // Maximum conversation history length (default: 10)\n    pub max_history_length: usize,\n}\n</code></pre>"},{"location":"aggregation/mcts/#example","title":"Example","text":"<pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = ModelClient::default();\n\n    // Configure MCTS for deep reasoning\n    let config = MarsConfig::new()\n        .with_advanced_features()\n        .with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n        .with_mcts_simulation_depth(3)\n        .with_mcts_exploration_weight(0.5)\n        .with_mcts_num_simulations(10);\n\n    let mut coordinator = MarsCoordinator::new(config);\n\n    let query = \"Plan a 5-day trip to Japan focusing on cultural sites\";\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    println!(\"Reasoning:\\n{}\", result.reasoning);\n    println!(\"Final Plan:\\n{}\", result.answer);\n\n    Ok(())\n}\n</code></pre>"},{"location":"aggregation/mcts/#understanding-the-parameters","title":"Understanding the Parameters","text":""},{"location":"aggregation/mcts/#simulation_depth","title":"<code>simulation_depth</code>","text":"<ul> <li>1: Shallow exploration, faster but less lookahead</li> <li>2-3: Balanced depth and speed (recommended)</li> <li>4+: Deep exploration, slow but comprehensive</li> </ul>"},{"location":"aggregation/mcts/#exploration_weight-c-in-ucb-formula","title":"<code>exploration_weight</code> (C in UCB formula)","text":"<ul> <li>0.1: More exploitation, use known good paths</li> <li>0.2: Balanced (default)</li> <li>0.5-1.0: More exploration, try diverse paths</li> </ul>"},{"location":"aggregation/mcts/#num_simulations","title":"<code>num_simulations</code>","text":"<ul> <li>2-4: Quick exploration</li> <li>4-8: Balanced (default range)</li> <li>10+: Thorough exploration, slow</li> </ul>"},{"location":"aggregation/mcts/#num_actions","title":"<code>num_actions</code>","text":"<ul> <li>2-3: Few options per step (default: 3)</li> <li>4-5: More diversity</li> <li>6+: Very diverse, slower</li> </ul>"},{"location":"aggregation/mcts/#how-mcts-differs","title":"How MCTS Differs","text":""},{"location":"aggregation/mcts/#vs-moa","title":"vs MOA","text":"<ul> <li>MOA: All approaches explored in parallel (horizontal)</li> <li>MCTS: Strategic exploration of tree (vertical)</li> </ul>"},{"location":"aggregation/mcts/#vs-simple-greedy","title":"vs Simple Greedy","text":"<ul> <li>Greedy: Pick highest-value action at each step (local optimization)</li> <li>MCTS: Consider future consequences via simulation (global optimization)</li> </ul>"},{"location":"aggregation/mcts/#vs-exhaustive-search","title":"vs Exhaustive Search","text":"<ul> <li>Exhaustive: Try every possible path (exponential time)</li> <li>MCTS: Intelligently focus on promising paths (polynomial time)</li> </ul>"},{"location":"aggregation/mcts/#performance","title":"Performance","text":""},{"location":"aggregation/mcts/#time-complexity","title":"Time Complexity","text":"<ul> <li>Per simulation: O(simulation_depth \u00d7 num_actions)</li> <li>Total: O(num_simulations \u00d7 simulation_depth \u00d7 num_actions)</li> <li>Typical: O(n\u00b3) where n = num_simulations</li> </ul>"},{"location":"aggregation/mcts/#space-complexity","title":"Space Complexity","text":"<ul> <li>Tree nodes: O(num_simulations \u00d7 branching_factor)</li> <li>Typical: Store ~100-1000 nodes</li> </ul>"},{"location":"aggregation/mcts/#cost","title":"Cost","text":"<ul> <li>Tokens: 20K-60K per search</li> <li>Time: 1-5 minutes for deep exploration</li> <li>Quality: +20-40% on multi-step problems</li> </ul>"},{"location":"aggregation/mcts/#tuning-for-your-problem","title":"Tuning for Your Problem","text":""},{"location":"aggregation/mcts/#for-accuracy-ignore-speed","title":"For Accuracy (Ignore Speed)","text":"<pre><code>.with_mcts_simulation_depth(4)\n.with_mcts_num_simulations(20)\n.with_mcts_exploration_weight(0.5)\n</code></pre>"},{"location":"aggregation/mcts/#for-speed-accept-lower-quality","title":"For Speed (Accept Lower Quality)","text":"<pre><code>.with_mcts_simulation_depth(1)\n.with_mcts_num_simulations(2)\n.with_mcts_exploration_weight(0.1)\n</code></pre>"},{"location":"aggregation/mcts/#balanced","title":"Balanced","text":"<pre><code>.with_mcts_simulation_depth(2)\n.with_mcts_num_simulations(4)\n.with_mcts_exploration_weight(0.3)\n</code></pre>"},{"location":"aggregation/mcts/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aggregation/mcts/#takes-too-long","title":"Takes Too Long","text":"<ul> <li>Reduce <code>simulation_depth</code> to 1-2</li> <li>Reduce <code>num_simulations</code> to 2-4</li> <li>Reduce <code>num_actions</code> to 2</li> </ul>"},{"location":"aggregation/mcts/#quality-too-low","title":"Quality Too Low","text":"<ul> <li>Increase <code>simulation_depth</code> to 3-4</li> <li>Increase <code>num_simulations</code> to 6-10</li> <li>Increase exploration_weight to 0.4-0.6</li> </ul>"},{"location":"aggregation/mcts/#token-usage-too-high","title":"Token Usage Too High","text":"<ul> <li>Use fewer simulations</li> <li>Reduce simulation depth</li> <li>Shorten problem description</li> </ul>"},{"location":"aggregation/mcts/#see-also","title":"See Also","text":"<ul> <li>Aggregation Overview - Compare with other methods</li> <li>MOA - Horizontal diversity alternative</li> <li>Configuration Guide - Full options</li> </ul>"},{"location":"aggregation/moa/","title":"MOA: Mixture of Agents","text":"<p>Mixture of Agents (MOA) generates diverse completions in parallel, critiques each for strengths and weaknesses, then synthesizes a final answer incorporating all insights.</p>"},{"location":"aggregation/moa/#paper","title":"Paper","text":"<p>Mixture of Agents: Enhancing LLM Capabilities through Collaborative Specialization</p>"},{"location":"aggregation/moa/#algorithm","title":"Algorithm","text":""},{"location":"aggregation/moa/#step-1-generate-diverse-completions","title":"Step 1: Generate Diverse Completions","text":"<p>Use high temperature (1.0) to generate N different completions of the same prompt.</p> <pre><code>Input: \"Solve: 3x + 5 = 20\"\n    \u2193\nGenerate 5 completions (temp=1.0)\n    \u251c\u2500 Solution 1: Step-by-step algebraic approach\n    \u251c\u2500 Solution 2: Visual/graphical reasoning\n    \u251c\u2500 Solution 3: Substitution method\n    \u251c\u2500 Solution 4: Alternative verification\n    \u2514\u2500 Solution 5: Conceptual explanation\n</code></pre>"},{"location":"aggregation/moa/#step-2-critique-each-completion","title":"Step 2: Critique Each Completion","text":"<p>Analyze each solution for: - Correctness - Is the answer right? - Completeness - Does it address all aspects? - Clarity - Is it easy to understand? - Rigor - Is the reasoning sound?</p> <pre><code>For each solution:\n    \u2193\nCritique prompt: \"Analyze this solution's strengths and weaknesses\"\n    \u2193\nGenerate critique\n    \u251c\u2500 Strengths: [list]\n    \u2514\u2500 Weaknesses: [list]\n</code></pre>"},{"location":"aggregation/moa/#step-3-synthesize-final-answer","title":"Step 3: Synthesize Final Answer","text":"<p>Combine all solutions and critiques into optimal final answer.</p> <pre><code>All Solutions + Critiques\n    \u2193\nSynthesis prompt: \"Synthesize the best answer\"\n    \u2193\nFinal Answer\n</code></pre>"},{"location":"aggregation/moa/#when-to-use-moa","title":"When to Use MOA","text":""},{"location":"aggregation/moa/#good-for","title":"\u2705 Good For:","text":"<ul> <li>Math problems with multiple solution approaches</li> <li>Code generation with multiple algorithmic solutions</li> <li>Essay writing combining multiple perspectives</li> <li>Analysis requiring comprehensive coverage</li> <li>Proof writing showing different proof techniques</li> </ul>"},{"location":"aggregation/moa/#avoid-for","title":"\u274c Avoid For:","text":"<ul> <li>Simple factual questions (overkill)</li> <li>Real-time applications (MOA is slow)</li> <li>Very short answers</li> <li>Tasks where all approaches are equivalent</li> </ul>"},{"location":"aggregation/moa/#configuration","title":"Configuration","text":"<pre><code>use code_mars::{MarsConfig, MarsCoordinator};\n\n// Enable MOA aggregation\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_moa_num_completions(5);  // Default: 3\n\nlet mut coordinator = MarsCoordinator::new(config);\n</code></pre>"},{"location":"aggregation/moa/#configuration-options","title":"Configuration Options","text":"<pre><code>MarsConfig {\n    // Number of completions to generate\n    moa_num_completions: 5,  // Default: 3\n\n    // Enable fallback if n parameter not supported by model\n    moa_fallback_enabled: true,  // Default: true\n}\n</code></pre>"},{"location":"aggregation/moa/#example","title":"Example","text":"<pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = ModelClient::default();\n\n    // Configure MOA for maximum diversity\n    let config = MarsConfig::new()\n        .with_advanced_features()\n        .with_moa_aggregation()\n        .with_moa_num_completions(7);\n\n    let mut coordinator = MarsCoordinator::new(config);\n\n    let query = \"Prove that the sum of squares of first n integers is n(n+1)(2n+1)/6\";\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    println!(\"Proof:\\n{}\", result.reasoning);\n    println!(\"Final Answer: {}\", result.answer);\n\n    Ok(())\n}\n</code></pre>"},{"location":"aggregation/moa/#how-moa-differs","title":"How MOA Differs","text":""},{"location":"aggregation/moa/#vs-single-agent","title":"vs Single Agent","text":"<ul> <li>Single: One attempt, one perspective</li> <li>MOA: Multiple perspectives synthesized</li> </ul>"},{"location":"aggregation/moa/#vs-ensemble-voting","title":"vs Ensemble Voting","text":"<ul> <li>Ensemble: Simple majority voting</li> <li>MOA: Intelligent synthesis of critiques</li> </ul>"},{"location":"aggregation/moa/#vs-mcts","title":"vs MCTS","text":"<ul> <li>MOA: Horizontal diversity (many approaches in parallel)</li> <li>MCTS: Vertical depth (strategic exploration of one path)</li> </ul>"},{"location":"aggregation/moa/#performance","title":"Performance","text":""},{"location":"aggregation/moa/#time-complexity","title":"Time Complexity","text":"<ul> <li>Generate completions: O(n) parallel (n = num_completions)</li> <li>Critique each: O(n) parallel</li> <li>Synthesize: O(1)</li> <li>Total: O(n) with parallelization</li> </ul>"},{"location":"aggregation/moa/#space-complexity","title":"Space Complexity","text":"<ul> <li>Store all completions: O(n \u00d7 completion_length)</li> <li>Typical: ~5K-50K tokens for 5 completions</li> </ul>"},{"location":"aggregation/moa/#cost","title":"Cost","text":"<ul> <li>Tokens: 10K-30K per aggregation</li> <li>Time: 30-60 seconds (with parallel API calls)</li> <li>Success rate: +10-30% on most tasks</li> </ul>"},{"location":"aggregation/moa/#algorithm-pseudocode","title":"Algorithm Pseudocode","text":"<pre><code>async fn aggregate_moa(\n    query: &amp;str,\n    system_prompt: &amp;str,\n    num_completions: usize,\n    provider: &amp;dyn LLMProvider,\n) -&gt; Result&lt;Solution&gt; {\n    // Step 1: Generate diverse completions\n    let mut completions = Vec::new();\n    for i in 0..num_completions {\n        let completion = provider.complete(\n            &amp;query,\n            Some(system_prompt)\n        ).await?;\n        completions.push(completion);\n    }\n\n    // Step 2: Critique each completion\n    let mut critiques = Vec::new();\n    for completion in &amp;completions {\n        let critique_prompt = format!(\n            \"Analyze this solution's strengths and weaknesses:\\n{}\",\n            completion\n        );\n        let critique = provider.complete(\n            &amp;critique_prompt,\n            Some(system_prompt)\n        ).await?;\n        critiques.push(critique);\n    }\n\n    // Step 3: Synthesize final answer\n    let synthesis_prompt = format!(\n        \"Given these solutions and critiques, synthesize the best answer:\\n\\\n         Solutions: {:?}\\n\\\n         Critiques: {:?}\",\n        completions, critiques\n    );\n    let final_answer = provider.complete(\n        &amp;synthesis_prompt,\n        Some(system_prompt)\n    ).await?;\n\n    Ok(Solution::new(\n        \"moa-aggregator\".to_string(),\n        format!(\"Completions: {:?}\\nCritiques: {:?}\", completions, critiques),\n        final_answer,\n        0.5,\n        token_count,\n    ))\n}\n</code></pre>"},{"location":"aggregation/moa/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"aggregation/moa/#high-quality-mode","title":"High-Quality Mode","text":"<pre><code>let config = MarsConfig::new()\n    .with_moa_aggregation()\n    .with_moa_num_completions(10)  // More completions\n    .with_num_agents(5);             // More initial agents\n</code></pre>"},{"location":"aggregation/moa/#fast-mode","title":"Fast Mode","text":"<pre><code>let config = MarsConfig::new()\n    .with_moa_aggregation()\n    .with_moa_num_completions(3)   // Fewer completions\n    .lightweight();                  // Fewer agents overall\n</code></pre>"},{"location":"aggregation/moa/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aggregation/moa/#quality-too-low","title":"Quality Too Low","text":"<ul> <li>Increase <code>moa_num_completions</code> to 5-7</li> <li>Ensure system prompt is clear</li> <li>Check model temperature is appropriate</li> </ul>"},{"location":"aggregation/moa/#takes-too-long","title":"Takes Too Long","text":"<ul> <li>Reduce <code>moa_num_completions</code> to 3-4</li> <li>Use lighter model (gpt-3.5-turbo)</li> <li>Disable other phases (strategy network)</li> </ul>"},{"location":"aggregation/moa/#token-usage-too-high","title":"Token Usage Too High","text":"<ul> <li>Reduce <code>moa_num_completions</code></li> <li>Use lighter model</li> <li>Shorten query and system prompt</li> </ul>"},{"location":"aggregation/moa/#see-also","title":"See Also","text":"<ul> <li>Aggregation Overview - Compare with other methods</li> <li>MCTS - Vertical exploration alternative</li> <li>Configuration Guide - Full config options</li> </ul>"},{"location":"aggregation/overview/","title":"Aggregation Methods Overview","text":"<p>Aggregation is Phase 2a of the MARS pipeline. After Phase 1 generates N initial solutions, aggregation synthesizes and refines them into improved solutions.</p>"},{"location":"aggregation/overview/#three-strategies","title":"Three Strategies","text":"<p>MARS provides three complementary aggregation strategies:</p>"},{"location":"aggregation/overview/#1-moa-mixture-of-agents-horizontal-diversity","title":"1. MOA (Mixture of Agents) - Horizontal Diversity","text":"<p>Explores different solution approaches in parallel.</p> <ul> <li>Best For: Mathematical proofs, multiple solution paths, diverse perspectives</li> <li>Paper: Mixture of Agents: Enhancing LLM Capabilities through Collaborative Specialization</li> <li>Speed: Fast (parallel completions)</li> <li>Quality: High diversity of approaches</li> </ul>"},{"location":"aggregation/overview/#2-mcts-monte-carlo-tree-search-vertical-exploration","title":"2. MCTS (Monte Carlo Tree Search) - Vertical Exploration","text":"<p>Explores deep reasoning chains with strategic selection.</p> <ul> <li>Best For: Complex multi-step problems, strategic decision making</li> <li>Paper: Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</li> <li>Speed: Slower (sequential tree exploration)</li> <li>Quality: High depth and strategic coverage</li> </ul>"},{"location":"aggregation/overview/#3-rsa-reward-seeking-aggregation-iterative-refinement","title":"3. RSA (Reward-Seeking Aggregation) - Iterative Refinement","text":"<p>Maintains and improves a population of solutions.</p> <ul> <li>Best For: Gradual improvement, population-based optimization</li> <li>Speed: Medium (iterative loops)</li> <li>Quality: Steady improvement trajectory</li> </ul>"},{"location":"aggregation/overview/#comparison-table","title":"Comparison Table","text":"Aspect MOA MCTS RSA Exploration Type Horizontal (parallel) Vertical (sequential) Iterative (refinement) Core Algorithm Generate \u2192 Critique \u2192 Synthesize Select \u2192 Expand \u2192 Simulate \u2192 Backprop Select \u2192 Refine \u2192 Loop Best For Diverse approaches Deep reasoning Iterative improvement Time Complexity O(n completions) O(depth \u00d7 simulations) O(loops \u00d7 selections) Quality Breadth Depth Consistency Paper MOA 2025 Coulom 2006 MARS 2025"},{"location":"aggregation/overview/#how-they-work","title":"How They Work","text":""},{"location":"aggregation/overview/#moa-horizontal","title":"MOA (Horizontal)","text":"<pre><code>Initial Solutions (3)\n    \u2193\nGenerate N Completions (high temp)\n    \u2193\nCritique Each Completion\n    \u2193\nSynthesize Final Answer\n    \u2193\nImproved Solution\n</code></pre>"},{"location":"aggregation/overview/#mcts-vertical","title":"MCTS (Vertical)","text":"<pre><code>Initial State (root node)\n    \u2193\nSelect Most Promising Path (UCB formula)\n    \u2193\nExpand: Generate Actions\n    \u2193\nSimulate: Rollout to Depth\n    \u2193\nBackpropagate: Update Values\n    \u2193 (repeat many times)\nBest Path\n</code></pre>"},{"location":"aggregation/overview/#rsa-iterative","title":"RSA (Iterative)","text":"<pre><code>Population: [S1, S2, S3, S4, S5, S6]\n    \u2193\nSelect K Solutions\n    \u2193\nRefine Each\n    \u2193\nAdd Back to Population\n    \u2193 (repeat T times)\nImproved Population\n</code></pre>"},{"location":"aggregation/overview/#decision-matrix","title":"Decision Matrix","text":"<p>Choose your aggregation method based on your problem:</p> Problem Type Recommended Reason Math proofs with multiple approaches MOA Capture different solution methods Complex logic with many decision points MCTS Strategic path exploration Problems needing gradual refinement RSA Iterative improvement loop Creative writing or generation MOA Diverse styles and perspectives Code generation (multiple implementations) MOA Different algorithmic approaches Strategy/planning problems MCTS Tree-based exploration Scientific hypotheses RSA Hypothesis refinement"},{"location":"aggregation/overview/#configuration-examples","title":"Configuration Examples","text":""},{"location":"aggregation/overview/#using-moa","title":"Using MOA","text":"<pre><code>let config = MarsConfig::new()\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_moa_num_completions(5);\n\nlet mut coordinator = MarsCoordinator::new(config);\n</code></pre>"},{"location":"aggregation/overview/#using-mcts","title":"Using MCTS","text":"<pre><code>use code_mars::types::AggregationMethod;\n\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n    .with_mcts_simulation_depth(2)\n    .with_mcts_exploration_weight(0.3)\n    .with_mcts_num_simulations(4);\n\nlet mut coordinator = MarsCoordinator::new(config);\n</code></pre>"},{"location":"aggregation/overview/#using-rsa-default","title":"Using RSA (Default)","text":"<pre><code>let config = MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation(true);  // RSA is default\n\nlet mut coordinator = MarsCoordinator::new(config);\n</code></pre>"},{"location":"aggregation/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"aggregation/overview/#moa","title":"MOA","text":"<ul> <li>Time: Fast (~30 seconds for 5 completions)</li> <li>Tokens: 10K-30K per aggregation</li> <li>Parallelizable: Yes (completions in parallel)</li> <li>Best For: Time-constrained tasks</li> </ul>"},{"location":"aggregation/overview/#mcts","title":"MCTS","text":"<ul> <li>Time: Slow (~2-5 minutes for deep exploration)</li> <li>Tokens: 20K-60K per aggregation</li> <li>Parallelizable: Limited (sequential tree)</li> <li>Best For: Accuracy-first tasks</li> </ul>"},{"location":"aggregation/overview/#rsa","title":"RSA","text":"<ul> <li>Time: Medium (~1-3 minutes for 3 loops)</li> <li>Tokens: 15K-45K per aggregation</li> <li>Parallelizable: Yes (loop iterations parallel)</li> <li>Best For: Balanced approach</li> </ul>"},{"location":"aggregation/overview/#combining-methods","title":"Combining Methods","text":"<p>You can also combine strategies for even better results:</p> <pre><code>// Phase 2a: Start with MOA for diversity\n// Then Phase 2b: Use strategy network to share insights\n// Then improve further with RSA-style refinement\n\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_strategy_network(true);\n</code></pre>"},{"location":"aggregation/overview/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>MOA Details - Deep dive into Mixture of Agents</li> <li>MCTS Details - Understanding Monte Carlo Tree Search</li> <li>RSA Details - Reward-Seeking Aggregation internals</li> </ul>"},{"location":"aggregation/overview/#next-steps","title":"Next Steps","text":"<ol> <li>Read MOA in Detail</li> <li>Understand MCTS Algorithm</li> <li>Learn RSA Strategy</li> <li>Try Configuration Guide</li> </ol>"},{"location":"aggregation/rsa/","title":"RSA: Reward-Seeking Aggregation","text":"<p>Reward-Seeking Aggregation (RSA) maintains a population of solutions, iteratively selects the best ones for refinement, and gradually improves the population through multiple refinement cycles.</p>"},{"location":"aggregation/rsa/#algorithm","title":"Algorithm","text":""},{"location":"aggregation/rsa/#step-1-initialize-population","title":"Step 1: Initialize Population","text":"<p>Start with N solutions (default: 6) from initial agent exploration.</p> <pre><code>Population: [S1, S2, S3, S4, S5, S6]\n</code></pre>"},{"location":"aggregation/rsa/#step-2-select-diverse-solutions","title":"Step 2: Select Diverse Solutions","text":"<p>Randomly select K solutions (default: 3) for refinement:</p> <pre><code>From Population: [S1, S2, S3, S4, S5, S6]\n    \u2193\nRandomly select K=3: [S2, S4, S6]\n</code></pre>"},{"location":"aggregation/rsa/#step-3-refine-selected-solutions","title":"Step 3: Refine Selected Solutions","text":"<p>Use LLM to improve each selected solution:</p> <pre><code>For each selected solution:\n    \u2193\nPrompt: \"Improve this solution: ...\"\n    \u2193\nRefined solution\n</code></pre>"},{"location":"aggregation/rsa/#step-4-add-back-to-population","title":"Step 4: Add Back to Population","text":"<p>Insert refined solutions into population:</p> <pre><code>Population + Refined = [S1, S2, S3, S4, S5, S6, S2', S4', S6']\n</code></pre>"},{"location":"aggregation/rsa/#step-5-repeat","title":"Step 5: Repeat","text":"<p>Loop steps 2-4 for T iterations (default: 3):</p> <pre><code>Iteration 1: Select &amp; refine\n    \u2193\nIteration 2: Select &amp; refine\n    \u2193\nIteration 3: Select &amp; refine\n    \u2193\nFinal Population\n</code></pre>"},{"location":"aggregation/rsa/#when-to-use-rsa","title":"When to Use RSA","text":""},{"location":"aggregation/rsa/#good-for","title":"\u2705 Good For:","text":"<ul> <li>Iterative improvement of solutions</li> <li>Refinement problems requiring multiple passes</li> <li>Balanced approach between quality and time</li> <li>Stable improvement with predictable trajectory</li> <li>Large solution spaces where diversity matters</li> </ul>"},{"location":"aggregation/rsa/#avoid-for","title":"\u274c Avoid For:","text":"<ul> <li>Simple problems (just use single agent)</li> <li>Very complex multi-step reasoning (use MCTS)</li> <li>Time-critical applications</li> <li>Problems with single correct approach</li> </ul>"},{"location":"aggregation/rsa/#configuration","title":"Configuration","text":"<pre><code>use code_mars::MarsConfig;\n\n// Enable RSA (enabled by default with advanced features)\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation(true);  // RSA is the default\n\nlet mut coordinator = MarsCoordinator::new(config);\n</code></pre>"},{"location":"aggregation/rsa/#configuration-options","title":"Configuration Options","text":"<pre><code>pub struct MarsConfig {\n    // Size of solution population\n    pub aggregation_population_size: usize,        // Default: 6\n\n    // Number of solutions to select per loop\n    pub aggregation_selection_size: usize,         // Default: 3\n\n    // Number of aggregation loops\n    pub aggregation_loops: usize,                  // Default: 3\n}\n</code></pre>"},{"location":"aggregation/rsa/#example","title":"Example","text":"<pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let client = ModelClient::default();\n\n    // Configure RSA for iterative improvement\n    let config = MarsConfig::new()\n        .with_advanced_features()\n        .with_aggregation(true)\n        .with_num_agents(3)\n        .with_max_iterations(7);\n\n    let mut coordinator = MarsCoordinator::new(config);\n\n    let query = \"Write a comprehensive essay on the French Revolution\";\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    println!(\"Final Essay:\\n{}\", result.reasoning);\n    println!(\"Summary: {}\", result.answer);\n\n    Ok(())\n}\n</code></pre>"},{"location":"aggregation/rsa/#how-rsa-works","title":"How RSA Works","text":""},{"location":"aggregation/rsa/#population-based-approach","title":"Population-Based Approach","text":"<p>Unlike MOA (parallel exploration) or MCTS (tree exploration), RSA maintains a population that evolves over time:</p> <pre><code>Generation 0: [S1, S2, S3, S4, S5, S6]\n    \u2193 Select K=3, refine, add back\nGeneration 1: [S1, S2, S3, S4, S5, S6, S2', S4', S6']\n    \u2193 Select K=3, refine, add back\nGeneration 2: [S1, S2, S3, S4, S5, S6, S2', S4', S6', S3'', S5'', S1'']\n    \u2193 Select K=3, refine, add back\nGeneration 3: [... evolved population ...]\n</code></pre>"},{"location":"aggregation/rsa/#random-selection-for-diversity","title":"Random Selection for Diversity","text":"<ul> <li>Not selecting only the best solutions</li> <li>Random selection ensures diversity in refinement targets</li> <li>Prevents premature convergence to local optima</li> </ul>"},{"location":"aggregation/rsa/#algorithm-pseudocode","title":"Algorithm Pseudocode","text":"<pre><code>async fn aggregate_rsa(\n    solutions: &amp;[Solution],\n    population_size: usize,\n    selection_size: usize,\n    num_loops: usize,\n) -&gt; Result&lt;Vec&lt;Solution&gt;&gt; {\n    let mut aggregated = Vec::new();\n\n    if solutions.is_empty() {\n        return Ok(aggregated);\n    }\n\n    let mut population = solutions.to_vec();\n\n    // Limit population to requested size\n    if population.len() &gt; population_size {\n        population.truncate(population_size);\n    }\n\n    // Perform aggregation loops\n    for loop_idx in 0..num_loops {\n        // Step 2: Select diverse solutions\n        let selected = select_diverse_solutions(&amp;population, selection_size)?;\n\n        // Step 3: Refine selected solutions\n        if !selected.is_empty() {\n            let refined = synthesize_solution(&amp;selected, loop_idx)?;\n            aggregated.push(refined.clone());\n\n            // Step 4: Add back to population\n            population.push(refined);\n        }\n    }\n\n    Ok(aggregated)\n}\n</code></pre>"},{"location":"aggregation/rsa/#performance","title":"Performance","text":""},{"location":"aggregation/rsa/#time-complexity","title":"Time Complexity","text":"<ul> <li>Per loop: O(selection_size) refinements</li> <li>Total: O(num_loops \u00d7 selection_size)</li> <li>Typical: Linear (much faster than MCTS)</li> </ul>"},{"location":"aggregation/rsa/#space-complexity","title":"Space Complexity","text":"<ul> <li>Population: O(population_size)</li> <li>Typical: Keep ~6-10 solutions</li> </ul>"},{"location":"aggregation/rsa/#cost","title":"Cost","text":"<ul> <li>Tokens: 15K-45K per aggregation</li> <li>Time: 1-3 minutes (faster than MCTS)</li> <li>Quality: Steady improvement ~+5-15% per loop</li> </ul>"},{"location":"aggregation/rsa/#comparison-with-moa-and-mcts","title":"Comparison with MOA and MCTS","text":"Aspect RSA MOA MCTS Approach Population + refinement Parallel diversity Tree exploration Time Medium Fast Slow Tokens 15K-45K 10K-30K 20K-60K Best For Iterative improvement Multiple perspectives Strategic planning Quality Steady improvement Broad coverage Deep lookahead Complexity Simple Medium Complex"},{"location":"aggregation/rsa/#tuning-rsa","title":"Tuning RSA","text":""},{"location":"aggregation/rsa/#for-higher-quality","title":"For Higher Quality","text":"<pre><code>.with_aggregation_population_size(8)   // Larger population\n.with_aggregation_selection_size(4)    // Refine more per loop\n.with_aggregation_loops(5)             // More iterations\n</code></pre>"},{"location":"aggregation/rsa/#for-speed","title":"For Speed","text":"<pre><code>.with_aggregation_population_size(4)   // Smaller population\n.with_aggregation_selection_size(2)    // Refine fewer per loop\n.with_aggregation_loops(2)             // Fewer iterations\n</code></pre>"},{"location":"aggregation/rsa/#balanced-default","title":"Balanced (Default)","text":"<pre><code>.with_aggregation_population_size(6)   // Default\n.with_aggregation_selection_size(3)    // Default\n.with_aggregation_loops(3)             // Default\n</code></pre>"},{"location":"aggregation/rsa/#why-random-selection","title":"Why Random Selection?","text":"<p>Instead of always selecting the best solutions:</p> <ol> <li>Preserves diversity - Poor solutions might improve significantly</li> <li>Avoids local optima - Exploiting low-quality solutions can reveal new insights</li> <li>Statistical validity - Population sampling is more robust</li> <li>Faster convergence - Random walk often beats greedy</li> </ol> <p>Example: <pre><code>Population: [S1(0.7), S2(0.5), S3(0.9), S4(0.4), S5(0.8), S6(0.6)]\n\nGreedy selection: [S3(0.9), S5(0.8), S1(0.7)]\n    \u2192 Refine only high-quality solutions\n    \u2192 May miss improvement opportunities in weaker solutions\n\nRandom selection: [S2(0.5), S4(0.4), S6(0.6)]\n    \u2192 Refine weaker solutions\n    \u2192 Discover unexpected improvements\n</code></pre></p>"},{"location":"aggregation/rsa/#troubleshooting","title":"Troubleshooting","text":""},{"location":"aggregation/rsa/#quality-not-improving","title":"Quality Not Improving","text":"<ul> <li>Increase <code>aggregation_loops</code> to 4-5</li> <li>Increase <code>aggregation_selection_size</code> to 4</li> <li>Check that refinement prompts are effective</li> </ul>"},{"location":"aggregation/rsa/#takes-too-long","title":"Takes Too Long","text":"<ul> <li>Reduce <code>aggregation_loops</code> to 1-2</li> <li>Reduce <code>aggregation_selection_size</code> to 2</li> <li>Reduce population size</li> </ul>"},{"location":"aggregation/rsa/#token-usage-too-high","title":"Token Usage Too High","text":"<ul> <li>Reduce <code>aggregation_loops</code></li> <li>Reduce <code>aggregation_selection_size</code></li> <li>Shorter solution descriptions</li> </ul>"},{"location":"aggregation/rsa/#advanced-hybrid-approaches","title":"Advanced: Hybrid Approaches","text":"<p>Combine RSA with other phases:</p> <pre><code>// Phase 2a: RSA aggregation\n// Phase 2b: Strategy network (extract insights from population)\n// Phase 4: Iterative improvement (improves unverified solutions)\n\n// This creates a \"double refinement\" system\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation(true)           // RSA\n    .with_strategy_network(true)       // Strategy extraction\n    .with_max_iterations(7);           // More improvement loops\n</code></pre>"},{"location":"aggregation/rsa/#see-also","title":"See Also","text":"<ul> <li>Aggregation Overview - Compare with other methods</li> <li>MOA - Parallel diversity approach</li> <li>MCTS - Tree-based exploration</li> <li>Configuration Guide - Full options</li> </ul>"},{"location":"api/types/","title":"Core Types","text":"<p>Key types in the MARS API.</p>"},{"location":"api/types/#solution","title":"Solution","text":"<pre><code>pub struct Solution {\n    pub id: String,\n    pub agent_id: String,\n    pub reasoning: String,\n    pub answer: String,\n    pub temperature: f32,\n    pub token_count: usize,\n    pub verification_passes: usize,\n    pub is_verified: bool,\n    pub verification_score: f32,\n}\n</code></pre>"},{"location":"api/types/#marsoutput","title":"MarsOutput","text":"<pre><code>pub struct MarsOutput {\n    pub answer: String,\n    pub reasoning: String,\n    pub all_solutions: Vec&lt;Solution&gt;,\n    pub final_solution_id: String,\n    pub selection_method: SelectionMethod,\n    pub iterations: usize,\n    pub total_tokens: usize,\n}\n</code></pre>"},{"location":"api/types/#marsevent","title":"MarsEvent","text":"<pre><code>pub enum MarsEvent {\n    ExplorationStarted { num_agents: usize },\n    SolutionGenerated { solution_id: String, agent_id: String },\n    VerificationStarted,\n    SolutionVerified { solution_id: String, is_correct: bool, score: f32 },\n    ImprovementStarted { iteration: usize },\n    AnswerSynthesized { answer: String },\n    Completed { final_answer: String, method: String },\n    Error { message: String },\n}\n</code></pre>"},{"location":"api/types/#aggregationmethod","title":"AggregationMethod","text":"<pre><code>pub enum AggregationMethod {\n    MOA,                    // Mixture of Agents\n    MCTS,                   // Monte Carlo Tree Search\n    RSA,                    // Reward-Seeking Aggregation\n    MajorityVoting,\n    BestOfN,\n}\n</code></pre> <p>See Architecture docs for detailed type reference.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>MARS is a five-phase multi-agent reasoning system designed to solve complex problems through exploration, verification, and iterative improvement.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Input Query                               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Phase 1: Multi-Agent Exploration                       \u2502\n\u2502  \u2022 3 agents with temperatures [0.3, 0.6, 1.0]             \u2502\n\u2502  \u2022 Parallel LLM calls                                      \u2502\n\u2502  \u2022 Initial solutions generated                            \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502   Phase 2a: Solution Aggregation      \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\u2502\n         \u2502  \u2502   MOA   \u2502  \u2502  MCTS   \u2502  \u2502  RSA   \u2502\u2502\n         \u2502  \u2502Horizontal\u2502 \u2502Vertical \u2502 \u2502Iterative\u2502\u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Phase 2b: Strategy Network           \u2502\n         \u2502  \u2022 Extract successful strategies      \u2502\n         \u2502  \u2022 Share insights across agents       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Phase 3: Verification                \u2502\n         \u2502  \u2022 Cross-agent verification           \u2502\n         \u2502  \u2022 2-pass consensus                   \u2502\n         \u2502  \u2022 Detailed feedback                  \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Phase 4: Iterative Improvement       \u2502\n         \u2502  \u2022 Target unverified solutions        \u2502\n         \u2502  \u2022 Enhance based on feedback          \u2502\n         \u2502  \u2022 Re-verify (up to 5 iterations)     \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502  Phase 5: Final Synthesis             \u2502\n         \u2502  \u2022 Majority voting                    \u2502\n         \u2502  \u2022 Best verified selection            \u2502\n         \u2502  \u2022 Synthesize if no consensus         \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                             \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                   Final Answer                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#1-coordinator-srccoordinatorrs","title":"1. Coordinator (<code>src/coordinator.rs</code>)","text":"<p>The orchestrator that manages all 5 phases and coordinates between components.</p> <ul> <li>Responsibilities:</li> <li>Phase sequencing and flow control</li> <li>Event emission for monitoring</li> <li>Error handling and recovery</li> <li>Configuration management</li> </ul>"},{"location":"architecture/overview/#2-agent-srcagentrs","title":"2. Agent (<code>src/agent.rs</code>)","text":"<p>Individual reasoning agents with temperature-based exploration.</p> <ul> <li>Responsibilities:</li> <li>Generate solutions via LLM</li> <li>Manage agent state</li> <li>Track token usage</li> <li>Support custom reasoning prompts</li> </ul>"},{"location":"architecture/overview/#3-workspace-srcworkspacers","title":"3. Workspace (<code>src/workspace.rs</code>)","text":"<p>Shared solution storage (Arc for thread-safe access). <ul> <li>Responsibilities:</li> <li>Store and retrieve solutions</li> <li>Query solutions by various criteria</li> <li>Concurrent access management</li> <li>Solution lifecycle tracking</li> </ul>"},{"location":"architecture/overview/#4-verifier-srcverifierrs","title":"4. Verifier (<code>src/verifier.rs</code>)","text":"<p>Cross-agent verification system.</p> <ul> <li>Responsibilities:</li> <li>Check solution correctness</li> <li>Generate detailed feedback</li> <li>Track verification scores</li> <li>Manage consensus threshold</li> </ul>"},{"location":"architecture/overview/#5-aggregator-srcaggregatorrs","title":"5. Aggregator (<code>src/aggregator.rs</code>)","text":"<p>Solution synthesis using specialized strategies.</p> <ul> <li>Responsibilities:</li> <li>Route to appropriate aggregation method</li> <li>Synthesize improved solutions</li> <li>Combine multiple perspectives</li> <li>Generate aggregation statistics</li> </ul>"},{"location":"architecture/overview/#6-moa-srcmoars","title":"6. MOA (<code>src/moa.rs</code>)","text":"<p>Mixture of Agents aggregation (horizontal diversity).</p> <ul> <li>Responsibilities:</li> <li>Generate diverse completions</li> <li>Critique each completion</li> <li>Synthesize final answer</li> </ul>"},{"location":"architecture/overview/#7-mcts-srcmctsrs","title":"7. MCTS (<code>src/mcts.rs</code>)","text":"<p>Monte Carlo Tree Search (vertical exploration).</p> <ul> <li>Responsibilities:</li> <li>Build reasoning tree</li> <li>Select via UCB formula</li> <li>Simulate rollouts</li> <li>Backpropagate values</li> </ul>"},{"location":"architecture/overview/#8-strategy-network-srcstrategyrs","title":"8. Strategy Network (<code>src/strategy.rs</code>)","text":"<p>Cross-agent learning system.</p> <ul> <li>Responsibilities:</li> <li>Extract strategies from solutions</li> <li>Calculate success rates</li> <li>Share insights across agents</li> <li>Generate strategy-based solutions</li> </ul>"},{"location":"architecture/overview/#9-types-srctypesrs","title":"9. Types (<code>src/types.rs</code>)","text":"<p>Core data structures and enums.</p> <ul> <li>Responsibilities:</li> <li>Define Solution, VerificationResult</li> <li>Define MarsEvent for streaming</li> <li>Define AggregationMethod enum</li> <li>Type safety across modules</li> </ul>"},{"location":"architecture/overview/#10-config-srcconfigrs","title":"10. Config (<code>src/config.rs</code>)","text":"<p>Configuration system with builder pattern.</p> <ul> <li>Responsibilities:</li> <li>Parse and validate config</li> <li>Provide sensible defaults</li> <li>Support builder pattern</li> <li>Generate phase-specific configs</li> </ul>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#solution-flow","title":"Solution Flow","text":"<pre><code>Agent 1 (temp=0.3) \u2500\u2500\u2510\nAgent 2 (temp=0.6) \u2500\u2500\u253c\u2500\u2500\u2192 Workspace \u2500\u2500\u2192 Aggregation \u2500\u2500\u2192 Verification\nAgent 3 (temp=1.0) \u2500\u2500\u2518                                         \u2193\n                                                          Improvement\n                                                                \u2193\n                                                          Final Synthesis\n</code></pre>"},{"location":"architecture/overview/#event-flow","title":"Event Flow","text":"<pre><code>Coordinator\n  \u2193 emits\nMarsEvent\n  \u251c\u2500 ExplorationStarted\n  \u251c\u2500 SolutionGenerated\n  \u251c\u2500 VerificationStarted\n  \u251c\u2500 SolutionVerified\n  \u251c\u2500 ImprovementStarted\n  \u251c\u2500 AnswerSynthesized\n  \u2514\u2500 Completed\n  \u2193\nEvent Channel (mpsc)\n  \u2193\nTUI / Monitoring\n</code></pre>"},{"location":"architecture/overview/#key-design-patterns","title":"Key Design Patterns","text":""},{"location":"architecture/overview/#1-arc-for-concurrency","title":"1. Arc for Concurrency <p>Allows lock-free reads during parallel phases:</p> <pre><code>workspace: Arc&lt;RwLock&lt;Vec&lt;Solution&gt;&gt;&gt;\n</code></pre>","text":""},{"location":"architecture/overview/#2-builder-pattern-for-configuration","title":"2. Builder Pattern for Configuration <p>Fluent API for composable configuration:</p> <pre><code>config\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_num_agents(4)\n</code></pre>","text":""},{"location":"architecture/overview/#3-trait-based-abstraction","title":"3. Trait-Based Abstraction <p>LLMProvider trait supports any model:</p> <pre><code>pub trait LLMProvider: Send + Sync {\n    async fn complete(&amp;self, prompt: &amp;str, system: Option&lt;&amp;str&gt;) -&gt; Result&lt;String&gt;;\n    // ...\n}\n</code></pre>","text":""},{"location":"architecture/overview/#4-event-streaming","title":"4. Event Streaming <p>Real-time progress tracking:</p> <pre><code>while let Some(event) = rx.recv().await {\n    match event {\n        MarsEvent::SolutionGenerated { solution_id, .. } =&gt; { /* ... */ }\n        // ...\n    }\n}\n</code></pre>","text":""},{"location":"architecture/overview/#5-error-handling","title":"5. Error Handling <p>Comprehensive error type:</p> <pre><code>pub enum MarsError {\n    AgentError(String),\n    VerificationError(String),\n    AggregationError(String),\n    // ... 10+ variants\n}\n</code></pre>","text":""},{"location":"architecture/overview/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code>MarsConfig (global settings)\n  \u251c\u2500 num_agents\n  \u251c\u2500 temperatures\n  \u251c\u2500 aggregation_method\n  \u2502   \u251c\u2500 MOA: moa_num_completions, moa_fallback_enabled\n  \u2502   \u251c\u2500 MCTS: mcts_simulation_depth, mcts_exploration_weight, etc.\n  \u2502   \u2514\u2500 RSA: aggregation_loops, aggregation_selection_size\n  \u251c\u2500 max_iterations\n  \u251c\u2500 verify settings\n  \u2514\u2500 token budgets\n\n\u2193 Phase-specific configs\n\nMCTSConfig, MoaMetadata, StrategyConfig\n</code></pre>"},{"location":"architecture/overview/#thread-safety","title":"Thread Safety","text":"<p>All shared data structures use proper synchronization:</p> <ul> <li>Arc - Solutions in workspace <li>mpsc::channel - Event distribution</li> <li>tokio::spawn - Agent parallelization</li> <li>No unsafe code - Type-safe Rust throughout</li>"},{"location":"architecture/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/overview/#phase-1-exploration","title":"Phase 1: Exploration <ul> <li>Parallelism: 3 agents in parallel</li> <li>Time: O(latency of slowest LLM call)</li> <li>Memory: O(num_agents \u00d7 max_tokens)</li> </ul>","text":""},{"location":"architecture/overview/#phase-2a-aggregation","title":"Phase 2a: Aggregation <ul> <li>MOA: O(num_completions) parallel</li> <li>MCTS: O(simulations \u00d7 depth) sequential</li> <li>RSA: O(loops \u00d7 selections) sequential</li> </ul>","text":""},{"location":"architecture/overview/#phase-3-verification","title":"Phase 3: Verification <ul> <li>Parallelism: 2 verifiers per solution in parallel</li> <li>Time: O(num_solutions / num_verifiers)</li> </ul>","text":""},{"location":"architecture/overview/#phase-4-improvement","title":"Phase 4: Improvement <ul> <li>Parallelism: Improve multiple solutions in parallel</li> <li>Time: O(max_iterations \u00d7 num_unverified / num_agents)</li> </ul>","text":""},{"location":"architecture/overview/#phase-5-synthesis","title":"Phase 5: Synthesis <ul> <li>Time: O(1) - pure selection/synthesis</li> </ul>","text":""},{"location":"architecture/overview/#extensibility-points","title":"Extensibility Points","text":""},{"location":"architecture/overview/#1-custom-llm-providers","title":"1. Custom LLM Providers <p>Implement <code>LLMProvider</code> trait:</p> <pre><code>struct MyProvider;\n\n#[async_trait]\nimpl LLMProvider for MyProvider {\n    async fn complete(&amp;self, prompt: &amp;str, system: Option&lt;&amp;str&gt;) -&gt; Result&lt;String&gt; {\n        // Your implementation\n    }\n    // ...\n}\n</code></pre>","text":""},{"location":"architecture/overview/#2-custom-aggregation-methods","title":"2. Custom Aggregation Methods <p>Add to <code>AggregationMethod</code> enum and implement in Aggregator.</p>","text":""},{"location":"architecture/overview/#3-custom-verification-strategies","title":"3. Custom Verification Strategies <p>Modify Verifier to support domain-specific checks.</p>","text":""},{"location":"architecture/overview/#4-custom-prompts","title":"4. Custom Prompts <p>Update <code>src/prompts.rs</code> with domain-specific templates.</p>","text":""},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>5-Phase Pipeline Details</li> <li>Core Components</li> <li>Configuration Guide</li> </ul>"},{"location":"architecture/pipeline/","title":"5-Phase Pipeline","text":"<p>MARS executes in 5 distinct phases to solve complex reasoning problems.</p>"},{"location":"architecture/pipeline/#phase-1-multi-agent-exploration","title":"Phase 1: Multi-Agent Exploration","text":"<p>Goal: Generate diverse initial solutions</p> <ul> <li>Spawn N agents (default: 3)</li> <li>Each agent has different temperature for diverse exploration</li> <li>Temperatures: [0.3, 0.6, 1.0]</li> <li>All agents run LLM in parallel</li> <li>Store all solutions in workspace</li> </ul> <p>Output: N solutions with varying approaches</p>"},{"location":"architecture/pipeline/#phase-2a-solution-aggregation","title":"Phase 2a: Solution Aggregation","text":"<p>Goal: Synthesize and refine solutions</p> <p>Choose one aggregation method:</p> <ul> <li>MOA: Generate diverse completions, critique, synthesize</li> <li>MCTS: Tree exploration with UCB selection and simulations</li> <li>RSA: Maintain population, iteratively refine</li> </ul> <p>Output: Enhanced solutions from aggregation</p>"},{"location":"architecture/pipeline/#phase-2b-strategy-network","title":"Phase 2b: Strategy Network","text":"<p>Goal: Share successful strategies across agents</p> <ul> <li>Extract reasoning patterns from verified solutions</li> <li>Identify what worked well</li> <li>Share insights across agents</li> <li>Generate enhanced solutions using peer strategies</li> </ul> <p>Output: Strategy-enhanced solutions</p>"},{"location":"architecture/pipeline/#phase-3-verification","title":"Phase 3: Verification","text":"<p>Goal: Verify solution correctness</p> <ul> <li>Cross-agent verification</li> <li>Each solution verified by multiple agents</li> <li>Consensus requirement: 2 consecutive \"CORRECT\" assessments</li> <li>Generate detailed feedback on weaknesses</li> <li>Score solutions (0.0-1.0)</li> </ul> <p>Output: Verified solutions with feedback</p>"},{"location":"architecture/pipeline/#phase-4-iterative-improvement","title":"Phase 4: Iterative Improvement","text":"<p>Goal: Improve unverified solutions</p> <p>Loop up to max_iterations (default: 5):</p> <ol> <li>Identify unverified solutions</li> <li>Target for enhancement based on feedback</li> <li>Generate improved versions</li> <li>Re-verify</li> <li>Stop if verified or max iterations reached</li> </ol> <p>Output: Improved solutions, some now verified</p>"},{"location":"architecture/pipeline/#phase-5-final-synthesis","title":"Phase 5: Final Synthesis","text":"<p>Goal: Select and synthesize final answer</p> <ol> <li>Try Majority Voting: If 2+ agents agree, use that answer</li> <li>Try Best Verified: If verified solutions exist, use highest-scored</li> <li>Try Synthesis: Otherwise, synthesize from top 3 solutions</li> <li>Clean up: Extract clean answer from thinking tags</li> </ol> <p>Output: Final answer with selection method</p>"},{"location":"architecture/pipeline/#phase-control-flow","title":"Phase Control Flow","text":"<pre><code>Phase 1: Exploration\n  \u2193 (3 parallel agents)\n  \u251c\u2500 Agent 1 (T=0.3): Solution A1\n  \u251c\u2500 Agent 2 (T=0.6): Solution A2\n  \u2514\u2500 Agent 3 (T=1.0): Solution A3\n\nPhase 2a: Aggregation\n  \u251c\u2500 MOA: Generate + Critique + Synthesize\n  \u251c\u2500 MCTS: Tree Search with UCB\n  \u2514\u2500 RSA: Population Refinement\n  \u2193\n  Generates N enhanced solutions\n\nPhase 2b: Strategy Network (optional)\n  \u2193\n  Extracts and shares strategies\n\nPhase 3: Verification\n  \u251c\u2500 Verify all solutions (2-pass consensus)\n  \u251c\u2500 Score: 0.0-1.0\n  \u2514\u2500 Generate feedback\n\nPhase 4: Improvement (up to 5 iterations)\n  \u251c\u2500 Target unverified\n  \u251c\u2500 Enhance based on feedback\n  \u2514\u2500 Re-verify\n\nPhase 5: Synthesis\n  \u251c\u2500 Majority voting?\n  \u251c\u2500 Best verified?\n  \u251c\u2500 Synthesize?\n  \u2514\u2500 Clean answer\n\nFinal Answer\n</code></pre>"},{"location":"architecture/pipeline/#performance-by-phase","title":"Performance by Phase","text":"Phase Time Parallelism LLM Calls 1 Fast 3 agents 3 2a (MOA) Medium Completions parallel 2N+1 2a (MCTS) Slow Sequential ~100 2a (RSA) Medium Selections parallel K*T 2b Medium Agents parallel N 3 Fast Solutions parallel 2N 4 Slow Improvements parallel 3*I 5 Instant None 0-1"},{"location":"architecture/pipeline/#configuration-by-phase","title":"Configuration by Phase","text":"<pre><code>// Phase 1 config\n.with_num_agents(3)\n.with_temperatures(vec![0.3, 0.6, 1.0])\n\n// Phase 2a config\n.with_aggregation(true)\n.with_aggregation_method(AggregationMethod::MOA)\n\n// Phase 2b config\n.with_strategy_network(true)\n\n// Phase 4 config\n.with_max_iterations(5)\n\n// Consensus config\n.with_consensus_threshold(2)\n</code></pre> <p>See Configuration Guide for details.</p>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/","title":"Plain/Background/Loading/Wait Migration","text":""},{"location":"archive/tui-migrations/plain_loading_wait_migration/#summary","title":"Summary","text":"<ul> <li>Replace all plain/background/loading/wait cell insertions and replacements in   <code>chatwidget.rs</code> with <code>HistoryDomainEvent</code> calls so <code>HistoryState</code> becomes the   source of truth for these stable cell types.</li> <li>Remove any remaining uses of <code>history_record_from_cell</code> for these families   and ensure IDs are assigned via the new domain-event helpers.</li> </ul>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/#prerequisites","title":"Prerequisites","text":"<ul> <li><code>HistoryDomainEvent</code> infrastructure landed and exported from   <code>history/state.rs</code>.</li> <li>Background system notices already routed through   <code>history_replace_with_record</code> (completed 2025-09-27).</li> </ul>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/#scope","title":"Scope","text":"<ul> <li><code>history_insert_plain_cell_with_key</code>, <code>history_push_plain_cell</code>, wait tool   completions, loading spinners, and generic system helpers (<code>push_system_cell</code>).</li> <li>Update <code>assign_history_id</code> branches that become redundant once the domain   events populate IDs.</li> <li>Extend <code>events_audit.md</code> with status notes as paths are migrated.</li> </ul>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/#deliverables","title":"Deliverables","text":"<ul> <li>New domain-event constructors for <code>PlainMessageState</code>, <code>WaitStatusState</code>,   <code>LoadingState</code>, and <code>BackgroundEventRecord</code>.</li> <li><code>chatwidget.rs</code> updated to call <code>history_state.apply_domain_event(...)</code>   (exact API name per infrastructure work).</li> <li>Unit or integration coverage that emits events and confirms <code>HistoryState</code>   stores the expected records without referencing <code>history_cells</code> directly.</li> <li>Documentation update in <code>events_audit.md</code> marking these categories as   migrated.</li> </ul>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/#status-2025-09-27","title":"Status (2025-09-27)","text":"<ul> <li>Domain-event constructors landed and <code>chatwidget.rs</code> now hydrates inserted   plain/loading/wait/background cells from <code>HistoryRecord</code> responses.</li> <li><code>events_audit.md</code> updated to reflect the new flow.</li> <li>Outstanding: add automated coverage for the hydration helpers so regressions   are caught when more cell types migrate.</li> </ul>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/#next-steps-for-agent","title":"Next Steps for Agent","text":"<ul> <li>Extend <code>chatwidget</code> tests (or add focused unit coverage) that drive   <code>HistoryDomainEvent</code> inserts for plain/loading/wait/background cells and   assert the resulting <code>HistoryState</code> records.</li> <li>Confirm there are no remaining <code>history_record_from_cell</code> call sites for   these variants; remove or document any unavoidable leftovers.</li> <li>Update this file with findings and link any follow-up TODOs uncovered during   testing.</li> </ul>"},{"location":"archive/tui-migrations/plain_loading_wait_migration/#references","title":"References","text":"<ul> <li><code>code-rs/tui/HISTORY_CELLS_PLAN.md</code> \u2013 Step 3, Event Pipeline Consolidation   Plan (Phase C \u2013 wave 1).</li> <li><code>code-rs/tui/src/chatwidget/events_audit.md</code> \u2013 mutation inventory.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/","title":"History Renderer Cache Implementation","text":""},{"location":"archive/tui-migrations/renderer_cache/#summary","title":"Summary","text":"<ul> <li>Implement the shared renderer cache described in Step 6 so per-cell layout   caches (exec/assistant/diff) can be retired once all migrations are complete.</li> <li>Provide APIs for cache lookup, invalidation, and telemetry that integrate with   the <code>HistoryDomainEvent</code> pipeline.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/#prerequisites","title":"Prerequisites","text":"<ul> <li>Domain-event migrations for waves 1 &amp; 2 complete (cells reconstruct from   <code>HistoryState</code> without mutating <code>history_cells</code>).</li> <li><code>HistoryRenderState</code> ready to accept cache invalidation signals.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/#scope","title":"Scope","text":"<ul> <li>Introduce <code>RenderedCell</code> structs, cache key types, and LRU storage inside   <code>history_render.rs</code> (or a new module) based on the design sketch in   <code>HISTORY_CELLS_PLAN.md</code>.</li> <li>Expose invalidation hooks that respond to History mutations (insert/replace/   remove) and viewport/theme changes.</li> <li>Wire ChatWidget rendering paths to consult the shared cache before invoking   cell-specific rendering logic.</li> <li>Define instrumentation (counters or tracing) to measure hit/miss rates.</li> <li>Plan removal of legacy per-cell caches once cache hit rates are validated.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/#deliverables","title":"Deliverables","text":"<ul> <li>New caching module with unit tests covering key eviction, width/theme   invalidation, and reasoning-visibility variants.</li> <li>ChatWidget rendering updated to consume cached buffers for all migrated cell   types.</li> <li>Documentation update (HISTORY_CELLS_PLAN.md Step 6) noting completion and   instructions for removing cell-local caches.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/#status-2025-09-27","title":"Status (2025-09-27)","text":"<ul> <li>Prerequisites outstanding: exec/assistant/diff domain-event migrations still   mutate cells directly, so this cache must remain on hold until those flows   emit <code>HistoryDomainEvent</code>s.</li> <li>Current work-in-progress skeletons: none.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/#next-steps-for-agent-design-spike","title":"Next Steps for Agent (Design Spike)","text":"<ul> <li>Draft a lightweight design doc or code comments outlining cache key shapes,   invalidation triggers, and integration points with <code>HistoryRenderState</code>.</li> <li>Explore adding feature-gated scaffolding (e.g., new module with TODOs) without   altering existing rendering behavior.</li> <li>Identify instrumentation needs and note open dependencies on ongoing   migrations.</li> <li>Update this file with findings and explicitly list blockers before attempting   implementation.</li> </ul>"},{"location":"archive/tui-migrations/renderer_cache/#references","title":"References","text":"<ul> <li><code>code-rs/tui/HISTORY_CELLS_PLAN.md</code> \u2013 Step 6 renderer cache design sketch.</li> <li><code>code-rs/tui/src/chatwidget/history_render.rs</code> \u2013 existing memoization hooks.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/","title":"Streaming / Exec / Assistant Migration","text":""},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#summary","title":"Summary","text":"<ul> <li>Move streaming deltas, finalized assistant messages, exec lifecycle updates,   and merged exec summaries onto the <code>HistoryDomainEvent</code> pipeline.</li> <li>Eliminate cell-side state mutations and redundant cached line buffers where   practical, while preserving the interim cache bridge called out in Step 2.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#prerequisites","title":"Prerequisites","text":"<ul> <li>Infrastructure: <code>HistoryDomainEvent</code> enums + helpers merged.</li> <li>Wave 1 (plain/background/loading/wait) migrated so supporting utilities are   battle-tested.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#scope","title":"Scope","text":"<ul> <li>Streaming handlers: <code>ensure_answer_stream_state</code>, tail updates, review-flow   paths, and final message insertions (<code>chatwidget.rs:11380-11890</code>).</li> <li>Exec lifecycle: active exec state, merged exec summaries, wait notes, and   tool completion merges (<code>chatwidget.rs:6515-7330</code>).</li> <li>Ensure <code>history_cell::ExecCell</code>, <code>StreamingContentCell</code>, and   <code>AssistantMarkdownCell</code> rebuild purely from <code>HistoryState</code> data after events.</li> <li>Update <code>assign_history_id</code> only where still required; prefer storing IDs via   the domain events.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#deliverables","title":"Deliverables","text":"<ul> <li>Domain-event variants covering exec begin/end/stream chunks, assistant stream   deltas, assistant final messages, and merged exec summaries.</li> <li>ChatWidget handlers updated to emit domain events instead of mutating   <code>history_cells</code> directly.</li> <li>Regression tests (prefer snapshot-based tests around   <code>chatwidget::tests::run_script</code>) validating that streaming/exec flows produce   equivalent UI output after the migration.</li> <li>Notes added to <code>events_audit.md</code> marking the executive/assistant entries as   completed or updated with any residual technical debt.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#status-2025-09-27","title":"Status (2025-09-27)","text":"<ul> <li>Infrastructure: <code>HistoryDomainEvent</code> enums + hydration helpers now exist and   wave 1 (plain/loading/wait/background) paths are migrated.</li> <li>Exec/streaming handlers still mutate <code>history_cells</code> and downcast cells in   place; caches must remain until the Step 6 renderer cache lands.</li> <li>No automated coverage yet exercises the domain events for these flows.</li> <li>\u2705 Exec streaming deltas now emit <code>HistoryDomainEvent::UpdateExecStream</code>, and   the resulting <code>HistoryRecord::Exec</code> is used to hydrate the running <code>ExecCell</code>.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#next-steps-for-agent","title":"Next Steps for Agent","text":"<ul> <li>Sketch domain-event variants for exec lifecycle and assistant streaming while   preserving the interim caches (do not remove them until the shared renderer   cache is live).</li> <li>Convert one representative exec path (e.g., command begin \u2192 output chunk \u2192   completion) and one assistant stream path to use the new domain events.</li> <li>Add focused regression tests via <code>run_script</code> covering the converted paths.</li> <li>Document remaining mutation spots in <code>events_audit.md</code> and update this file   with progress notes or follow-up TODOs.</li> </ul>"},{"location":"archive/tui-migrations/stream_exec_assistant_migration/#references","title":"References","text":"<ul> <li><code>code-rs/tui/HISTORY_CELLS_PLAN.md</code> \u2013 Step 2 bridge &amp; Step 3 Phase C wave 2.</li> <li><code>code-rs/tui/src/chatwidget/events_audit.md</code> \u2013 exec/stream mutation list.</li> </ul>"},{"location":"configuration/config/","title":"Configuration Guide","text":"<p>MARS provides comprehensive configuration via the fluent builder pattern.</p>"},{"location":"configuration/config/#default-configuration","title":"Default Configuration","text":"<pre><code>let config = MarsConfig::default();\n// num_agents: 3\n// temperatures: [0.3, 0.6, 1.0]\n// max_iterations: 5\n// enable_aggregation: false\n// enable_strategy_network: false\n// timeout_seconds: 300\n</code></pre>"},{"location":"configuration/config/#builder-pattern","title":"Builder Pattern","text":"<p>Configure MARS fluently:</p> <pre><code>let config = MarsConfig::new()\n    .with_num_agents(4)\n    .with_max_iterations(7)\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_debug(true);\n</code></pre>"},{"location":"configuration/config/#configuration-options","title":"Configuration Options","text":""},{"location":"configuration/config/#exploration-phase","title":"Exploration Phase","text":"<pre><code>.with_num_agents(n)                    // Number of agents (default: 3)\n.with_temperatures(vec![...])          // Temperature values (default: [0.3, 0.6, 1.0])\n</code></pre>"},{"location":"configuration/config/#aggregation-phase","title":"Aggregation Phase","text":"<pre><code>// MOA\n.with_moa_aggregation()\n.with_moa_num_completions(5)           // Default: 3\n.with_moa_fallback_enabled(true)       // Default: true\n\n// MCTS\n.with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n.with_mcts_simulation_depth(2)         // Default: 1\n.with_mcts_exploration_weight(0.3)     // Default: 0.2\n.with_mcts_num_simulations(4)          // Default: 2\n.with_mcts_num_actions(3)              // Default: 3\n\n// RSA\n.with_aggregation(true)\n.with_aggregation_population_size(8)   // Default: 6\n.with_aggregation_selection_size(4)    // Default: 3\n.with_aggregation_loops(5)             // Default: 3\n</code></pre>"},{"location":"configuration/config/#verification-phase","title":"Verification Phase","text":"<pre><code>.with_consensus_threshold(2)           // Default: 2 (2-pass consensus)\n</code></pre>"},{"location":"configuration/config/#improvement-phase","title":"Improvement Phase","text":"<pre><code>.with_max_iterations(7)                // Default: 5\n</code></pre>"},{"location":"configuration/config/#general","title":"General","text":"<pre><code>.with_debug(true)                      // Enable debug output\n.with_timeout_seconds(600)             // Request timeout (default: 300)\n\n.lightweight()                         // Quick config: fewer agents, fewer iterations\n.with_advanced_features()              // Enable aggregation + strategy network\n</code></pre>"},{"location":"configuration/config/#quick-presets","title":"Quick Presets","text":""},{"location":"configuration/config/#lightweight-mode","title":"Lightweight Mode","text":"<pre><code>MarsConfig::default().lightweight()\n// 2 agents, 2 iterations, no aggregation\n// Fast but lower quality\n</code></pre>"},{"location":"configuration/config/#advanced-mode","title":"Advanced Mode","text":"<pre><code>MarsConfig::default().with_advanced_features()\n// Enables aggregation + strategy network\n// Slower but higher quality\n</code></pre>"},{"location":"configuration/config/#moa-focus","title":"MOA Focus","text":"<pre><code>MarsConfig::new()\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_moa_num_completions(7)\n// Horizontal diversity with 7 parallel completions\n</code></pre>"},{"location":"configuration/config/#mcts-focus","title":"MCTS Focus","text":"<pre><code>MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n    .with_mcts_simulation_depth(3)\n    .with_mcts_num_simulations(10)\n// Vertical exploration with deep tree search\n</code></pre> <p>See Advanced Configuration for more options.</p>"},{"location":"dev/history-debugging/","title":"History Debugging Toolkit","text":"<p>The history view now exposes a trace channel that makes it easier to reason about ordering bugs and stranded running cells. The traces are gated behind the <code>CODEX_TRACE_HISTORY</code> environment variable so regular sessions stay silent.</p>"},{"location":"dev/history-debugging/#enabling-traces","title":"Enabling Traces","text":"<pre><code>export CODEX_TRACE_HISTORY=1\n</code></pre> <p>With the flag set the TUI emits structured <code>trace!</code> logs on the <code>codex_history</code> target and also buffers them in\u2013process for tests.</p>"},{"location":"dev/history-debugging/#manual-reproduction-flow","title":"Manual Reproduction Flow","text":"<ol> <li>Create a worktree \u2013 run <code>/branch</code> or perform any action that yields the    \u201cCreating worktree\u2026\u201d / \u201cCreated worktree.\u201d system banners. Confirm they are    adjacent.</li> <li>Start a long running tool \u2013 trigger a custom tool (for example an    MCP-backed command that waits on I/O). Leave it running.</li> <li>Restore history \u2013 use the existing \u201crestore snapshot\u201d action (or in dev    builds, <code>/history restore latest</code>). This exercises the rehydration path.</li> <li>Let the tool complete \u2013 allow the tool to finish or send its end event.</li> <li>Inspect logs \u2013 look for:</li> <li><code>restore_history_snapshot.start/done</code> pair with cell/order counts.</li> <li><code>rehydrate_tool_state.*</code> entries detailing each running tool.</li> <li><code>custom_tool_end.*</code> / <code>mcp_tool_end.*</code> / <code>web_search_end.*</code> entries showing      whether the completion was in-place or fallback.</li> <li><code>system_order_cache.reset</code> confirming the banner cache was rebuilt.    You should not see spinners left over; the banners should stay contiguous.</li> </ol>"},{"location":"dev/history-debugging/#programmatic-checks","title":"Programmatic Checks","text":"<p>In tests (or a debug REPL) you can read the buffered trace lines via <code>ChatWidget::history_debug_events()</code>. This is how the new regression tests verify that logging is wired correctly.</p>"},{"location":"dev/history-debugging/#observing-state","title":"Observing State","text":"<p>If you need a snapshot of caches at runtime, evaluate the helper methods:</p> <ul> <li><code>ChatWidget::assert_history_consistent()</code> (in tests) validates matched   lengths between <code>history_cells</code>, <code>cell_order_seq</code>, and <code>system_cell_by_id</code>.</li> <li>The trace messages include the counts of custom/web/wait/kill maps after   each rehydrate or finalize phase.</li> </ul> <p>These hooks should give enough signal to detect regressions without adding noise to day-to-day sessions.</p>"},{"location":"dev/history-invariants/","title":"History &amp; Tool-State Invariants","text":"<p>This document records the assumptions the TUI relies on when rendering history cells, managing running tools, and replaying snapshots. Whenever we touch the chat history or tool bookkeeping, we should keep these invariant checks in mind.</p>"},{"location":"dev/history-invariants/#running-tool-maps","title":"Running Tool Maps","text":"<ul> <li>The <code>running_custom_tools</code>, <code>running_web_search</code>, <code>running_wait_tools</code>, and   <code>running_kill_tools</code> maps must always reflect a visible running cell in   <code>history_cells</code>. After a snapshot restore or any rebuild, we must rehydrate   them from <code>history_state</code> before handling new events.</li> <li>Rehydrate helpers may synthesize lookup entries, but they must not invent   new history rows\u2014only map call IDs to existing running cells.</li> </ul>"},{"location":"dev/history-invariants/#finalization-rules","title":"Finalization Rules","text":"<ul> <li>When finalizing (during TaskComplete or an explicit end event), only clear a   running map entry if we actually replaced/collapsed the associated running   cell. Entries that could not be resolved must remain so the next pass can   retry.</li> <li>End events are idempotent. Receiving the same <code>*ToolCallEnd</code> twice may   re-run finalization, but it must not create duplicates or resurrect a   spinner.</li> <li>Spinner rows should always be collapsed: after finalization there should be   at most one completed tool row per call ID.</li> </ul>"},{"location":"dev/history-invariants/#ordering","title":"Ordering","text":"<ul> <li>Preserve the <code>OrderKey</code> whenever a running row is replaced in place. If a   fallback insertion is needed, reuse the incoming order key and remove the   stale running row first.</li> <li>System banners (\u201cCreating worktree\u2026\u201d, \u201cCreated worktree.\u201d, etc.) should stay   contiguous even across restores and tool completions. If we need to recompute   caches, do so atomically and keep the order vector in sync.</li> <li>After any rebuild or rehydrate, we must ensure <code>history_cells.len() ==   cell_order_seq.len() == history_cell_ids.len()</code>, and every stored system   index must remain in bounds.</li> </ul>"},{"location":"dev/history-invariants/#fallback-heuristics","title":"Fallback Heuristics","text":"<ul> <li>Prefer matching running cells by <code>HistoryId</code> first (when available), or by   <code>call_id</code>. If neither yields a match, scan for a generic running tool cell as   a last resort before inserting a replacement.</li> <li>Any fallback insertion must inherit the original order and remove the   spinner row so we do not present two entries for the same call.</li> </ul> <p>Keeping these invariants explicit makes it easier to reason about history refactors and reduces the risk of reintroducing ordering bugs.</p>"},{"location":"features/auto-drive/architecture/","title":"Auto-Drive Architecture","text":"<p>Auto-Drive is the autonomous task orchestration system at the heart of kode. It coordinates multi-step workflows, manages AI agent interactions, handles intelligent error recovery, and automates decision-making. This document provides a comprehensive technical overview of how Auto-Drive is architected and implemented.</p>"},{"location":"features/auto-drive/architecture/#system-overview","title":"System Overview","text":"<p>Auto-Drive enables users to specify complex goals and let the system autonomously execute them through iterative planning, execution, and refinement cycles. The system consists of several interconnected components that work together to:</p> <ol> <li>Understand user intent through natural language goal descriptions</li> <li>Plan next steps using the Claude AI coordinator</li> <li>Execute tasks by generating and running CLI actions</li> <li>Manage state across multiple turns with conversation history</li> <li>Handle failures gracefully with intelligent retry logic</li> <li>Optimize context by compacting conversation history to fit token limits</li> <li>Validate progress through automated quality assurance</li> </ol>"},{"location":"features/auto-drive/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph UI[\"TUI Layer\"]\n        GoalPanel[\"Goal Entry Panel\"]\n        CoordView[\"Coordinator View\"]\n        HistoryCell[\"History Cell\"]\n        CardUI[\"Auto-Drive Cards\"]\n    end\n\n    subgraph Core[\"Auto-Drive Core\"]\n        Controller[\"Controller&lt;br/&gt;State Machine\"]\n        Coordinator[\"Auto Coordinator&lt;br/&gt;Planning Engine\"]\n        History[\"Auto-Drive History&lt;br/&gt;Conversation Management\"]\n        Compact[\"Auto Compact&lt;br/&gt;Context Optimization\"]\n        Router[\"Coordinator Router&lt;br/&gt;Message Routing\"]\n        Retry[\"Retry Manager&lt;br/&gt;Resilience\"]\n    end\n\n    subgraph Support[\"Support Systems\"]\n        Metrics[\"Session Metrics&lt;br/&gt;Telemetry\"]\n        Faults[\"Fault Injection&lt;br/&gt;Dev/Testing\"]\n        QA[\"Auto QA&lt;br/&gt;Quality Assurance\"]\n    end\n\n    subgraph Execution[\"Execution Layer\"]\n        CLIGen[\"CLI Action Generator\"]\n        AgentExec[\"Agent Executor\"]\n    end\n\n    GoalPanel --&gt;|User Input| Controller\n    CoordView --&gt;|Display Status| Controller\n    HistoryCell --&gt;|Update State| Controller\n    CardUI --&gt;|Render Cards| Controller\n\n    Controller --&gt;|Manage Phases| Coordinator\n    Controller --&gt;|Query State| History\n\n    Coordinator --&gt;|Read/Write| History\n    Coordinator --&gt;|Plan Next Steps| Router\n    Coordinator --&gt;|Generate Actions| CLIGen\n    Coordinator --&gt;|Estimate Tokens| Metrics\n\n    History --&gt;|Compact When Needed| Compact\n    History --&gt;|Update| Metrics\n\n    Router --&gt;|Route Messages| Coordinator\n\n    CLIGen --&gt;|Execute| AgentExec\n    AgentExec --&gt;|On Error| Retry\n    Retry --&gt;|Backoff Loop| Coordinator\n\n    Coordinator --&gt;|Monitor Quality| QA\n    QA --&gt;|Feedback| Controller</code></pre>"},{"location":"features/auto-drive/architecture/#core-components","title":"Core Components","text":""},{"location":"features/auto-drive/architecture/#1-controller-controllerrs-32kb","title":"1. Controller (<code>controller.rs</code> - 32KB)","text":"<p>The Controller is the state machine that manages the Auto-Drive lifecycle. It orchestrates transitions between different execution phases and coordinates communications between the TUI and the coordinator.</p>"},{"location":"features/auto-drive/architecture/#autorunphase-state-machine","title":"AutoRunPhase State Machine","text":"<p>Auto-Drive executes through a well-defined state machine with 9 distinct phases:</p> <pre><code>stateDiagram-v2\n    [*] --&gt; Idle\n\n    Idle --&gt; AwaitingGoalEntry: User Opens Auto-Drive\n\n    AwaitingGoalEntry --&gt; AwaitingGoalEntry: Editing Goal\n    AwaitingGoalEntry --&gt; Launching: Goal Submitted\n\n    Launching --&gt; Active: Ready to Execute\n\n    Active --&gt; AwaitingCoordinator: Submit Plan\n    Active --&gt; PausedManual: User Pauses\n\n    AwaitingCoordinator --&gt; AwaitingDiagnostics: Approved\n    AwaitingCoordinator --&gt; PausedManual: Reject/Edit\n\n    AwaitingDiagnostics --&gt; AwaitingReview: Model Response Ready\n\n    AwaitingReview --&gt; Active: Continue Immediately\n    AwaitingReview --&gt; Active: Continue in 10s\n    AwaitingReview --&gt; Active: Continue in 60s\n    AwaitingReview --&gt; PausedManual: Continue Manually\n\n    PausedManual --&gt; Active: Resume\n    PausedManual --&gt; AwaitingCoordinator: Submit Plan\n\n    Active --&gt; TransientRecovery: Transient Error\n    TransientRecovery --&gt; Active: Backoff+Retry\n    TransientRecovery --&gt; PausedManual: Max Retries\n\n    Active --&gt; Idle: Complete Successfully\n    Active --&gt; Idle: User Stops\n    TransientRecovery --&gt; Idle: User Stops</code></pre> <p>Phase Descriptions:</p> Phase Purpose User Actions Next Phase Idle No active run Open goal panel AwaitingGoalEntry AwaitingGoalEntry Goal input panel visible Edit/submit goal Launching Launching Preparing first turn (automatic) Active Active Normal execution Submit plan, pause, stop AwaitingCoordinator, PausedManual AwaitingCoordinator Waiting for user approval Approve, reject, edit AwaitingDiagnostics, PausedManual AwaitingDiagnostics Model generating response (automatic) AwaitingReview AwaitingReview Result ready for action Choose continue option Active, PausedManual PausedManual User-paused execution Resume, edit, submit Active, AwaitingCoordinator TransientRecovery Recovering from error (automatic backoff) Active, PausedManual"},{"location":"features/auto-drive/architecture/#continue-modes","title":"Continue Modes","text":"<p>After the model generates a plan, users choose how to proceed:</p> <ul> <li>Immediate: Execute the next action immediately</li> <li>10 seconds: Wait 10 seconds then auto-execute</li> <li>60 seconds: Wait 60 seconds then auto-execute</li> <li>Manual: Wait for explicit user approval</li> </ul>"},{"location":"features/auto-drive/architecture/#restart-logic","title":"Restart Logic","text":"<p>When transient failures occur:</p> <ol> <li>First failure: Immediate 1-second backoff</li> <li>Subsequent failures: Exponential backoff (2s, 4s, 8s, 16s, 32s)</li> <li>Max 6 retry attempts before requiring manual intervention</li> <li>User can abort or retry during recovery phase</li> </ol>"},{"location":"features/auto-drive/architecture/#2-auto-coordinator-auto_coordinatorrs-87kb","title":"2. Auto Coordinator (<code>auto_coordinator.rs</code> - 87KB)","text":"<p>The Coordinator is the planning engine powered by Claude AI (GPT-5). It's the \"brain\" of Auto-Drive, responsible for understanding goals, making decisions, and generating the next actions.</p>"},{"location":"features/auto-drive/architecture/#architecture","title":"Architecture","text":"<pre><code>sequenceDiagram\n    participant User\n    participant Controller\n    participant Coordinator\n    participant History\n    participant CLIGen\n    participant Agent\n\n    User-&gt;&gt;Controller: Submit Goal\n    Controller-&gt;&gt;Coordinator: Initialize with Goal\n    Coordinator-&gt;&gt;History: Create Turn Entry\n\n    loop Until Goal Complete or User Stops\n        Coordinator-&gt;&gt;Coordinator: Analyze Current State\n        Coordinator-&gt;&gt;Claude: \"Given history, what's next?\"\n        Claude--&gt;&gt;Coordinator: Plan + CLI Action\n        Coordinator-&gt;&gt;History: Log Coordinator Turn\n        Coordinator-&gt;&gt;Controller: Display Plan for Approval\n\n        Controller-&gt;&gt;User: Show Plan\n        User-&gt;&gt;Controller: Approve/Reject/Edit\n\n        alt User Approves\n            Controller-&gt;&gt;Coordinator: Proceed\n            Coordinator-&gt;&gt;CLIGen: Generate CLI Action\n            CLIGen-&gt;&gt;Agent: Execute Action\n            Agent--&gt;&gt;CLIGen: Result (stdout/stderr/status)\n            CLIGen--&gt;&gt;Coordinator: Action Result\n            Coordinator-&gt;&gt;History: Log Result\n            Coordinator-&gt;&gt;Coordinator: Decide Next Step\n        else User Rejects or Edits\n            Coordinator-&gt;&gt;Coordinator: Incorporate Feedback\n        end\n    end</code></pre>"},{"location":"features/auto-drive/architecture/#key-responsibilities","title":"Key Responsibilities","text":"<ol> <li>State Analysis: Reads conversation history and current context</li> <li>Planning: Calls Claude to decide next steps</li> <li>Decision Making: Routes user input and generates appropriate actions</li> <li>Context Management: Monitors token usage and requests compaction when needed</li> <li>Error Recovery: Implements retry logic with exponential backoff</li> <li>Rate Limiting: Handles Claude API rate limits gracefully</li> <li>Metrics Collection: Tracks token usage and session statistics</li> </ol>"},{"location":"features/auto-drive/architecture/#claude-integration","title":"Claude Integration","text":"<p>The Coordinator maintains an ongoing conversation with Claude that includes:</p> <ul> <li>System Prompt: Describes Auto-Drive's capabilities and constraints</li> <li>Conversation History: Complete turn-by-turn transcript</li> <li>Current Context: Recent actions and their results</li> <li>User Feedback: User-provided corrections or guidance</li> </ul> <p>Each turn:</p> <pre><code>User: Goal description or feedback\nCoordinator: Sends history to Claude\nClaude: Analyzes and returns:\n  - Thinking/reasoning\n  - Next CLI action to execute\n  - Expected outcome\nCoordinator: Logs response and displays to user\n</code></pre>"},{"location":"features/auto-drive/architecture/#events-emitted","title":"Events Emitted","text":"<p>The Coordinator publishes events for TUI consumption:</p> <ul> <li>Decision: Planning output ready for approval</li> <li>Thinking: Coordinator is analyzing (used for UI feedback)</li> <li>UserReply: User message incorporated</li> <li>TokenMetrics: Token usage update</li> <li>CompactedHistory: History was compacted</li> </ul>"},{"location":"features/auto-drive/architecture/#3-auto-drive-history-auto_drive_historyrs-17kb","title":"3. Auto-Drive History (<code>auto_drive_history.rs</code> - 17KB)","text":"<p>Manages the complete conversation transcript between the user, coordinator, and Claude. This is the single source of truth for the Auto-Drive session.</p>"},{"location":"features/auto-drive/architecture/#responsibilities","title":"Responsibilities","text":"<ol> <li>Message Storage: Maintains chronological transcript</li> <li>Deduplication: Prevents duplicate messages</li> <li>Turn Management: Tracks multi-turn conversations</li> <li>Metrics Aggregation: Collects session-level statistics</li> <li>History Retrieval: Provides formatted history for coordinator</li> </ol>"},{"location":"features/auto-drive/architecture/#data-structure","title":"Data Structure","text":"<pre><code>Session\n\u251c\u2500\u2500 Turn 1\n\u2502   \u251c\u2500\u2500 User Input\n\u2502   \u251c\u2500\u2500 Coordinator Response\n\u2502   \u251c\u2500\u2500 Action Executed\n\u2502   \u2514\u2500\u2500 Result\n\u251c\u2500\u2500 Turn 2\n\u2502   \u251c\u2500\u2500 User Input\n\u2502   \u251c\u2500\u2500 Coordinator Response\n\u2502   \u251c\u2500\u2500 Action Executed\n\u2502   \u2514\u2500\u2500 Result\n\u2514\u2500\u2500 Session Metrics\n    \u251c\u2500\u2500 Total Tokens\n    \u251c\u2500\u2500 Total Turns\n    \u2514\u2500\u2500 Elapsed Time\n</code></pre>"},{"location":"features/auto-drive/architecture/#4-auto-compact-auto_compactrs-17kb","title":"4. Auto Compact (<code>auto_compact.rs</code> - 17KB)","text":"<p>Manages token context by intelligently compacting conversation history when approaching token limits. This allows Auto-Drive to handle long-running tasks without losing important context.</p>"},{"location":"features/auto-drive/architecture/#compaction-strategy","title":"Compaction Strategy","text":"<pre><code>graph LR\n    A[\"Full History&lt;br/&gt;Recent Turns\"]\n    B[\"Identify Checkpoints&lt;br/&gt;Logical Breakpoints\"]\n    C[\"Create Summary&lt;br/&gt;of Earlier Turns\"]\n    D[\"Keep Recent Turns&lt;br/&gt;Verbatim\"]\n    E[\"Compacted History&lt;br/&gt;Summary + Recent\"]\n\n    A --&gt; B\n    B --&gt; C\n    B --&gt; D\n    C --&gt; E\n    D --&gt; E</code></pre>"},{"location":"features/auto-drive/architecture/#algorithm","title":"Algorithm","text":"<ol> <li>Token Estimation: Counts tokens (4 bytes per token approximation)</li> <li>Checkpoint Identification: Finds logical breakpoints in history</li> <li>Summary Generation: Creates concise summary of earlier turns</li> <li>Preservation: Keeps recent N turns in full detail</li> <li>Validation: Ensures compacted history fits within token limit</li> </ol>"},{"location":"features/auto-drive/architecture/#token-budgeting","title":"Token Budgeting","text":"<ul> <li>Input token limit: 200,000 tokens (Claude model limit)</li> <li>Compaction triggers at 75% capacity</li> <li>Recent 10 turns always kept verbatim</li> <li>Summary preserves key decisions and outcomes</li> </ul>"},{"location":"features/auto-drive/architecture/#5-coordinator-router-coordinator_routerrs-4kb","title":"5. Coordinator Router (<code>coordinator_router.rs</code> - 4KB)","text":"<p>Routes user messages to appropriate handlers based on content. Enables quick responses to common queries without calling Claude.</p>"},{"location":"features/auto-drive/architecture/#routing-rules","title":"Routing Rules","text":"Pattern Handler Purpose <code>status</code>, <code>where am i</code> Status Query Current phase and summary <code>plan</code>, <code>next</code> Plan Request Display current plan <code>stop</code>, <code>abort</code> Stop Command Graceful termination <code>other</code> Coordinator Full Claude reasoning <p>This provides sub-100ms responses for common queries while delegating complex reasoning to Claude.</p>"},{"location":"features/auto-drive/architecture/#6-retry-manager-retryrs-5kb","title":"6. Retry Manager (<code>retry.rs</code> - 5KB)","text":"<p>Implements resilient error handling with exponential backoff for transient failures.</p>"},{"location":"features/auto-drive/architecture/#backoff-strategy","title":"Backoff Strategy","text":"<pre><code>Attempt 1: Fail \u2192 Wait 1s\nAttempt 2: Fail \u2192 Wait 2s\nAttempt 3: Fail \u2192 Wait 4s\nAttempt 4: Fail \u2192 Wait 8s\nAttempt 5: Fail \u2192 Wait 16s\nAttempt 6: Fail \u2192 Wait 32s\nAttempt 7: Manual intervention required\n</code></pre>"},{"location":"features/auto-drive/architecture/#features","title":"Features","text":"<ul> <li>Jitter: Random variation (\u00b110%) to prevent thundering herd</li> <li>Cancellation: User can abort retry loop at any time</li> <li>Status Callbacks: TUI receives updates during backoff</li> <li>Rate Limit Handling: Special handling for 429 responses</li> </ul>"},{"location":"features/auto-drive/architecture/#7-session-metrics-session_metricsrs-3kb","title":"7. Session Metrics (<code>session_metrics.rs</code> - 3KB)","text":"<p>Collects telemetry about Auto-Drive sessions.</p>"},{"location":"features/auto-drive/architecture/#tracked-metrics","title":"Tracked Metrics","text":"<ul> <li>Token Usage: Input, output, and total tokens per turn</li> <li>Turn Count: Number of coordinator interactions</li> <li>Elapsed Time: Wall-clock session duration</li> <li>Error Count: Failures and retries</li> <li>Completion Status: Successful completion vs. abortion</li> </ul>"},{"location":"features/auto-drive/architecture/#tui-integration","title":"TUI Integration","text":"<p>Auto-Drive integrates with the TUI through several specialized components:</p>"},{"location":"features/auto-drive/architecture/#goal-entry-panel","title":"Goal Entry Panel","text":"<p>Located in the chat input area, allows users to: - Describe their goal in natural language - Review previously executed goals - Edit and re-run goals</p>"},{"location":"features/auto-drive/architecture/#coordinator-view-bottom-pane","title":"Coordinator View (Bottom Pane)","text":"<p>Displays: - Current Goal: What Auto-Drive is trying to accomplish - Status: Current phase and elapsed time - Pending Plan: Next action awaiting approval - Countdown Timer: For auto-submission modes - Action Buttons: Approve, reject, edit, continue options</p>"},{"location":"features/auto-drive/architecture/#history-cells","title":"History Cells","text":"<p>Special history cells showing: - Card Title: Goal description - Status Badge: Running/Paused/Failed/Complete - Action Log: Timestamp-annotated actions - Completion Animation: Celebration effect when done - Gradient Effects: Visual progression through phases</p>"},{"location":"features/auto-drive/architecture/#auto-drive-cards","title":"Auto-Drive Cards","text":"<p>Higher-level card abstraction managing: - Card Lifecycle: Creation, updates, completion - Session Tracking: Multiple concurrent Auto-Drive sessions - Card Persistence: Long-term session tracking</p>"},{"location":"features/auto-drive/architecture/#auto-qa-orchestration","title":"Auto QA Orchestration","text":"<p>Auto-Drive includes automated quality assurance to validate execution quality and catch issues before user review.</p>"},{"location":"features/auto-drive/architecture/#observer-lifecycle","title":"Observer Lifecycle","text":"<pre><code>graph LR\n    A[\"Bootstrap&lt;br/&gt;Passive Monitoring\"]\n    B[\"Cadence Checks&lt;br/&gt;Periodic Validation\"]\n    C[\"Cross-Check&lt;br/&gt;Before Completion\"]\n    D[\"Complete\"]\n\n    A --&gt;|Every N turns| B\n    B --&gt;|Final Step| C\n    C --&gt;|Pass| D\n    C --&gt;|Fail| A</code></pre>"},{"location":"features/auto-drive/architecture/#bootstrap-phase","title":"Bootstrap Phase","text":"<ul> <li>Duration: First coordinator turn</li> <li>Tool Access: Read-only operations only</li> <li>Purpose: Initialize observer, gather baseline state</li> <li>Example: <code>ls</code>, <code>pwd</code>, <code>git status</code> (safe inspection)</li> </ul>"},{"location":"features/auto-drive/architecture/#cadence-phase","title":"Cadence Phase","text":"<ul> <li>Frequency: Every <code>CODE_QA_CADENCE</code> turns (default: 3)</li> <li>Tool Access: Limited safe operations</li> <li>Purpose: Periodic health checks</li> <li>Example: Verify build still compiles, tests still pass</li> </ul>"},{"location":"features/auto-drive/architecture/#cross-check-phase","title":"Cross-Check Phase","text":"<ul> <li>Trigger: Before marking completion</li> <li>Tool Access: Full audit capabilities</li> <li>Purpose: Final validation before user review</li> <li>Example: Full test suite execution, comprehensive validation</li> </ul>"},{"location":"features/auto-drive/architecture/#configuration","title":"Configuration","text":"<pre><code>CODE_QA_CADENCE=3              # Turns between observer checks\nCODE_QA_REVIEW_COOLDOWN_TURNS=1 # Turns before review request\n</code></pre>"},{"location":"features/auto-drive/architecture/#error-handling","title":"Error Handling","text":"<p>Auto-Drive distinguishes between error types:</p>"},{"location":"features/auto-drive/architecture/#transient-errors","title":"Transient Errors","text":"<p>Temporarily retried with exponential backoff: - Network timeouts - Rate limiting (429) - Temporary service unavailability - Resource contention</p> <p>Recovery: Automatic retry with backoff, max 6 attempts</p>"},{"location":"features/auto-drive/architecture/#user-recoverable-errors","title":"User-Recoverable Errors","text":"<p>Require user feedback: - Invalid goal specification - Ambiguous instructions - Contradictory requirements</p> <p>Recovery: Controller transitions to <code>PausedManual</code> for editing</p>"},{"location":"features/auto-drive/architecture/#terminal-errors","title":"Terminal Errors","text":"<p>Require manual intervention: - Invalid credentials - Permissions issues - Configuration problems</p> <p>Recovery: User must edit and restart</p>"},{"location":"features/auto-drive/architecture/#token-management","title":"Token Management","text":"<p>Auto-Drive implements sophisticated token management:</p>"},{"location":"features/auto-drive/architecture/#token-tracking","title":"Token Tracking","text":"<ul> <li>Each turn logs input and output tokens</li> <li>Running total maintained in session metrics</li> <li>4-byte approximation for quick estimation</li> </ul>"},{"location":"features/auto-drive/architecture/#compaction-triggers","title":"Compaction Triggers","text":"<ol> <li>Threshold: Token usage &gt; 75% of limit</li> <li>Initiated By: Coordinator detects threshold</li> <li>Compaction Process: Summarizes older turns, keeps recent turns</li> <li>Result: History fits within token limits for next turn</li> </ol>"},{"location":"features/auto-drive/architecture/#context-window-strategy","title":"Context Window Strategy","text":"<ul> <li>Preserve: Recent 10 turns in full detail</li> <li>Summarize: Older turns compressed to key decisions</li> <li>Drop: Very old turns if necessary for fit</li> </ul>"},{"location":"features/auto-drive/architecture/#development-and-testing","title":"Development and Testing","text":""},{"location":"features/auto-drive/architecture/#fault-injection-faultsrs","title":"Fault Injection (<code>faults.rs</code>)","text":"<p>Behind <code>dev-faults</code> feature flag, allows testing error scenarios:</p> <pre><code>#[cfg(feature = \"dev-faults\")]\nsimulate_network_error()?;\n\n#[cfg(feature = \"dev-faults\")]\nsimulate_rate_limit()?;\n</code></pre>"},{"location":"features/auto-drive/architecture/#local-testing","title":"Local Testing","text":"<p>Test Auto-Drive with realistic scenarios:</p> <ol> <li>Long-running goal (triggers compaction)</li> <li>Network failure (triggers retry)</li> <li>User interruption (tests state cleanup)</li> <li>Complex multi-step workflow (tests planning)</li> </ol>"},{"location":"features/auto-drive/architecture/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"features/auto-drive/architecture/#latency","title":"Latency","text":"<ul> <li>Goal Submission to First Plan: 2-5 seconds (Claude latency)</li> <li>Status Query: &lt;100ms (routed query)</li> <li>Phase Transitions: &lt;50ms (state machine)</li> <li>History Compaction: 500-1000ms for large histories</li> </ul>"},{"location":"features/auto-drive/architecture/#token-efficiency","title":"Token Efficiency","text":"<ul> <li>Typical Goal: 5,000-10,000 tokens for complete session</li> <li>Complex Goal: 50,000-100,000 tokens possible</li> <li>Compaction Overhead: ~10% token reduction per compaction</li> </ul>"},{"location":"features/auto-drive/architecture/#scalability","title":"Scalability","text":"<ul> <li>Concurrent Sessions: Limited only by TUI capacity</li> <li>Turn Depth: Unlimited (compaction handles arbitrarily long sessions)</li> <li>History Size: Multi-megabyte histories supported</li> </ul>"},{"location":"features/auto-drive/architecture/#integration-points","title":"Integration Points","text":"<p>Auto-Drive integrates with:</p> <ol> <li>Claude AI: For planning and decision-making</li> <li>Agent Executor: For running generated CLI actions</li> <li>TUI: For user interaction and visualization</li> <li>History System: For conversation persistence</li> <li>Configuration System: For user preferences</li> </ol>"},{"location":"features/auto-drive/architecture/#future-enhancements","title":"Future Enhancements","text":""},{"location":"features/auto-drive/architecture/#core-improvements","title":"Core Improvements","text":"<p>Potential improvements to Auto-Drive architecture:</p> <ol> <li>Distributed Execution: Run multiple parallel action sequences</li> <li>Learning: Refine decision-making based on outcomes</li> <li>Tool Integration: Custom tool definitions for domain-specific tasks</li> <li>Observability: Enhanced tracing and debugging tools</li> <li>Multi-Agent Coordination: Collaborative agent workflows</li> </ol>"},{"location":"features/auto-drive/architecture/#pubsub-systems-for-multi-agent-communication","title":"Pub/Sub Systems for Multi-Agent Communication","text":"<p>A critical enabler for distributed execution and multi-agent coordination is a publish-subscribe (pub/sub) messaging system that decouples sub-agents from each other, enabling asynchronous, scalable communication patterns.</p> <p>Current Limitation: Today, agents work in isolation and communicate through point-to-point callbacks and shared state. This limits parallelism and makes it difficult for agents to learn from each other's work.</p> <p>Pub/Sub Solution: Introducing a message-oriented event bus would enable:</p> <ul> <li>Agent Progress Broadcasting: Sub-agents can publish their status, discoveries, and intermediate results to a shared event bus, with multiple subscribers (TUI, metrics collector, distributed tracer, other agents) consuming relevant events</li> <li>Strategy Sharing: In MARS-style multi-agent systems, agents can broadcast discovered strategies or heuristics, allowing peer agents to incorporate successful approaches without blocking</li> <li>Work Coordination: Agents can announce what they're working on, enabling collaborative systems to avoid duplicate effort and distribute work more intelligently</li> <li>Result Aggregation: Multiple agents working on parallel tasks can publish results asynchronously, with an aggregator subscribing to compile final outputs</li> </ul> <p>Evolution Path: The system can evolve in two phases:</p> <ul> <li> <p>Phase 1 - Local Coordination: For single-node setups, lightweight internal Rust-based event buses using tokio's <code>broadcast</code> channels or libraries like <code>async-channel</code> provide low-latency, in-process pub/sub without external dependencies</p> </li> <li> <p>Phase 2 - Distributed Scaling: As Auto-Drive scales to multi-node deployments, external message brokers enable geography-distributed agent coordination:</p> </li> <li>Apache Kafka: Highly scalable distributed event streaming platform ideal for high-throughput data pipelines and durable event logs</li> <li>RabbitMQ: Feature-rich message broker with flexible routing patterns, persistence guarantees, and support for multiple messaging protocols</li> <li>Apache Pulsar: Modern distributed platform developed by Yahoo!, known for high scalability and multi-tenant capabilities</li> <li>ActiveMQ: Lightweight open-source broker with broad protocol support, suitable for smaller deployments</li> <li>Redis: Fast, simple pub/sub suitable for low-latency, non-persistent messaging in lightweight scenarios</li> </ul> <p>A pub/sub layer would also enhance observability by funneling all agent activities through typed events, enabling comprehensive tracing, metrics collection, and audit logging for complex multi-agent workflows.</p> <p>Complementary Pattern: The pub/sub architecture can be combined with the actor model for even more powerful concurrent system design. In the actor model, each agent is an independent actor that receives messages asynchronously and processes them in isolation, avoiding shared mutable state. Tokio's async runtime is well-suited for implementing actor patterns in Rust.</p>"},{"location":"features/auto-drive/architecture/#references","title":"References","text":""},{"location":"features/auto-drive/architecture/#internal-documentation","title":"Internal Documentation","text":"<ul> <li>State Inventory - Detailed state management</li> <li>Phase Migration - Migration guide</li> <li>Auto QA Documentation - Quality assurance details</li> <li>Source: <code>code-rs/code-auto-drive-core/</code></li> </ul>"},{"location":"features/auto-drive/architecture/#further-reading-on-concurrent-patterns","title":"Further Reading on Concurrent Patterns","text":"<ul> <li>Actors with Tokio - Comprehensive guide to implementing the actor pattern with Tokio for concurrent, message-driven architecture</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Rust 1.75+ - Install Rust</li> <li>Cargo - Comes with Rust</li> <li>An LLM API - OpenAI, Anthropic, or any provider with MARS support</li> </ul>"},{"location":"getting-started/installation/#adding-to-your-project","title":"Adding to Your Project","text":""},{"location":"getting-started/installation/#via-cargotoml","title":"Via Cargo.toml","text":"<p>Add MARS to your <code>Cargo.toml</code>:</p> <pre><code>[dependencies]\ncode-mars = { path = \"../code-rs/code-mars\" }\ncode-core = { path = \"../code-rs/code-core\" }  # For ModelClient\ntokio = { version = \"1\", features = [\"full\"] }\n</code></pre> <p>Or if using from a published crate:</p> <pre><code>[dependencies]\ncode-mars = \"0.1.0\"\ncode-core = \"0.1.0\"\ntokio = { version = \"1\", features = [\"full\"] }\n</code></pre>"},{"location":"getting-started/installation/#quick-setup","title":"Quick Setup","text":""},{"location":"getting-started/installation/#1-create-a-rust-project","title":"1. Create a Rust Project","text":"<pre><code>cargo new my_mars_project\ncd my_mars_project\n</code></pre>"},{"location":"getting-started/installation/#2-add-dependencies","title":"2. Add Dependencies","text":"<pre><code>cargo add code-mars code-core tokio --features tokio/full\n</code></pre>"},{"location":"getting-started/installation/#3-create-your-first-program","title":"3. Create Your First Program","text":"<p>Create <code>src/main.rs</code>:</p> <pre><code>use code_mars::{MarsCoordinator, MarsConfig};\nuse code_core::ModelClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Create a ModelClient (uses OPENAI_API_KEY env var)\n    let client = ModelClient::default();\n\n    // Create MARS coordinator with default settings\n    let config = MarsConfig::default();\n    let mut coordinator = MarsCoordinator::new(config);\n\n    // Ask a question\n    let query = \"Solve: If 3x + 5 = 20, what is x?\";\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    println!(\"Question: {}\", query);\n    println!(\"Answer: {}\", result.answer);\n    println!(\"Method: {:?}\", result.selection_method);\n    println!(\"Tokens used: {}\", result.total_tokens);\n\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/installation/#4-set-your-api-key","title":"4. Set Your API Key","text":"<pre><code>export OPENAI_API_KEY=sk-...\n</code></pre>"},{"location":"getting-started/installation/#5-run-it","title":"5. Run It","text":"<pre><code>cargo run --release\n</code></pre>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#openai-api","title":"OpenAI API","text":"<pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"getting-started/installation/#multi-model-setup-litellm-rs","title":"Multi-Model Setup (litellm-rs)","text":"<pre><code># Configure provider routing\nexport MARS_PROVIDER_ROUTING='{\n  \"default_provider\": \"openai\",\n  \"providers\": {\n    \"openai\": {\n      \"model\": \"gpt-4\",\n      \"base_url\": \"https://api.openai.com/v1\"\n    }\n  }\n}'\n</code></pre>"},{"location":"getting-started/installation/#debug-mode","title":"Debug Mode","text":"<pre><code>export MARS_DEBUG=1\n</code></pre>"},{"location":"getting-started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>Run the test suite to verify everything is working:</p> <pre><code>cd code-rs/code-mars\ncargo test --lib\ncargo test --test '*'\n</code></pre> <p>Expected output: <pre><code>test result: ok. 43 passed; 0 failed; 2 ignored\nrunning 29 tests\ntest result: ok. 29 passed; 0 failed\n</code></pre></p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#error-could-not-find-crate-code_mars","title":"<code>Error: could not find crate 'code_mars'</code>","text":"<p>Make sure you're using the correct path or published version in <code>Cargo.toml</code>.</p>"},{"location":"getting-started/installation/#error-openai_api_key-not-set","title":"<code>Error: OPENAI_API_KEY not set</code>","text":"<p>Set your API key: <pre><code>export OPENAI_API_KEY=sk-...\n</code></pre></p>"},{"location":"getting-started/installation/#timeout-errors","title":"<code>Timeout errors</code>","text":"<p>Increase the timeout in configuration: <pre><code>let config = MarsConfig::default();\n// Timeout is 300 seconds by default\n</code></pre></p>"},{"location":"getting-started/installation/#out-of-memory","title":"<code>Out of memory</code>","text":"<p>MARS can use significant memory with many iterations and solutions. Reduce: - <code>max_iterations</code> (default: 5) - <code>num_agents</code> (default: 3) - Use <code>lightweight()</code> config</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide</li> <li>Basic Usage Examples</li> <li>Advanced Configuration</li> </ul>"},{"location":"getting-started/installation/#support","title":"Support","text":"<p>For issues or questions: 1. Check Troubleshooting above 2. Review API Reference 3. Look at Examples 4. Open an issue on GitHub</p>"},{"location":"getting-started/overview/","title":"MARS Overview","text":"<p>MARS (Multi-Agent Reasoning System) is a Rust framework for advanced reasoning with large language models. It coordinates multiple agents to explore different solution paths, verify answers, and synthesize optimal results.</p>"},{"location":"getting-started/overview/#core-concept","title":"Core Concept","text":"<p>Instead of asking an LLM a question once, MARS:</p> <ol> <li>Explores - Spawn multiple agents with different exploration parameters (temperatures)</li> <li>Aggregates - Combine insights using specialized strategies (MOA for diversity, MCTS for depth)</li> <li>Verifies - Cross-check solutions with multiple agents to catch errors</li> <li>Improves - Target weak solutions for enhancement based on verification feedback</li> <li>Synthesizes - Select the best answer via majority voting, consensus, or synthesis</li> </ol>"},{"location":"getting-started/overview/#why-multiple-agents","title":"Why Multiple Agents?","text":"<p>Language models exhibit stochasticity - the same prompt produces different outputs at different temperatures:</p> <ul> <li>Low Temperature (0.3) - Focused, deterministic responses</li> <li>Medium Temperature (0.6) - Balanced exploration</li> <li>High Temperature (1.0) - Creative, diverse responses</li> </ul> <p>By running all three in parallel, MARS captures different reasoning paths and can pick the best one.</p>"},{"location":"getting-started/overview/#the-5-phase-pipeline","title":"The 5-Phase Pipeline","text":""},{"location":"getting-started/overview/#phase-1-multi-agent-exploration","title":"Phase 1: Multi-Agent Exploration","text":"<ul> <li>Spawn N agents (default: 3)</li> <li>Each explores with different temperature</li> <li>Generate initial solutions in parallel</li> <li>Store all solutions in shared workspace</li> </ul>"},{"location":"getting-started/overview/#phase-2a-solution-aggregation","title":"Phase 2a: Solution Aggregation","text":"<p>Choose a strategy to combine and refine solutions: - MOA - Generate completions, critique, synthesize (horizontal diversity) - MCTS - Tree exploration with UCB selection (vertical depth) - RSA - Population-based iterative refinement</p>"},{"location":"getting-started/overview/#phase-2b-strategy-network","title":"Phase 2b: Strategy Network","text":"<ul> <li>Extract successful strategies from solutions</li> <li>Identify patterns that work well</li> <li>Share insights across agents</li> <li>Generate enhanced solutions</li> </ul>"},{"location":"getting-started/overview/#phase-3-verification","title":"Phase 3: Verification","text":"<ul> <li>Cross-agent verification of all solutions</li> <li>Consensus requirement: 2 consecutive \"CORRECT\" assessments</li> <li>Detailed feedback on strengths and weaknesses</li> </ul>"},{"location":"getting-started/overview/#phase-4-iterative-improvement","title":"Phase 4: Iterative Improvement","text":"<ul> <li>Target unverified solutions</li> <li>Use feedback to guide improvements</li> <li>Re-verify improved solutions</li> <li>Repeat up to max iterations (default: 5)</li> </ul>"},{"location":"getting-started/overview/#phase-5-final-synthesis","title":"Phase 5: Final Synthesis","text":"<ul> <li>Majority Voting - If 2+ agents agree, use that answer</li> <li>Best Verified - Otherwise use highest-scoring verified solution</li> <li>Synthesis - If no consensus, synthesize from top 3</li> <li>Clean up answer and return result</li> </ul>"},{"location":"getting-started/overview/#when-to-use-mars","title":"When to Use MARS","text":""},{"location":"getting-started/overview/#good-for","title":"\u2705 Good For:","text":"<ul> <li>Complex reasoning tasks (math, logic, proofs)</li> <li>Multi-step problem solving</li> <li>Tasks where accuracy &gt; speed</li> <li>Problems with multiple solution approaches</li> <li>Tasks requiring verification and improvement</li> </ul>"},{"location":"getting-started/overview/#not-ideal-for","title":"\u274c Not Ideal For:","text":"<ul> <li>Simple factual questions (single-agent LLM sufficient)</li> <li>Real-time applications (MARS takes 2-5 minutes)</li> <li>Tasks with fixed output format (verification becomes complex)</li> <li>Cost-sensitive applications (MARS uses 50K-200K tokens)</li> </ul>"},{"location":"getting-started/overview/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>Input Query\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phase 1: Exploration           \u2502\n\u2502  3 Agents \u00d7 3 Temperatures      \u2502\n\u2502  (temp: 0.3, 0.6, 1.0)          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193 (3 solutions)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phase 2a: Aggregation          \u2502\n\u2502  MOA / MCTS / RSA               \u2502\n\u2502  (synthesize improved solutions) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193 (6+ solutions)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phase 2b: Strategy Network     \u2502\n\u2502  (extract and share strategies) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phase 3: Verification          \u2502\n\u2502  Cross-agent verification       \u2502\n\u2502  (2-pass consensus)             \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phase 4: Improvement           \u2502\n\u2502  (max 5 iterations)             \u2502\n\u2502  Re-verify improved solutions   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Phase 5: Final Synthesis       \u2502\n\u2502  \u2022 Majority voting              \u2502\n\u2502  \u2022 Best verified                \u2502\n\u2502  \u2022 Synthesized answer           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \u2193\nFinal Answer\n</code></pre>"},{"location":"getting-started/overview/#key-metrics","title":"Key Metrics","text":"Metric Value Test Coverage 43 unit + 29 integration tests Code Size ~3,500 LOC Typical Runtime 2-5 minutes Token Usage 50K-200K per task Temperature Variants 3 (0.3, 0.6, 1.0) Max Iterations 5 (improvement phase) Verification Threshold 2-pass consensus"},{"location":"getting-started/overview/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature MARS Ensemble Tree-of-Thought Single-Agent Multiple agents \u2705 \u2705 \u274c \u274c Temperature diversity \u2705 \u2705 \u274c \u274c Verification \u2705 \u274c \u274c \u274c Iterative improvement \u2705 \u274c \u2705 \u274c Specialized aggregation \u2705 \u274c \u2705 \u274c Strategy sharing \u2705 \u274c \u274c \u274c Production ready \u2705 \u2705 \u274c \u2705"},{"location":"getting-started/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Installation - Set up MARS in your project</li> <li>Quick Start - Run your first example</li> <li>Architecture Details - Deep dive into the system</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get MARS up and running in 5 minutes.</p>"},{"location":"getting-started/quick-start/#step-1-install","title":"Step 1: Install","text":"<pre><code># Clone the repo\ngit clone https://github.com/GeorgePearse/code.git\ncd code/code-rs/code-mars\n\n# Verify installation\ncargo test --lib\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-set-api-key","title":"Step 2: Set API Key","text":"<pre><code>export OPENAI_API_KEY=sk-your-key-here\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-create-your-first-program","title":"Step 3: Create Your First Program","text":"<p>Create <code>examples/solve_math.rs</code>:</p> <pre><code>use code_mars::{MarsCoordinator, MarsConfig};\nuse code_core::ModelClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // Create client\n    let client = ModelClient::default();\n\n    // Create MARS with default settings\n    let config = MarsConfig::default();\n    let mut coordinator = MarsCoordinator::new(config);\n\n    // Solve a problem\n    let query = \"What is the square root of 144?\";\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    println!(\"\u2713 Answer: {}\", result.answer);\n    Ok(())\n}\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-run-it","title":"Step 4: Run It","text":"<pre><code>cargo run --example solve_math\n</code></pre>"},{"location":"getting-started/quick-start/#common-examples","title":"Common Examples","text":""},{"location":"getting-started/quick-start/#example-1-basic-question","title":"Example 1: Basic Question","text":"<pre><code>let query = \"What is 25 * 4?\";\nlet result = coordinator.run_with_client(query, &amp;client).await?;\nprintln!(\"Answer: {}\", result.answer);\n</code></pre>"},{"location":"getting-started/quick-start/#example-2-with-advanced-features","title":"Example 2: With Advanced Features","text":"<pre><code>let config = MarsConfig::default()\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_num_agents(4)\n    .with_max_iterations(7);\n\nlet mut coordinator = MarsCoordinator::new(config);\nlet result = coordinator.run_with_client(query, &amp;client).await?;\n</code></pre>"},{"location":"getting-started/quick-start/#example-3-using-mcts-for-deep-reasoning","title":"Example 3: Using MCTS for Deep Reasoning","text":"<pre><code>use code_mars::types::AggregationMethod;\n\nlet config = MarsConfig::new()\n    .with_advanced_features()\n    .with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n    .with_mcts_simulation_depth(2)\n    .with_mcts_exploration_weight(0.3);\n\nlet mut coordinator = MarsCoordinator::new(config);\nlet result = coordinator.run_with_client(query, &amp;client).await?;\n</code></pre>"},{"location":"getting-started/quick-start/#example-4-lightweight-mode-faster","title":"Example 4: Lightweight Mode (Faster)","text":"<pre><code>let config = MarsConfig::default().lightweight();\nlet mut coordinator = MarsCoordinator::new(config);\nlet result = coordinator.run_with_client(query, &amp;client).await?;\n</code></pre>"},{"location":"getting-started/quick-start/#example-5-debug-output","title":"Example 5: Debug Output","text":"<pre><code>let config = MarsConfig::default()\n    .with_debug(true);\n\nlet mut coordinator = MarsCoordinator::new(config);\n// Will print detailed logs\nlet result = coordinator.run_with_client(query, &amp;client).await?;\n</code></pre>"},{"location":"getting-started/quick-start/#understanding-the-output","title":"Understanding the Output","text":"<pre><code>let result = coordinator.run_with_client(query, &amp;client).await?;\n\n// Access the result\nprintln!(\"Answer: {}\", result.answer);                      // Final answer\nprintln!(\"Method: {:?}\", result.selection_method);          // How it was selected\nprintln!(\"Tokens: {}\", result.total_tokens);                // Total tokens used\nprintln!(\"Iterations: {}\", result.iterations);              // Improvement iterations\nprintln!(\"Solution ID: {}\", result.final_solution_id);      // Which solution won\nprintln!(\"All solutions: {}\", result.all_solutions.len());  // Total generated\n</code></pre>"},{"location":"getting-started/quick-start/#configuration-cheat-sheet","title":"Configuration Cheat Sheet","text":""},{"location":"getting-started/quick-start/#temperature-control","title":"Temperature Control","text":"<pre><code>.with_temperatures(vec![0.2, 0.5, 0.9])  // Custom temps\n</code></pre>"},{"location":"getting-started/quick-start/#agent-count","title":"Agent Count","text":"<pre><code>.with_num_agents(4)  // Default: 3\n</code></pre>"},{"location":"getting-started/quick-start/#aggregation","title":"Aggregation","text":"<pre><code>.with_moa_aggregation()                        // Horizontal diversity\n.with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)  // Vertical depth\n</code></pre>"},{"location":"getting-started/quick-start/#iteration-control","title":"Iteration Control","text":"<pre><code>.with_max_iterations(10)  // Default: 5\n</code></pre>"},{"location":"getting-started/quick-start/#token-budget","title":"Token Budget","text":"<pre><code>.get_token_budget(false)  // For complex reasoning: 64K\n.get_token_budget(true)   // For lightweight: 4K\n</code></pre>"},{"location":"getting-started/quick-start/#debug","title":"Debug","text":"<pre><code>.with_debug(true)\n</code></pre>"},{"location":"getting-started/quick-start/#monitoring-progress","title":"Monitoring Progress","text":"<p>Use event streaming to track MARS progress:</p> <pre><code>use tokio::sync::mpsc;\nuse code_mars::types::MarsEvent;\n\nlet (tx, mut rx) = mpsc::channel::&lt;MarsEvent&gt;(100);\n\n// Run MARS in background\ntokio::spawn(async move {\n    // coordinator.run_with_events(query, &amp;client, tx).await\n});\n\n// Listen to events\nwhile let Some(event) = rx.recv().await {\n    match event {\n        MarsEvent::ExplorationStarted { num_agents } =&gt; {\n            println!(\"Starting with {} agents\", num_agents);\n        }\n        MarsEvent::SolutionGenerated { solution_id, .. } =&gt; {\n            println!(\"Generated solution: {}\", solution_id);\n        }\n        MarsEvent::SolutionVerified { solution_id, is_correct, .. } =&gt; {\n            println!(\"Verified {}: {}\", solution_id, is_correct);\n        }\n        MarsEvent::AnswerSynthesized { answer } =&gt; {\n            println!(\"Final answer: {}\", answer);\n        }\n        _ =&gt; {}\n    }\n}\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ol> <li>Run more examples - Check <code>code-rs/code-mars/examples/</code></li> <li>Learn configuration - Read Configuration Guide</li> <li>Understand architecture - See Architecture Overview</li> <li>Explore aggregation methods - Learn MOA, MCTS</li> <li>API reference - Check Core Types</li> </ol>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/quick-start/#getting-timeout-errors","title":"Getting Timeout Errors?","text":"<pre><code>let config = MarsConfig::default()\n    .with_timeout_seconds(600);  // Increase timeout\n</code></pre>"},{"location":"getting-started/quick-start/#token-usage-too-high","title":"Token Usage Too High?","text":"<pre><code>let config = MarsConfig::default()\n    .lightweight();  // Use fewer agents and iterations\n</code></pre>"},{"location":"getting-started/quick-start/#need-custom-llm-provider","title":"Need Custom LLM Provider?","text":"<pre><code>// Implement LLMProvider trait\nstruct MyCustomProvider;\n\n#[async_trait::async_trait]\nimpl LLMProvider for MyCustomProvider {\n    async fn complete(&amp;self, prompt: &amp;str, system: Option&lt;&amp;str&gt;) -&gt; Result&lt;String&gt; {\n        // Your implementation\n    }\n    // ... implement other methods\n}\n\nlet result = coordinator.run_with_provider(query, &amp;my_provider).await?;\n</code></pre>"},{"location":"getting-started/quick-start/#common-tasks","title":"Common Tasks","text":""},{"location":"getting-started/quick-start/#verify-a-math-solution","title":"Verify a Math Solution","text":"<pre><code>let config = MarsConfig::default()\n    .with_num_agents(4)\n    .with_consensus_threshold(3);\nlet query = \"Is 2^10 = 1024? Prove it.\";\n</code></pre>"},{"location":"getting-started/quick-start/#generate-code","title":"Generate Code","text":"<pre><code>let config = MarsConfig::default()\n    .with_moa_aggregation()\n    .with_moa_num_completions(5);\nlet query = \"Write a Rust function that checks if a number is prime\";\n</code></pre>"},{"location":"getting-started/quick-start/#complex-logic-problem","title":"Complex Logic Problem","text":"<pre><code>let config = MarsConfig::default()\n    .with_aggregation_method(AggregationMethod::MonteCarloTreeSearch)\n    .with_advanced_features();\nlet query = \"Solve this logic puzzle: ...\";\n</code></pre>"},{"location":"getting-started/quick-start/#full-example-program","title":"Full Example Program","text":"<pre><code>use code_mars::{MarsCoordinator, MarsConfig};\nuse code_core::ModelClient;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // 1. Create client\n    let client = ModelClient::default();\n\n    // 2. Configure MARS\n    let config = MarsConfig::default()\n        .with_advanced_features()\n        .with_num_agents(3)\n        .with_max_iterations(5)\n        .with_debug(true);\n\n    // 3. Create coordinator\n    let mut coordinator = MarsCoordinator::new(config);\n\n    // 4. Ask question\n    let query = \"What is the derivative of x^3?\";\n\n    // 5. Get answer\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    // 6. Display results\n    println!(\"Question: {}\", query);\n    println!(\"Answer: {}\", result.answer);\n    println!(\"Selected via: {:?}\", result.selection_method);\n    println!(\"Tokens used: {}\", result.total_tokens);\n    println!(\"Iterations: {}\", result.iterations);\n\n    Ok(())\n}\n</code></pre> <p>Ready to go deeper? Continue with Configuration Guide or Architecture Details.</p>"},{"location":"maintenance/prompt-architecture/","title":"Prompt Architecture Overview","text":"<p>Updated: 2025-10-05</p> <p>This document captures the current prompt surface area for <code>code-rs</code>, notes where each prompt is consumed, and flags any fork-specific guidance that must be preserved when we sync with upstream Codex.</p>"},{"location":"maintenance/prompt-architecture/#prompt-inventory","title":"Prompt Inventory","text":"Category File Runtime Usage Notes Base instructions <code>code-rs/core/prompt.md</code> Loaded by default for non\u2013GPT\u20115 models Mirrors upstream, kept in sync during merges. Base instructions <code>code-rs/core/gpt_5_code_prompt.md</code> Selected when GPT\u20115/Codex family is active Upstream content; differences tracked in <code>UPSTREAM_PARITY_CHECKLIST.md</code>. Fork overlay <code>code-rs/core/prompt_coder.md</code> Appended after base prompt in fork builds Holds browser/tooling UX guidance and approve/sandbox policies. Review flows <code>code-rs/core/review_prompt.md</code> Used by review executor Shared with upstream; lightweight customisations allowed. Compact summaries <code>code-rs/core/templates/compact/prompt.md</code> Generated during conversation compaction Template strings only; keep deterministic for snapshots. Onboarding <code>code-rs/tui/prompt_for_init_command.md</code> <code>/init</code> command and AGENTS.md bootstrap Fork-only instructions for building agent manifests. Operational playbooks <code>prompts/MERGE.md</code> Release &amp; upstream merge checklist Human-facing; live in repo root for quick reference. Operational playbooks <code>prompts/TRIAGE.md</code> Issue triage workflow Human-facing; fork-specific wording."},{"location":"maintenance/prompt-architecture/#layering-strategy","title":"Layering Strategy","text":"<ol> <li>Base prompt is selected by model family (<code>prompt.md</code> or    <code>gpt_5_code_prompt.md</code>).</li> <li>Fork overlay (<code>prompt_coder.md</code>) is appended to inject tool UX, sandbox    policy, and parallel-agent expectations. All edits here must remain fork    scoped.</li> <li>Scenario prompts (review, compact, onboarding) are loaded as-needed.</li> </ol> <p>This layering lets upstream change base instructions without colliding with our fork guidance. During merges, diff base prompts first, then verify overlay still reads correctly.</p>"},{"location":"maintenance/prompt-architecture/#maintenance-guidelines","title":"Maintenance Guidelines","text":"<ul> <li>When upstream updates a base prompt, record the delta in   <code>UPSTREAM_PARITY_CHECKLIST.md</code> and regenerate local snapshots if wording   changes.</li> <li>Fork-specific copy belongs in <code>prompt_coder.md</code> or the operational playbooks;   avoid patching upstream files unless we intend to upstream the change.</li> <li>Avoid introducing new prompt files without documenting them here; stale   prompts should move to <code>docs/archive/</code> so the active surface stays small.</li> <li>Tests that rely on prompt text should treat strings as data fixtures to avoid   tying unit tests to copy changes.</li> </ul>"},{"location":"maintenance/prompt-architecture/#open-follow-ups","title":"Open Follow-ups","text":"<ul> <li>Evaluate whether onboarding instructions in   <code>code-rs/tui/prompt_for_init_command.md</code> can be slimmed down now that the   <code>/init</code> command defaults to the fork UX.</li> <li>For new agents or tooling flows, extend <code>prompt_coder.md</code> rather than forking   additional prompt variants unless we need per-feature toggles.</li> </ul>"},{"location":"maintenance/upstream-diff/","title":"Upstream Tracking and Diff Pipeline","text":"<p>This document describes the process for tracking differences between the fork (<code>code-rs</code>) and upstream (<code>codex-rs</code>), identifying critical changes, and planning merges.</p>"},{"location":"maintenance/upstream-diff/#overview","title":"Overview","text":"<p>The fork maintains parallel crate trees: - <code>codex-rs/</code> - Upstream baseline (synced from <code>https://github.com/openai/codex.git</code>) - <code>code-rs/</code> - Fork implementation with enhancements</p> <p>The tracking pipeline helps identify: 1. Structural differences - Which crates have diverged 2. Critical changes - Changes affecting prompts, API surface, or executor 3. Merge decisions - Historical log of what was adopted/adapted/preserved</p>"},{"location":"maintenance/upstream-diff/#pipeline-scripts","title":"Pipeline Scripts","text":"<p>All scripts are located in <code>scripts/upstream-merge/</code>:</p>"},{"location":"maintenance/upstream-diff/#1-diff-cratessh-structural-diff","title":"1. <code>diff-crates.sh</code> - Structural Diff","text":"<p>Compares codex-rs vs code-rs on a per-crate basis.</p> <p>Usage: <pre><code># Compare a single crate\n./scripts/upstream-merge/diff-crates.sh core\n\n# Compare all shared crates\n./scripts/upstream-merge/diff-crates.sh --all\n\n# Generate summary report\n./scripts/upstream-merge/diff-crates.sh --summary\n</code></pre></p> <p>Output: - Individual diff files: <code>.github/auto/upstream-diffs/&lt;crate&gt;.diff</code> - Summary report: <code>.github/auto/upstream-diffs/SUMMARY.md</code></p> <p>What it tracks: - Line-by-line differences between matching crates (after normalizing branding like <code>code-*</code> \u2192 <code>codex-*</code> and ignoring <code>.DS_Store</code> noise) - Added/removed lines count - Files present in one tree but not the other</p>"},{"location":"maintenance/upstream-diff/#2-highlight-critical-changessh-critical-change-detection","title":"2. <code>highlight-critical-changes.sh</code> - Critical Change Detection","text":"<p>Analyzes diffs to highlight changes in sensitive subsystems.</p> <p>Usage: <pre><code># Analyze a specific crate\n./scripts/upstream-merge/highlight-critical-changes.sh core\n\n# Analyze all crates\n./scripts/upstream-merge/highlight-critical-changes.sh --all\n</code></pre></p> <p>Output: - Per-crate critical changes: <code>.github/auto/upstream-diffs/critical-changes/&lt;crate&gt;-critical.md</code> - Summary: <code>.github/auto/upstream-diffs/critical-changes/CRITICAL-SUMMARY.md</code></p> <p>What it tracks: - Prompts: Changes to <code>prompts/*.md</code>, <code>openai_tools.rs</code>, <code>agent_tool.rs</code> - API Surface: Changes to <code>protocol.rs</code>, <code>app-server-protocol</code>, <code>mcp-types</code> - Executor: Changes to <code>codex.rs</code>, <code>exec</code>, <code>apply-patch</code>, <code>acp.rs</code> - Config: Changes to config types and behavior toggles - Function signature changes - Enum/struct definition changes - Protocol message changes (EventMsg, Op variants)</p>"},{"location":"maintenance/upstream-diff/#3-log-mergesh-merge-activity-logger","title":"3. <code>log-merge.sh</code> - Merge Activity Logger","text":"<p>Tracks merge decisions and resolutions for future reference.</p> <p>Usage: <pre><code># Start a new merge\n./scripts/upstream-merge/log-merge.sh init upstream/main\n\n# Add notes during merge\n./scripts/upstream-merge/log-merge.sh note conflict \"core/src/codex.rs has protocol changes\"\n\n# Log decisions\n./scripts/upstream-merge/log-merge.sh decision core preserve \"Keep fork's QueueUserInput support\"\n./scripts/upstream-merge/log-merge.sh decision exec adopt \"Upstream exec improvements compatible\"\n\n# Finalize when done\n./scripts/upstream-merge/log-merge.sh finalize\n\n# View historical logs\n./scripts/upstream-merge/log-merge.sh summary\n</code></pre></p> <p>Output: - Timestamped log: <code>docs/maintenance/upstream-merge-logs/merge-YYYYMMDD.md</code></p> <p>What it tracks: - Upstream ref and commit being merged - Fork state at merge start - Critical changes identified - Decisions made (adopt/adapt/preserve) with rationale - Post-merge checklist status</p>"},{"location":"maintenance/upstream-diff/#4-verifysh-post-merge-verification","title":"4. <code>verify.sh</code> - Post-Merge Verification","text":"<p>Validates that the fork still builds and maintains critical functionality after a merge.</p> <p>Usage: <pre><code>./scripts/upstream-merge/verify.sh\n</code></pre></p> <p>What it verifies: - Build passes (<code>build-fast.sh</code>) - Core API surface compiles - Static guards (browser/agent tools still registered) - Version handling intact - Branding consistency</p>"},{"location":"maintenance/upstream-diff/#monthly-cadence","title":"Monthly Cadence","text":"<p>We follow a two-phase cadence. Use the first Monday for lightweight scans; reserve the second Monday for merge planning if changes warrant action.</p>"},{"location":"maintenance/upstream-diff/#first-monday-quick-diff","title":"First Monday \u2014 Quick Diff","text":"<pre><code># Fetch latest upstream\ngit fetch upstream\n\n# Check for new commits\ngit log HEAD..upstream/main --oneline\n\n# Generate structural diff\n./scripts/upstream-merge/diff-crates.sh --all\n\n# Review high-level summary\ncat .github/auto/upstream-diffs/SUMMARY.md\n</code></pre> <p>Goal: understand upstream churn. If nothing critical changed, stop here.</p>"},{"location":"maintenance/upstream-diff/#second-monday-merge-planning-conditional","title":"Second Monday \u2014 Merge Planning (Conditional)","text":"<pre><code># Highlight critical changes\n./scripts/upstream-merge/highlight-critical-changes.sh --all\n\n# Review critical summary\ncat .github/auto/upstream-diffs/critical-changes/CRITICAL-SUMMARY.md\n\n# Initialize merge log when planning an integration\n./scripts/upstream-merge/log-merge.sh init upstream/main\n</code></pre> <p>Goal: categorize changes (adopt/adapt/preserve) and document decisions.</p>"},{"location":"maintenance/upstream-diff/#typical-workflow","title":"Typical Workflow","text":"<pre><code># 1. Initialize merge log\n./scripts/upstream-merge/log-merge.sh init upstream/main\n\n# 2. Review diffs\ncat .github/auto/upstream-diffs/SUMMARY.md\ncat .github/auto/upstream-diffs/critical-changes/CRITICAL-SUMMARY.md\n\n# 3. For each critical crate, make decisions\n#    Actions: adopt (take upstream), adapt (merge changes), preserve (keep fork)\n./scripts/upstream-merge/log-merge.sh decision core adapt \"Merge ACP support while preserving QueueUserInput\"\n./scripts/upstream-merge/log-merge.sh decision protocol adapt \"Add new ACP variants, keep existing Op types\"\n./scripts/upstream-merge/log-merge.sh decision tui preserve \"Fork-only Rust TUI, ignore upstream TS changes\"\n\n# 4. Apply changes crate-by-crate\n#    (Manual or scripted - depends on complexity)\n\n# 5. Verify after each crate\n./scripts/upstream-merge/verify.sh\n\n# 6. Finalize and document\n./scripts/upstream-merge/log-merge.sh finalize\ngit add .\ngit commit -m \"chore(codex-rs): sync with upstream main\n\nSee docs/maintenance/upstream-merge-logs/merge-YYYYMMDD.md\"\n</code></pre>"},{"location":"maintenance/upstream-diff/#integration-strategies","title":"Integration Strategies","text":"<p>Adopt (take upstream as-is): - Upstream change is a strict improvement - No fork-specific functionality conflicts - Example: Performance improvements, bug fixes in non-critical paths</p> <p>Adapt (merge both): - Upstream adds new capability, fork has extensions - Combine both sets of features - Example: Upstream adds ACP events, fork keeps existing event types</p> <p>Preserve (keep fork): - Upstream removes/changes functionality fork depends on - Fork behavior is intentionally different - Example: Rust TUI vs TypeScript TUI, fork-specific tools</p>"},{"location":"maintenance/upstream-diff/#critical-subsystems","title":"Critical Subsystems","text":"<p>Pay special attention to changes in:</p>"},{"location":"maintenance/upstream-diff/#prompts-and-tools","title":"Prompts and Tools","text":"<ul> <li><code>prompts/*.md</code> - System prompts</li> <li><code>codex-rs/core/src/openai_tools.rs</code> - Tool definitions</li> <li><code>codex-rs/core/src/agent_tool.rs</code> - Agent-specific tools</li> </ul> <p>Changes here affect LLM behavior and available capabilities.</p>"},{"location":"maintenance/upstream-diff/#protocol-and-api","title":"Protocol and API","text":"<ul> <li><code>codex-rs/core/src/protocol.rs</code> - Core event types</li> <li><code>codex-rs/app-server-protocol/</code> - Frontend communication</li> <li><code>codex-rs/mcp-types/</code> - MCP protocol definitions</li> </ul> <p>Changes here affect client compatibility and event flow.</p>"},{"location":"maintenance/upstream-diff/#executor","title":"Executor","text":"<ul> <li><code>codex-rs/core/src/codex.rs</code> - Main execution loop</li> <li><code>codex-rs/exec/src/</code> - Command execution</li> <li><code>codex-rs/apply-patch/src/</code> - File modification</li> <li><code>codex-rs/core/src/acp.rs</code> - Agent Client Protocol bridge</li> </ul> <p>Changes here affect how tools execute and side effects are managed.</p>"},{"location":"maintenance/upstream-diff/#configuration","title":"Configuration","text":"<ul> <li><code>codex-rs/core/src/config.rs</code> - Config loading</li> <li><code>codex-rs/core/src/config_types.rs</code> - Config schema</li> </ul> <p>Changes here affect default behavior and user-facing options.</p>"},{"location":"maintenance/upstream-diff/#parity-tracking","title":"Parity Tracking","text":"<p>After each merge, update <code>docs/code-crate-parity-tracker.md</code>: - Note which crates moved closer to parity - Update \"Ready to delete code-*?\" column - Add notes about remaining divergence</p> <p>Goal: Eventually delete <code>code-rs/*</code> crates that match upstream, consolidating to single implementation.</p>"},{"location":"maintenance/upstream-diff/#references","title":"References","text":"<ul> <li>Upstream ACP Integration: <code>docs/upstream-acp-merge.md</code> - Detailed notes on integrating Agent Client Protocol</li> <li>Parity Tracker: <code>docs/code-crate-parity-tracker.md</code> - Per-crate divergence status</li> <li>Merge Prompts: <code>prompts/MERGE.md</code> - Guidance for resolving conflicts</li> <li>Triage Prompts: <code>prompts/TRIAGE.md</code> - Categorizing upstream changes</li> </ul>"},{"location":"maintenance/upstream-diff/#automation-opportunities","title":"Automation Opportunities","text":"<p>Future enhancements to pipeline: - Auto-detect upstream releases via GitHub API - Generate merge preview PRs automatically - Classify changes by semantic impact (breaking/compatible/internal) - Extract commit messages for changelog integration - Cross-reference with fork's issue tracker</p>"},{"location":"maintenance/upstream-diff/#troubleshooting","title":"Troubleshooting","text":"<p>Diff shows everything changed: - Check line endings (CRLF vs LF) - Verify both trees are clean (<code>git status</code>) - Ensure upstream remote is correctly configured</p> <p>Critical changes not detected: - Update patterns in <code>highlight-critical-changes.sh</code> - Check that diff files exist in <code>.github/auto/upstream-diffs/</code></p> <p>Verify script fails: - Review <code>.github/auto/VERIFY.json</code> for specific failures - Check logs in <code>.github/auto/VERIFY_*.log</code> - Ensure all fork-specific guards are still valid</p> <p>Merge log not found: - Logs are in <code>docs/maintenance/upstream-merge-logs/</code> - Initialize with <code>log-merge.sh init</code> before logging notes/decisions</p>"},{"location":"maintenance/upstream-diff/#maintenance","title":"Maintenance","text":"<p>Update these scripts when: - New critical files/patterns emerge (update <code>CRITICAL_PATTERNS</code> in <code>highlight-critical-changes.sh</code>) - New crates are added to either tree (update <code>SHARED_CRATES</code> in <code>diff-crates.sh</code>) - New verification guards needed (update <code>verify.sh</code>) - Merge workflow evolves (update this doc and <code>log-merge.sh</code>)</p> <p>Document created: 2025-10-05 Last updated: 2025-10-05</p>"},{"location":"migration/legacy-tests-retirement/","title":"Legacy Tests Retirement Playbook","text":""},{"location":"migration/legacy-tests-retirement/#executive-summary","title":"Executive Summary","text":"<p>This playbook provides an actionable plan to safely retire legacy upstream tests from <code>codex-rs/</code> after confirming equivalent or superior coverage exists in the <code>code-rs/</code> fork. The goal is to reduce maintenance burden while preserving test quality and preventing regressions.</p> <p>Current Status (2025-10-05): - Total test files in <code>codex-rs/</code>: 106 - Total test files in <code>code-rs/</code>: 99 - Legacy-only tests identified: 9 files - Fork-only tests (new coverage): 3 files</p>"},{"location":"migration/legacy-tests-retirement/#test-coverage-analysis","title":"Test Coverage Analysis","text":""},{"location":"migration/legacy-tests-retirement/#upstream-only-tests-candidates-for-retirement","title":"Upstream-Only Tests (Candidates for Retirement)","text":"<p>These tests exist in <code>codex-rs/</code> but not in <code>code-rs/</code>:</p>"},{"location":"migration/legacy-tests-retirement/#core-package-tests-codex-rscoretestssuite","title":"Core Package Tests (<code>codex-rs/core/tests/suite/</code>)","text":"Test File Coverage Area Code-rs Replacement Gap Status <code>model_tools.rs</code> Model tool selection, SSE tool identifiers \u274c No replacement COVERAGE GAP - Critical <code>read_file.rs</code> File reading tool functionality \u274c No replacement COVERAGE GAP - High <code>tool_harness.rs</code> Tool execution framework \u274c No replacement COVERAGE GAP - High <code>tools.rs</code> General tool integration tests \u274c No replacement COVERAGE GAP - High <code>unified_exec.rs</code> Unified executor stdin/timeout tests \u274c No replacement COVERAGE GAP - Critical <code>view_image.rs</code> Image viewing tool tests \u274c No replacement COVERAGE GAP - Medium"},{"location":"migration/legacy-tests-retirement/#tui-package-tests-codex-rstuitestssuite","title":"TUI Package Tests (<code>codex-rs/tui/tests/suite/</code>)","text":"Test File Coverage Area Code-rs Replacement Gap Status <code>status_indicator.rs</code> Status widget rendering \u274c No focused assertions yet COVERAGE GAP - Medium <code>vt100_history.rs</code> VT100 history rendering, wrapping, ANSI \u274c vt100 harness not ported COVERAGE GAP - High <code>vt100_live_commit.rs</code> VT100 live commit rendering \u274c vt100 harness not ported COVERAGE GAP - Medium"},{"location":"migration/legacy-tests-retirement/#fork-only-tests-current-coverage","title":"Fork-Only Tests (Current Coverage)","text":"<p>The fork currently adds a single smoke suite:</p> Test File Package Coverage Area Notes <code>tui/tests/ui_smoke.rs</code> tui CLI flag defaults, streaming events, approvals Requires <code>chatwidget::smoke_helpers</code>; fails without <code>#[cfg(test)]</code> exports"},{"location":"migration/legacy-tests-retirement/#identical-tests-safe-to-keep-in-code-rs","title":"Identical Tests (Safe to Keep in code-rs)","text":"<p>The following test files exist in both trees with equivalent coverage: - All <code>app-server</code> tests (14 files) \u2705 - All <code>apply-patch</code> tests (2 files) \u2705 - All <code>chatgpt</code> tests (2 files) \u2705 - Most <code>core</code> tests (18 of 27 files) \u2705 - All <code>exec</code> tests (7 files) \u2705 - All <code>execpolicy</code> tests (10 files) \u2705 - All <code>linux-sandbox</code> tests (2 files) \u2705 - All <code>login</code> tests (3 files) \u2705 - All <code>mcp-server</code> tests (2 files) \u2705 - All <code>mcp-types</code> tests (3 files) \u2705</p>"},{"location":"migration/legacy-tests-retirement/#deletion-prerequisites-and-blockers","title":"Deletion Prerequisites and Blockers","text":""},{"location":"migration/legacy-tests-retirement/#critical-gaps-requiring-action","title":"Critical Gaps Requiring Action","text":""},{"location":"migration/legacy-tests-retirement/#1-unified-executor-tests-high-priority","title":"1. Unified Executor Tests (HIGH PRIORITY)","text":"<p>Files: <code>codex-rs/core/tests/suite/unified_exec.rs</code></p> <p>Coverage: - <code>unified_exec_reuses_session_via_stdin()</code> - Tests stdin session reuse - <code>unified_exec_timeout_and_followup_poll()</code> - Tests timeout handling and polling</p> <p>Blockers: - \u274c No equivalent tests in <code>code-rs/core/tests/</code> - \u274c Fork uses legacy executor architecture (see <code>subsystem-migration-status.md</code>)</p> <p>Prerequisites for Deletion: 1. Port unified executor stdin tests to <code>code-rs/core/tests/suite/exec.rs</code> 2. Port timeout/polling tests to <code>code-rs/core/tests/suite/exec_stream_events.rs</code> 3. Verify coverage with: <code>cargo test -p code-core unified_exec</code> 4. Run smoke test: Execute multi-turn conversation with stdin input</p> <p>Owner: Core runtime pod Estimated Duration: 2-3 days</p>"},{"location":"migration/legacy-tests-retirement/#2-model-tool-tests-high-priority","title":"2. Model &amp; Tool Tests (HIGH PRIORITY)","text":"<p>Files: - <code>codex-rs/core/tests/suite/model_tools.rs</code> - <code>codex-rs/core/tests/suite/tool_harness.rs</code> - <code>codex-rs/core/tests/suite/tools.rs</code> - <code>codex-rs/core/tests/suite/read_file.rs</code> - <code>codex-rs/core/tests/suite/view_image.rs</code></p> <p>Coverage: - Model-specific tool selection logic - Tool execution framework and harness - File reading and image viewing tools - SSE tool identifier parsing</p> <p>Blockers: - \u274c No tool-specific tests in <code>code-rs/core/tests/</code> - \u26a0\ufe0f  Fork uses different tool router (hybrid approach per migration status)</p> <p>Prerequisites for Deletion: 1. Create <code>code-rs/core/tests/suite/tool_selection.rs</code> for model tool tests 2. Create <code>code-rs/core/tests/suite/tool_execution.rs</code> for harness/tools tests 3. Create <code>code-rs/core/tests/suite/file_tools.rs</code> for read_file/view_image tests 4. Verify coverage with: <code>cargo test -p code-core tool</code> 5. Run smoke tests:    - Request file read via tool use    - Request image view via tool use    - Verify model-specific tool selection</p> <p>Owner: Core runtime pod Estimated Duration: 3-5 days</p>"},{"location":"migration/legacy-tests-retirement/#3-tui-rendering-tests-medium-priority","title":"3. TUI Rendering Tests (MEDIUM PRIORITY)","text":"<p>Files: - <code>codex-rs/tui/tests/suite/status_indicator.rs</code> - <code>codex-rs/tui/tests/suite/vt100_history.rs</code> - <code>codex-rs/tui/tests/suite/vt100_live_commit.rs</code></p> <p>Coverage: - Status widget rendering and updates - VT100 terminal history rendering - Word wrapping, ANSI sequences, emoji/CJK handling - Cursor restoration - Live commit display</p> <p>Blockers: - \u26a0\ufe0f Existing smoke test depends on private modules gated behind <code>#[cfg(test)]</code> - \u26a0\ufe0f Fork uses legacy TUI layout (fork-primary per migration status)</p> <p>Prerequisites for Deletion: 1. Create <code>code-rs/tui/tests/</code> directory structure 2. Port <code>vt100_history.rs</code> tests (7+ test cases for wrapping/ANSI) 3. Port <code>vt100_live_commit.rs</code> tests 4. Port <code>status_indicator.rs</code> tests 5. Add test infrastructure: <code>code-rs/tui/tests/all.rs</code>, <code>suite/mod.rs</code> 6. Verify coverage with: <code>cargo test -p code-tui</code> 7. Run visual regression tests with captured VT100 output</p> <p>Owner: TUI pod Estimated Duration: 4-6 days</p>"},{"location":"migration/legacy-tests-retirement/#smoke-and-regression-test-matrix","title":"Smoke and Regression Test Matrix","text":"<p>Before deleting any legacy test file, run these smoke tests to ensure coverage:</p>"},{"location":"migration/legacy-tests-retirement/#core-runtime-smoke-tests","title":"Core Runtime Smoke Tests","text":"<pre><code># Unified executor\ncargo test -p code-core exec -- --test-threads=1\ncargo test -p code-core stream -- --test-threads=1\n\n# Tool execution\ncargo test -p code-core -- tool --test-threads=1\ncargo test -p code-core -- read_file view_image --test-threads=1\n\n# Model integration\ncargo test -p code-core -- model --test-threads=1\n</code></pre> <p>Expected: All tests pass with no panics or timeouts</p>"},{"location":"migration/legacy-tests-retirement/#tui-smoke-tests","title":"TUI Smoke Tests","text":"<pre><code># Once TUI tests are created\ncargo test -p code-tui -- --test-threads=1\n\n# Manual regression test\n./scripts/test-tui-rendering.sh  # Create this script\n</code></pre> <p>Expected: - VT100 output matches golden files - No rendering glitches with emoji/CJK/ANSI - Status indicators update correctly</p>"},{"location":"migration/legacy-tests-retirement/#end-to-end-regression-tests","title":"End-to-End Regression Tests","text":"<pre><code># Full conversation flow\n./build-fast.sh --workspace code\ncargo run -p code-cli -- chat \"read the file README.md and summarize it\"\n\n# Multi-turn with tools\ncargo run -p code-cli -- chat \"list files, then read the largest one\"\n\n# Image tools (if applicable)\ncargo run -p code-cli -- chat \"show me the image at docs/logo.png\"\n</code></pre> <p>Expected: - Tools execute successfully - Streaming output is ordered correctly - No executor timeouts or hangs</p>"},{"location":"migration/legacy-tests-retirement/#deletion-checklist-by-package","title":"Deletion Checklist by Package","text":""},{"location":"migration/legacy-tests-retirement/#phase-1-core-runtime-tests-2025-10-20-2025-10-31","title":"Phase 1: Core Runtime Tests (2025-10-20 \u2192 2025-10-31)","text":"<p>Owner: Core runtime pod</p> <ul> <li> BLOCKED - Create tool coverage tests in <code>code-rs/core/tests/suite/</code></li> <li> Port <code>model_tools.rs</code> \u2192 <code>tool_selection.rs</code></li> <li> Port <code>tool_harness.rs</code> + <code>tools.rs</code> \u2192 <code>tool_execution.rs</code></li> <li> Port <code>read_file.rs</code> + <code>view_image.rs</code> \u2192 <code>file_tools.rs</code></li> <li> <p> Port <code>unified_exec.rs</code> \u2192 enhance <code>exec.rs</code> and <code>exec_stream_events.rs</code></p> </li> <li> <p> VERIFY - Run smoke tests (listed above)</p> </li> <li> All cargo tests pass</li> <li> Manual tool execution works</li> <li> <p> No regressions in CI/CD</p> </li> <li> <p> DELETE - Remove upstream tests once verified</p> </li> <li> Delete <code>codex-rs/core/tests/suite/model_tools.rs</code></li> <li> Delete <code>codex-rs/core/tests/suite/tool_harness.rs</code></li> <li> Delete <code>codex-rs/core/tests/suite/tools.rs</code></li> <li> Delete <code>codex-rs/core/tests/suite/read_file.rs</code></li> <li> Delete <code>codex-rs/core/tests/suite/view_image.rs</code></li> <li> Delete <code>codex-rs/core/tests/suite/unified_exec.rs</code></li> <li> <p> Update <code>codex-rs/core/tests/suite/mod.rs</code> to remove deleted modules</p> </li> <li> <p> DOCUMENT - Update this playbook</p> </li> <li> Mark tests as retired in this document</li> <li> Update <code>subsystem-migration-status.md</code> with test coverage status</li> <li> Record deletion in PR description</li> </ul> <p>Estimated Duration: 2 weeks Risk Level: HIGH (critical path functionality)</p>"},{"location":"migration/legacy-tests-retirement/#phase-2-tui-rendering-tests-2025-11-01-2025-11-15","title":"Phase 2: TUI Rendering Tests (2025-11-01 \u2192 2025-11-15)","text":"<p>Owner: TUI pod</p> <ul> <li> BLOCKED - Create TUI test infrastructure in <code>code-rs/tui/</code></li> <li> Create <code>tests/all.rs</code> entry point</li> <li> Create <code>tests/suite/mod.rs</code> module file</li> <li> <p> Create <code>tests/common/</code> for shared fixtures</p> </li> <li> <p> PORT - Port upstream TUI tests</p> </li> <li> Port <code>vt100_history.rs</code> (7+ test cases)</li> <li> Port <code>vt100_live_commit.rs</code></li> <li> Port <code>status_indicator.rs</code></li> <li> <p> Add golden file fixtures for VT100 output</p> </li> <li> <p> VERIFY - Run smoke tests</p> </li> <li> <code>cargo test -p code-tui</code> passes</li> <li> Visual regression: VT100 output matches golden files</li> <li> <p> Manual TUI test: emoji, CJK, ANSI codes render correctly</p> </li> <li> <p> DELETE - Remove upstream tests once verified</p> </li> <li> Delete <code>codex-rs/tui/tests/suite/vt100_history.rs</code></li> <li> Delete <code>codex-rs/tui/tests/suite/vt100_live_commit.rs</code></li> <li> Delete <code>codex-rs/tui/tests/suite/status_indicator.rs</code></li> <li> <p> Update <code>codex-rs/tui/tests/suite/mod.rs</code></p> </li> <li> <p> DOCUMENT - Update documentation</p> </li> <li> Mark tests as retired</li> <li> Update <code>tui-chatwidget-refactor.md</code> with test coverage</li> </ul> <p>Estimated Duration: 2 weeks Risk Level: MEDIUM (UX-critical but isolated)</p>"},{"location":"migration/legacy-tests-retirement/#phase-3-final-cleanup-2025-11-16-2025-11-30","title":"Phase 3: Final Cleanup (2025-11-16 \u2192 2025-11-30)","text":"<p>Owner: Repo maintainers</p> <ul> <li> AUDIT - Verify all legacy tests are retired or ported</li> <li> Run coverage comparison script: <code>./scripts/compare-test-coverage.sh</code></li> <li> Confirm no orphaned test files in <code>codex-rs/</code></li> <li> <p> Verify <code>code-rs/</code> has equal or better coverage</p> </li> <li> <p> OPTIMIZE - Clean up test infrastructure</p> </li> <li> Remove unused <code>codex-rs/*/tests/common/</code> if fully duplicated</li> <li> Consolidate test fixtures between packages</li> <li> <p> Remove deprecated test utilities</p> </li> <li> <p> DELETE - Remove entire <code>codex-rs/</code> test directories (if appropriate)</p> </li> <li> Evaluate if <code>codex-rs/</code> can be archived entirely</li> <li> <p> If yes, move to <code>archive/codex-rs/</code> or remove from repo</p> </li> <li> <p> DOCUMENT - Final documentation update</p> </li> <li> Mark playbook as COMPLETE</li> <li> Update <code>code-dead-code-sweeps.md</code> with test deletion outcomes</li> <li> Archive this playbook to <code>docs/archive/</code></li> </ul> <p>Estimated Duration: 2 weeks Risk Level: LOW (cleanup phase)</p>"},{"location":"migration/legacy-tests-retirement/#sequencing-timeline","title":"Sequencing Timeline","text":"<pre><code>2025-10-20  Phase 1 Start: Core Runtime Tests\n\u2502           \u251c\u2500 Create tool coverage tests (3-5 days)\n\u2502           \u251c\u2500 Port unified executor tests (2-3 days)\n\u2502           \u251c\u2500 Run smoke tests (1 day)\n\u2502           \u2514\u2500 Delete upstream tests (1 day)\n2025-10-31  Phase 1 Complete\n\n2025-11-01  Phase 2 Start: TUI Rendering Tests\n\u2502           \u251c\u2500 Create TUI test infrastructure (2 days)\n\u2502           \u251c\u2500 Port VT100 rendering tests (3-4 days)\n\u2502           \u251c\u2500 Port status indicator tests (2 days)\n\u2502           \u251c\u2500 Run visual regression tests (2 days)\n\u2502           \u2514\u2500 Delete upstream tests (1 day)\n2025-11-15  Phase 2 Complete\n\n2025-11-16  Phase 3 Start: Final Cleanup\n\u2502           \u251c\u2500 Audit coverage (3 days)\n\u2502           \u251c\u2500 Optimize test infrastructure (4 days)\n\u2502           \u251c\u2500 Remove orphaned tests (2 days)\n\u2502           \u2514\u2500 Final documentation (2 days)\n2025-11-30  Phase 3 Complete - All Legacy Tests Retired\n</code></pre> <p>Total Duration: 6 weeks Critical Path: Phase 1 (Core Runtime Tests) blocks Phase 2 and 3</p>"},{"location":"migration/legacy-tests-retirement/#safety-checklist","title":"Safety Checklist","text":"<p>Before deleting ANY legacy test file, confirm:</p> <ul> <li> \u2705 Replacement test exists in <code>code-rs/</code> OR coverage gap is documented and accepted</li> <li> \u2705 Replacement test covers all scenarios from legacy test (verified line-by-line)</li> <li> \u2705 <code>cargo test -p &lt;package&gt;</code> passes with replacement tests</li> <li> \u2705 Smoke tests pass (see matrix above)</li> <li> \u2705 CI/CD pipeline shows no regressions</li> <li> \u2705 Manual regression test completed for critical paths</li> <li> \u2705 Deletion is recorded in PR description and this playbook</li> <li> \u2705 Team reviewed and approved deletion (require 2+ approvals for high-risk tests)</li> </ul>"},{"location":"migration/legacy-tests-retirement/#risk-mitigation","title":"Risk Mitigation","text":""},{"location":"migration/legacy-tests-retirement/#high-risk-deletions-require-extra-scrutiny","title":"High-Risk Deletions (Require Extra Scrutiny)","text":"<ol> <li>Unified executor tests - Critical for multi-turn conversations</li> <li>Mitigation: Port first, verify extensively, delete last</li> <li> <p>Rollback plan: Restore from git history if issues found</p> </li> <li> <p>Tool execution tests - Core functionality</p> </li> <li>Mitigation: Create comprehensive replacement suite before deletion</li> <li> <p>Rollback plan: Re-enable upstream tests via feature flag</p> </li> <li> <p>VT100 rendering tests - UX-critical</p> </li> <li>Mitigation: Use golden file regression testing</li> <li>Rollback plan: Keep upstream tests until visual parity confirmed</li> </ol>"},{"location":"migration/legacy-tests-retirement/#rollback-procedure","title":"Rollback Procedure","text":"<p>If a deletion causes regressions:</p> <ol> <li>Immediate: Revert the deletion commit</li> <li>Short-term: Re-enable affected upstream test in <code>codex-rs/</code></li> <li>Long-term: Investigate gap, enhance replacement test, retry deletion</li> </ol>"},{"location":"migration/legacy-tests-retirement/#metrics-and-success-criteria","title":"Metrics and Success Criteria","text":"<p>Track these metrics throughout retirement:</p> Metric Baseline (2025-10-05) Target (2025-11-30) Legacy test files 9 files 0 files \u2705 Code-rs test coverage ~85% (estimated) \u226590% Test execution time TBD \u2264 baseline + 10% CI/CD test failures TBD No increase Regression bugs filed 0 0 <p>Success Criteria: - All 9 legacy test files deleted or coverage gaps explicitly documented - No increase in production bugs related to deleted test areas - <code>code-rs/</code> has equal or better test coverage than <code>codex-rs/</code> - Team confidence in test suite quality remains high</p>"},{"location":"migration/legacy-tests-retirement/#references","title":"References","text":"<ul> <li>Test comparison script: <code>./scripts/compare-test-coverage.sh</code> (create this)</li> <li>Coverage gaps: Tracked in GitHub issues with label <code>test-coverage-gap</code></li> <li>Related docs:</li> <li><code>docs/subsystem-migration-status.md</code> - Subsystem adoption status</li> <li><code>docs/code-dead-code-sweeps.md</code> - Dead code cleanup schedule</li> <li><code>docs/tui-chatwidget-refactor.md</code> - TUI refactoring plan</li> </ul>"},{"location":"migration/legacy-tests-retirement/#appendix-test-file-details","title":"Appendix: Test File Details","text":""},{"location":"migration/legacy-tests-retirement/#legacy-test-inventory","title":"Legacy Test Inventory","text":""},{"location":"migration/legacy-tests-retirement/#codex-rscoretestssuitemodel_toolsrs","title":"<code>codex-rs/core/tests/suite/model_tools.rs</code>","text":"<ul> <li>Functions tested: 4 tests</li> <li>Key scenarios: SSE tool completion, model-specific tool selection</li> <li>Dependencies: Mock SSE server, tool identifier parsing</li> <li>Replacement status: \u274c Not yet ported</li> </ul>"},{"location":"migration/legacy-tests-retirement/#codex-rscoretestssuiteunified_execrs","title":"<code>codex-rs/core/tests/suite/unified_exec.rs</code>","text":"<ul> <li>Functions tested: 2 tests</li> <li>Key scenarios: stdin session reuse, timeout and followup poll</li> <li>Dependencies: Unified executor module (not in fork)</li> <li>Replacement status: \u274c Blocked by executor migration</li> </ul>"},{"location":"migration/legacy-tests-retirement/#codex-rstuitestssuitevt100_historyrs","title":"<code>codex-rs/tui/tests/suite/vt100_history.rs</code>","text":"<ul> <li>Functions tested: 7+ tests</li> <li>Key scenarios:</li> <li>Basic insertion without wrap</li> <li>Long token wrapping</li> <li>Emoji and CJK character handling</li> <li>ANSI escape sequence rendering</li> <li>Cursor restoration</li> <li>Word wrap without mid-word split</li> <li>Em-dash and space word wrap</li> <li>Dependencies: VT100 renderer, word wrap logic</li> <li>Replacement status: \u274c No TUI tests in fork</li> </ul> <p>Last Updated: 2025-10-05 Next Review: 2025-10-20 (Phase 1 kickoff) Playbook Status: \ud83d\udfe1 IN PROGRESS - Awaiting test porting</p>"},{"location":"performance/benchmarks/","title":"Benchmarks","text":"<p>MARS achieves significant improvements on reasoning benchmarks.</p>"},{"location":"performance/benchmarks/#results","title":"Results","text":"Benchmark Baseline MARS Improvement AIME 2025 43.3% 73.3% +30pp (+69%) IMO 2025 16.7% 33.3% +16.7pp (+100%) LiveCodeBench 39.05% 50.48% +11.43pp (+29%)"},{"location":"performance/benchmarks/#test-coverage","title":"Test Coverage","text":"<ul> <li>43 unit tests across all modules</li> <li>29 integration tests (MCTS + multi-provider)</li> <li>100% core API coverage</li> </ul>"},{"location":"performance/benchmarks/#performance-metrics","title":"Performance Metrics","text":"<p>See Performance Guide for detailed analysis.</p>"},{"location":"plans/background-ordering-refactor/","title":"Background Ordering Refactor Plan","text":""},{"location":"plans/background-ordering-refactor/#objective","title":"Objective","text":"<p>Guarantee that all UI background events carry immutable order metadata captured at the moment the originating action begins, so banners can no longer drift into later turns. The refactor removes the fallback heuristic that re-evaluates global UI state at delivery time and replaces it with explicit handles that freeze the request ordinal and sequence.</p>"},{"location":"plans/background-ordering-refactor/#scope","title":"Scope","text":"<ul> <li><code>code-rs/tui/src/chatwidget.rs</code></li> <li><code>code-rs/tui/src/app_event_sender.rs</code></li> <li>Async producers that currently call <code>AppEventSender::send_background_event*</code></li> <li>Associated unit tests under <code>code-rs/tui/src/chatwidget.rs</code></li> </ul>"},{"location":"plans/background-ordering-refactor/#constraints-signals","title":"Constraints &amp; Signals","text":"<ul> <li>Preserve existing <code>OrderMeta</code> semantics (<code>output_index = i32::MAX</code>) for tail background notices.</li> <li>Avoid regressions to snapshot/restore flows that rely on <code>UiBackgroundOrderHandle</code> sequence rehydration.</li> <li>Maintain compatibility for <code>BackgroundPlacement::BeforeNextOutput</code>, but refuse tail inserts without an explicit order.</li> <li>Update or add tests covering:</li> <li>Slash commands (<code>/branch</code>) emitting banners after user prompts without a provider turn.</li> <li>Ghost snapshot timeout notices.</li> <li>Restored sessions emitting late events.</li> </ul>"},{"location":"plans/background-ordering-refactor/#implementation-strategy","title":"Implementation Strategy","text":""},{"location":"plans/background-ordering-refactor/#1-promote-order-handles-to-public-api","title":"1. Promote Order Handles to Public API","text":"<ul> <li>Expose a new lightweight wrapper (e.g., <code>BackgroundOrderTicket</code>) that encapsulates <code>UiBackgroundOrderHandle</code>.</li> <li>Store it in <code>code-rs/tui/src/app_event_sender.rs</code> so async producers can hold a clone even after moving off the main thread.</li> <li>Provide constructors on <code>ChatWidget</code>:</li> <li><code>fn make_background_tail_ticket(&amp;mut self) -&gt; BackgroundOrderTicket</code></li> <li><code>fn make_background_before_next_output_ticket(&amp;mut self) -&gt; BackgroundOrderTicket</code></li> </ul>"},{"location":"plans/background-ordering-refactor/#2-eliminate-orderless-tail-sends","title":"2. Eliminate Orderless Tail Sends","text":"<ul> <li>Deprecate <code>AppEventSender::send_background_event</code> and <code>send_background_event_before_next_output</code> by replacing them with:</li> <li><code>send_background_event_with_ticket(ticket, message)</code></li> <li><code>send_background_before_next_output_with_ticket(ticket, message)</code></li> <li>Enforce at compile-time: no tail placement path accepts <code>order: None</code>.</li> <li>Update <code>AppEvent::InsertBackgroundEvent</code> producers to require an <code>OrderMeta</code> (or ticket) argument.</li> </ul>"},{"location":"plans/background-ordering-refactor/#3-capture-tickets-at-slash-command-dispatch","title":"3. Capture Tickets at Slash Command Dispatch","text":"<ul> <li>In <code>ChatWidget::handle_branch_command</code>, grab a tail ticket before spawning the async block and pass it into the closure.</li> <li>Repeat for other slash commands (<code>/merge</code>, <code>/cmd</code>, <code>/update</code>, etc.) and any UI-only flows that spawn tasks.</li> <li>For synchronous notices (e.g., \u201cCreating branch worktree\u2026\u201d), use the \u201cbefore next output\u201d ticket captured before the prompt increments the pending counter.</li> </ul>"},{"location":"plans/background-ordering-refactor/#4-reset-pending-prompt-state-on-ui-only-commands","title":"4. Reset Pending Prompt State on UI-Only Commands","text":"<ul> <li>Introduce a helper <code>fn consume_pending_prompt_for_ui_only_turn(&amp;mut self)</code> that decrements the counter once the slash command response is queued.</li> <li>Call it immediately after scheduling the command\u2019s banners so the UI state reflects \u201ccurrent turn resolved.\u201d</li> <li>Ensure it does not fire when a provider request truly starts (<code>TaskStarted</code> still clears the counter).</li> </ul>"},{"location":"plans/background-ordering-refactor/#5-update-async-helpers-services","title":"5. Update Async Helpers &amp; Services","text":"<ul> <li><code>ghost_snapshot.rs</code>, auto-upgrade, GitHub Actions watcher, browser connectors, and cloud task helpers must accept a ticket captured at trigger time.</li> <li>For global helpers (<code>AppEventSender</code> clones passed into other modules), thread the ticket through their APIs so they no longer call the legacy <code>send_background_event</code>.</li> </ul>"},{"location":"plans/background-ordering-refactor/#6-remove-fallback-logic","title":"6. Remove Fallback Logic","text":"<ul> <li>Delete the branch in <code>insert_background_event_with_placement</code> that synthesizes order metadata when <code>order.is_none()</code>.</li> <li>Simplify <code>system_order_key</code> and <code>background_tail_request_ordinal</code> to assume <code>order</code> is always present for tail placements.</li> <li>Add defensive logging/assertions that fire if new code attempts to insert without order metadata.</li> </ul>"},{"location":"plans/background-ordering-refactor/#7-expand-test-coverage","title":"7. Expand Test Coverage","text":"<ul> <li>Extend existing tests to cover the previously failing scenario:</li> <li>Seed <code>pending_user_prompts_for_next_turn = 1</code>, capture a ticket, enqueue a tail event, assert <code>OrderKey.req</code> matches the current request.</li> <li>Add async-style tests that simulate <code>/branch</code> completion without <code>TaskStarted</code>, verifying history ordering remains stable after subsequent user prompts.</li> <li>Snapshot tests for restored sessions to confirm sequence rehydration still advances monotonically.</li> </ul>"},{"location":"plans/background-ordering-refactor/#8-migration-cleanup","title":"8. Migration &amp; Cleanup","text":"<ul> <li>Provide shims during transition (feature-gated) if needed, but plan to remove the old API once all call sites migrate.</li> <li>Update developer docs to explain ticket usage for background events.</li> <li>Run <code>./build-fast.sh</code> and targeted unit tests.</li> </ul>"},{"location":"plans/background-ordering-refactor/#current-progress-2025-10-02","title":"Current Progress (2025-10-02)","text":""},{"location":"plans/background-ordering-refactor/#step-1-ticket-api","title":"\u2705 Step 1 \u2013 Ticket API","text":"<ul> <li><code>BackgroundOrderTicket</code> exposed with cloning semantics and production/test constructors.</li> <li>Tail and before-next-output helpers (<code>make_background_tail_ticket</code>, <code>make_background_before_next_output_ticket</code>) now power all UI-issued banners.</li> <li>Conversation bootstrap failures, <code>/branch</code>, ghost snapshots, auto-upgrade notices, GitHub Actions watcher, browser/CDP flows, cloud tasks, <code>/cloud</code> subcommands, and slash-command banners all mint tickets up front.</li> </ul>"},{"location":"plans/background-ordering-refactor/#step-2-ticket-plumbing-throughout-the-ui","title":"\u2705 Step 2 \u2013 Ticket Plumbing Throughout the UI","text":"<ul> <li>All chatwidget flows (browser toggles, cloud tasks, branch/merge, Chrome connection, <code>/login</code>, <code>/update</code>, <code>/github</code>, etc.) now thread <code>BackgroundOrderTicket</code> instances into async closures and visible toasts.</li> <li><code>consume_pending_prompt_for_ui_only_turn()</code> clears stale prompt counters so slash commands no longer misclassify delayed banners as the \u201cnext\u201d turn.</li> <li>Bottom pane views (update settings, notifications, theme selection, login/add-account) and background helpers now require tickets at construction time.</li> </ul>"},{"location":"plans/background-ordering-refactor/#step-3-approval-modal-flows","title":"\u2705 Step 3 \u2013 Approval &amp; Modal flows","text":"<ul> <li><code>UserApprovalWidget</code> accepts a before-next-output ticket; decisions send ordered banners instead of relying on fallback synthesis.</li> <li><code>ApprovalModalView</code> queues <code>(ApprovalRequest, BackgroundOrderTicket)</code> pairs so multiple approvals preserve per-request ordinals.</li> <li>Tests were updated to exercise the new APIs via <code>BackgroundOrderTicket::test_for_request</code>.</li> </ul>"},{"location":"plans/background-ordering-refactor/#step-4-fallback-removal-api-cleanup","title":"\u2705 Step 4 \u2013 Fallback Removal &amp; API Cleanup","text":"<ul> <li><code>ChatWidget::insert_background_event_with_placement</code> now refuses tail inserts without explicit order metadata (logs and drops the message instead of synthesizing).</li> <li>Deprecated helpers (<code>send_background_event</code>, <code>send_background_event_before_next_output</code>) were removed from <code>AppEventSender</code>.</li> <li><code>push_background_tail</code> and related helpers internally mint tickets; legacy <code>order: None</code> paths are gone.</li> <li>Regression test <code>background_event_before_next_output_precedes_later_cells</code> now verifies ordered insertions using explicit tickets.</li> </ul>"},{"location":"plans/background-ordering-refactor/#remaining-focus-areas","title":"Remaining Focus Areas","text":"<ol> <li>Additional regression coverage \u2013 add tests capturing:</li> <li>Pending prompt &gt; 0 when slash commands emit multi-stage banners.</li> <li>Session restore + late async completion to ensure monotonic sequence rehydration.</li> <li>Docs &amp; developer guidance \u2013 document ticket usage patterns for async producers (README/DEVELOPING or a dedicated dev note).</li> <li>Manual QA \u2013 run <code>/branch</code>, <code>/cloud</code>, browser toggles, approval modals, and login flows end-to-end to confirm grouping across user prompts.</li> </ol>"},{"location":"plans/background-ordering-refactor/#verification-checklist","title":"Verification Checklist","text":"<ul> <li> Add targeted unit tests covering pending prompt scenarios with explicit tickets.</li> <li> Document ticket best practices for new async producers.</li> <li> Manual walkthrough keeps <code>/branch</code> banners ordered after subsequent prompts.</li> <li> Manual walkthrough keeps approval decisions adjacent to the associated command/tool begin cell.</li> <li> <code>./build-fast.sh</code> and focused tests pass without warnings.</li> </ul>"},{"location":"plans/background-ordering-refactor/#verification-checklist_1","title":"Verification Checklist","text":"<ul> <li>All background banners stay adjacent to their originating prompt across repeated <code>/branch</code>, <code>/merge</code>, ghost snapshot, and auto-upgrade scenarios.</li> <li>No warnings about \u201clegacy background event without order\u201d remain.</li> <li><code>tail_background_event_keeps_position_vs_next_output</code> passes with pending prompts set both to 0 and 1.</li> <li>Manual TUI walkthrough confirms ordering stability after multiple delayed async completions.</li> </ul>"},{"location":"plans/settings-overlay-two-level/","title":"Two-Level Settings Overlay Design","text":"<p>Last updated: 2025-10-16</p>"},{"location":"plans/settings-overlay-two-level/#goals","title":"Goals","text":"<ul> <li>Introduce an explicit two-phase UX for the Settings overlay: a top-level   menu (\u201cMenu mode\u201d) and focused section views (\u201cSection mode\u201d).</li> <li>Reserve navigation keys so <code>\u2191</code>/<code>\u2193</code> move within the menu, <code>Enter</code>/<code>\u2192</code> drill   into a section, <code>Esc</code>/<code>\u2190</code> navigate back toward the menu, and <code>Esc</code> from the   menu closes the overlay.</li> <li>Add a breadcrumb/header row that clarifies where the user is and surfaces   the primary key hints without overloading the existing section content.</li> <li>Preserve the existing section renderers and business logic with minimal   rewrites (Agents/Limits integrations remain as-is inside Section mode).</li> </ul>"},{"location":"plans/settings-overlay-two-level/#non-goals","title":"Non-Goals","text":"<ul> <li>Rewriting individual section UIs (e.g., the Agents or Limits editors).</li> <li>Changing how slash commands and AppEvents populate section data (they just   target the correct mode).</li> <li>Adding new settings sections\u2014this refactor only restructures navigation.</li> </ul>"},{"location":"plans/settings-overlay-two-level/#current-state-summary","title":"Current State Summary","text":"<ul> <li><code>SettingsOverlayView</code> renders a sidebar and section content simultaneously,   with <code>\u2191/\u2193</code>, <code>Tab</code>, and <code>\u2190/\u2192</code> all selecting sections. <code>Esc</code> always closes the   overlay.</li> <li>Section views also consume <code>\u2190/\u2192</code> for in-form toggles (Agents, MCP, etc.) and   occasionally use <code>Tab</code> for intra-form navigation.</li> <li>There is no explicit state to distinguish \u201cmenu focus\u201d vs. \u201csection focus\u201d; a   slash command jumps directly into a section but the key handlers still assume   global focus.</li> <li>Header chrome is a simple hint row (no breadcrumb, limited context).</li> </ul> <p>The combination leads to confusing focus: pressing <code>\u2191/\u2193</code> inside a section may   unexpectedly switch sections instead of scrolling the current content, and   <code>Esc</code> provides no intermediate \u201cback to menu\u201d step.</p>"},{"location":"plans/settings-overlay-two-level/#proposed-architecture","title":"Proposed Architecture","text":""},{"location":"plans/settings-overlay-two-level/#high-level-state-machine","title":"High-Level State Machine","text":"<p>Introduce an overlay mode enum that lives inside <code>SettingsOverlayView</code>:</p> <pre><code>enum SettingsOverlayMode {\n    Menu(MenuState),\n    Section(SectionState),\n}\n</code></pre> <ul> <li><code>MenuState</code> tracks the highlighted section index and optional metadata (e.g.,   last-opened timestamp for future polish).</li> <li><code>SectionState</code> stores the active <code>SettingsSection</code> and whether the user got   there from a menu selection vs. a direct AppEvent (used for breadcrumb and   \u201cgo back\u201d logic).</li> </ul> <p><code>SettingsState</code> in <code>chatwidget.rs</code> continues to hold <code>Option&lt;SettingsOverlayView&gt;</code>;   the view now encapsulates the mode, menu selection, and section content.</p>"},{"location":"plans/settings-overlay-two-level/#data-model-additions","title":"Data Model Additions","text":"<ul> <li><code>SettingsOverlayView</code> gains:</li> <li><code>mode: SettingsOverlayMode</code></li> <li><code>last_section: SettingsSection</code> (used to pre-select a menu row when     reopening).</li> <li>helper accessors: <code>is_menu_active()</code>, <code>active_section()</code>,     <code>set_mode_menu(selected: Option&lt;SettingsSection&gt;)</code>, and     <code>set_mode_section(section: SettingsSection)</code>.</li> <li><code>MenuState</code> holds a vector of <code>MenuItem</code> structs with label, optional icon,   description, and section pointer.</li> <li><code>BreadcrumbHeader</code> helper (see Rendering) stores the current breadcrumb   segments and key hint strings.</li> </ul>"},{"location":"plans/settings-overlay-two-level/#interaction-flow","title":"Interaction Flow","text":"<ol> <li>Opening <code>/settings</code> calls <code>show_settings_overlay_full(None)</code>:</li> <li>Overlay instantiates in <code>SettingsOverlayMode::Menu</code>, highlighting the last      visited section (default <code>SettingsSection::Model</code>).</li> <li>User presses <code>Enter</code>/<code>\u2192</code> to drill into the highlighted section. Overlay    switches to <code>Section</code> mode, breadcrumb updates to <code>Settings \u25b8 {Section}</code>.</li> <li>Inside <code>Section</code> mode:</li> <li><code>\u2190</code> or <code>Esc</code> returns to the menu (same selection retained).</li> <li>Content-specific keys (<code>Tab</code>, letter shortcuts, etc.) remain scoped.</li> <li>From <code>Menu</code> mode, <code>Esc</code> closes the overlay (and triggers <code>notify_close</code>).</li> <li>Slash commands or AppEvents that need a specific section call    <code>ensure_settings_overlay_section</code>, which now:</li> <li>Creates the overlay if missing.</li> <li>Sets mode to <code>Section</code> and ensures the menu highlight follows the section      so the user\u2019s back navigation is predictable.</li> </ol>"},{"location":"plans/settings-overlay-two-level/#key-handling-changes","title":"Key Handling Changes","text":"<ul> <li>Update <code>handle_settings_key</code> (and any direct overlay key entry points) to   branch on <code>mode</code> before delegating.</li> <li>Menu mode: accept <code>\u2191/\u2193</code>, <code>k/j</code> (existing aliases), alphanumeric     shortcuts, <code>Home</code>, <code>End</code>; <code>Enter/Right</code> transitions to Section; <code>Esc</code>     closes; other keys ignored.</li> <li>Section mode: forward keys to the active <code>SettingsContent</code> first. If the     content does not consume the key, handle <code>Esc</code>/<code>Left</code> by switching back to     Menu. Suppress <code>Tab</code>/<code>Shift+Tab</code> switching sections\u2014the content gets those.</li> <li>As part of this refactor we will remap conflicting section keys per the   keymap audit:</li> <li>Replace <code>\u2190/\u2192</code> toggles in section views with <code>Space</code>/<code>Enter</code> or explicit     letter shortcuts (<code>agents_settings_view</code>, <code>mcp_settings_view</code>, etc.).</li> <li>Limit top-level section cycling to <code>\u2191/\u2193</code> (drop <code>Tab</code>/<code>Shift+Tab</code>).</li> <li>Retain <code>PgUp/PgDn</code> semantics inside content views; they are unaffected.</li> </ul>"},{"location":"plans/settings-overlay-two-level/#rendering-updates","title":"Rendering Updates","text":"<ul> <li>Breadcrumb header: introduce a lightweight component (see agent proposal)   that renders two rows:</li> <li>Breadcrumb path: <code>Settings \u25b8 Agents</code> (menu displays <code>Settings \u25b8 Menu</code>).</li> <li>Key hints: <code>\u2191/\u2193 move  Enter open  Esc back  ? help</code> (Menu) or      <code>\u2190/Esc back  Tab cycle  ? help</code> (Section).</li> <li>Menu layout:</li> <li> <p>When in <code>Menu</code> mode, the body area renders a vertical list of sections with     label, short description (\u2264 60 chars), and an ASCII \u201cicon\u201d token. Suggested     tokens:</p> Section Token Short description (\u226460 chars) Agents <code>[AG]</code> Manage assistants and defaults Model <code>[ML]</code> Pick provider, temperature, safety Limits <code>[LM]</code> View and tune rate-limit guards Chrome <code>[CH]</code> Connect the Chrome bridge GitHub <code>[GH]</code> Link repos and choose scopes Updates <code>[UP]</code> Check version &amp; release channel Validation <code>[VL]</code> Configure pre-flight validations Notifications <code>[NT]</code> Choose desktop / in-app alerts Theme <code>[TH]</code> Switch palette, fonts, density MCP <code>[MC]</code> Manage MCP servers &amp; permissions <p>(UI can optionally swap in emoji later; ASCII placeholders keep rendering deterministic.) - Section layout:   - When in <code>Section</code> mode, reuse the existing content renderer. The menu list is hidden (or shrunk to 0 width) so keys and focus stay local to the content area.   - Breadcrumb header ensures users can hop back.</p> </li> </ul>"},{"location":"plans/settings-overlay-two-level/#appevent-and-command-wiring","title":"AppEvent and Command Wiring","text":"<ul> <li>Update <code>ChatWidget::ensure_settings_overlay_section</code>/   <code>show_settings_overlay_full</code> so any external entry point:</li> <li>Ensures the overlay exists.</li> <li>Sets <code>mode = Section(section)</code> and syncs the menu highlight.</li> <li>Prefills Section content and requests redraw.</li> <li>Adjust AppEvent handlers (<code>ShowAgentsOverview</code>, <code>ShowAgentEditor</code>, etc.) to   call the new helpers, but do not bypass the menu; returning via <code>Esc</code>   should land on the proper menu selection.</li> <li>Slash command handlers for <code>/settings</code>, <code>/agents</code>, <code>/limits</code>, <code>/model</code>, etc.   call <code>show_settings_overlay_full</code> with either <code>None</code> (menu) or <code>Some(section)</code>.</li> </ul>"},{"location":"plans/settings-overlay-two-level/#persistence-considerations","title":"Persistence Considerations","text":"<ul> <li>When the overlay closes from Section mode, we automatically revert to Menu   mode and reselect the active section before dismissal so reopening starts at   the same menu row.</li> <li>Optionally persist <code>last_section</code> in <code>SettingsState</code> if the overlay is torn   down and recreated frequently.</li> </ul>"},{"location":"plans/settings-overlay-two-level/#testing-verification-plan","title":"Testing &amp; Verification Plan","text":"<ul> <li>Snapshot coverage: add VT100 snapshots for</li> <li>Menu-only rendering (fresh open, highlight first section).</li> <li>Drill-down flow (open Agents, ensure breadcrumb updates, verify ESC twice     path).</li> <li>Keyboard guard rails (simulate <code>Esc</code> in Section mode ensures menu visible).</li> <li>Unit tests:</li> <li>Mode transition helpers (<code>set_mode_menu</code>, <code>set_mode_section</code>).</li> <li><code>handle_settings_key</code> behavior table-driven by mode.</li> <li>Manual checks:</li> <li>Ensure each section\u2019s custom key handling still works (esp. Agents editor,     Limits tabs) and no longer steals <code>\u2190/\u2192</code> reserved for overlay back/forward.</li> </ul>"},{"location":"plans/settings-overlay-two-level/#incremental-rollout-strategy","title":"Incremental Rollout Strategy","text":"<ol> <li>Scaffold modes: Introduce <code>SettingsOverlayMode</code>, restructure rendering to    hide content while in Menu mode, but leave key bindings temporarily shared.</li> <li>Keymap cleanup: Apply the remaps highlighted in the key audit so section    content no longer requires <code>\u2190/\u2192</code> for core actions.</li> <li>Mode-aware key handling: Update <code>handle_settings_key</code> and friends to    branch on mode, wire breadcrumb header, and adjust ESC logic.</li> <li>Slash command / AppEvent integration: Ensure all entry points set the    mode correctly and request redraws.</li> <li>Docs/help refresh: Update slash command docs and inline help once the UX    is firm.</li> <li>Testing sweep: Add VT100 snapshots, run <code>./build-fast.sh</code>, and manually    validate navigation.</li> </ol> <p>Each step keeps the overlay functional, allowing incremental PRs if desired.</p>"},{"location":"plans/settings-overlay-two-level/#open-questions-follow-ups","title":"Open Questions / Follow-Ups","text":"<ul> <li>Should we expose accelerator keys in the menu (e.g., <code>M</code> jumps to Model)? If   yes, we can display shortcut glyphs alongside the menu rows (the current   overlay already supports this).</li> <li>Do we want to remember \u201clast drill-down section\u201d between app launches? That   would require persisting the selection in user settings.</li> <li>The ASCII icon tokens may eventually need a design pass\u2014are emoji acceptable   in the TUI for higher visual separation?</li> <li>Some section views (Agents editor) still surface their own \u201cEsc closes\u201d copy;   we should revisit wording once the new breadcrumb/back flow lands.</li> </ul>"},{"location":"usage/basic/","title":"Basic Usage","text":"<p>Get started with MARS in minutes.</p>"},{"location":"usage/basic/#simple-example","title":"Simple Example","text":"<pre><code>#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    // 1. Create client\n    let client = ModelClient::default();\n\n    // 2. Create coordinator\n    let config = MarsConfig::default();\n    let mut coordinator = MarsCoordinator::new(config);\n\n    // 3. Ask a question\n    let query = \"What is 2 + 2?\";\n\n    // 4. Get answer\n    let result = coordinator.run_with_client(query, &amp;client).await?;\n\n    // 5. Display result\n    println!(\"Answer: {}\", result.answer);\n\n    Ok(())\n}\n</code></pre>"},{"location":"usage/basic/#with-advanced-features","title":"With Advanced Features","text":"<pre><code>let config = MarsConfig::default()\n    .with_advanced_features()\n    .with_moa_aggregation()\n    .with_num_agents(4);\n\nlet mut coordinator = MarsCoordinator::new(config);\nlet result = coordinator.run_with_client(query, &amp;client).await?;\n\nprintln!(\"Answer: {}\", result.answer);\nprintln!(\"Reasoning: {}\", result.reasoning);\nprintln!(\"Method: {:?}\", result.selection_method);\nprintln!(\"Tokens: {}\", result.total_tokens);\n</code></pre> <p>See Advanced Examples for more patterns.</p>"}]}